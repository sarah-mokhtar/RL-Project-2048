{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8b2d3935",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import random\n",
    "import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from gymcts.gymcts_agent import GymctsAgent\n",
    "from gymcts.gymcts_deepcopy_wrapper import DeepCopyMCTSGymEnvWrapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "32ac62f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class TwentyFortyEightEnv(gym.Env):\n",
    "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
    "\n",
    "    def __init__(self, render_mode=None,log_reward = True,negative_reward = -10, stop_stationary = True):\n",
    "        super().__init__()\n",
    "        self.board_size = 4\n",
    "        self.log_reward = log_reward\n",
    "        # Actions: 0 = up, 1 = down, 2 = left, 3 = right\n",
    "        self.action_space = spaces.Discrete(4)\n",
    "        self.negative_reward = negative_reward\n",
    "        self.stop_stationary = stop_stationary\n",
    "        # Board is 4x4 integers; observation is a flattened vector of size 16\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=0,\n",
    "            high=2**16,\n",
    "            shape=(1,4,4),\n",
    "            dtype=np.int32\n",
    "        )\n",
    "\n",
    "        self.render_mode = render_mode\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self, seed=None, options=None):\n",
    "        super().reset(seed=seed)\n",
    "\n",
    "        self.board = np.zeros((self.board_size, self.board_size), dtype=np.int32)\n",
    "        self.score = 0\n",
    "\n",
    "        self._add_tile()\n",
    "        self._add_tile()\n",
    "\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def get_mask(self):\n",
    "        return [\n",
    "            self._move_up(test_is_masked=True),\n",
    "            self._move_down(test_is_masked=True),\n",
    "            self._move_left(test_is_masked=True),\n",
    "            self._move_right(test_is_masked=True),\n",
    "\n",
    "        ]\n",
    "\n",
    "    def step(self, action):\n",
    "        old_board = self.board[:][:]\n",
    "        if action == 0:\n",
    "            reward = self._move_up()\n",
    "        elif action == 1:\n",
    "            reward = self._move_down()\n",
    "        elif action == 2:\n",
    "            reward = self._move_left()\n",
    "        elif action == 3:\n",
    "            reward = self._move_right()\n",
    "\n",
    "        # # Invalid move (board unchanged)\n",
    "        if np.array_equal(self.board, old_board):\n",
    "            \n",
    "            # done = True\n",
    "            reward = self.negative_reward\n",
    "        else:\n",
    "            # Only add a tile after a valid move\n",
    "            self._add_tile()\n",
    "        done = not self._moves_available()\n",
    "\n",
    "        return self._get_obs(), reward, done, False, {\"score\": self.score}\n",
    "\n",
    "    # -------- Rendering -------- #\n",
    "\n",
    "    def render(self):\n",
    "        print(\"\\nScore:\", self.score)\n",
    "        print(\"-\" * 25)\n",
    "        for row in self.board:\n",
    "            print(\"|\" + \"|\".join(f\"{num:^5}\" if num != 0 else \"     \" for num in row) + \"|\")\n",
    "            print(\"-\" * 25)\n",
    "\n",
    "    # -------- Helper Methods -------- #\n",
    "\n",
    "    def _get_obs(self):\n",
    "        return self.board\n",
    "    \n",
    "    def _add_tile(self):\n",
    "        empty = list(zip(*np.where(self.board == 0)))\n",
    "        if not empty:\n",
    "            return\n",
    "        i, j = random.choice(empty)\n",
    "        self.board[i, j] = 4 if random.random() < 0.1 else 2\n",
    "\n",
    "    def _moves_available(self):\n",
    "        if np.any(self.board == 0):\n",
    "            return True\n",
    "        for i in range(4):\n",
    "            for j in range(4):\n",
    "                if j < 3 and self.board[i, j] == self.board[i, j + 1]:\n",
    "                    return True\n",
    "                if i < 3 and self.board[i, j] == self.board[i + 1, j]:\n",
    "                    return True\n",
    "        return False\n",
    "\n",
    "    # -------- Movement Logic -------- #\n",
    "\n",
    "    def _compress(self, row):\n",
    "        new = row[row != 0]\n",
    "        return np.concatenate([new, np.zeros(4 - len(new), dtype=np.int32)])\n",
    "\n",
    "    def _merge(self, row):\n",
    "        score_gain = 0\n",
    "        for i in range(3):\n",
    "            if row[i] != 0 and row[i] == row[i + 1]:\n",
    "                row[i] *= 2\n",
    "                row[i + 1] = 0\n",
    "                score_gain += np.log2(row[i]) if self.log_reward else row[i]\n",
    "        return row, score_gain\n",
    "\n",
    "    def _move_left(self,test_is_masked = False):\n",
    "        total_gain = 0\n",
    "        new_board = np.zeros((self.board_size, self.board_size), dtype=np.int32)\n",
    "        old_board = self.board.copy()\n",
    "        for i in range(4):\n",
    "            row = self._compress(self.board[i])\n",
    "            row, gain = self._merge(row)\n",
    "            row = self._compress(row)\n",
    "            new_board[i] = row\n",
    "            total_gain += gain\n",
    "        if test_is_masked:\n",
    "            self.board = old_board\n",
    "            return  np.array_equal(self.board,new_board)\n",
    "        self.board = new_board\n",
    "        self.score += total_gain\n",
    "        return float(total_gain)\n",
    "\n",
    "    def _move_right(self,test_is_masked = False):\n",
    "        self.board = np.fliplr(self.board)\n",
    "        reward = self._move_left(test_is_masked=test_is_masked)\n",
    "        self.board = np.fliplr(self.board)\n",
    "        if test_is_masked:\n",
    "            return reward\n",
    "        return reward\n",
    "\n",
    "    def _move_up(self,test_is_masked = False):\n",
    "        self.board = self.board.T\n",
    "        reward = self._move_left(test_is_masked=test_is_masked)\n",
    "        self.board = self.board.T\n",
    "        if test_is_masked:\n",
    "            return reward\n",
    "        return reward\n",
    "\n",
    "    def _move_down(self,test_is_masked = False):\n",
    "        self.board = self.board.T\n",
    "        reward = self._move_right(test_is_masked=test_is_masked)\n",
    "        self.board = self.board.T\n",
    "        if test_is_masked:\n",
    "            return reward\n",
    "        return reward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce6b22ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 1: Reward 2.0 state is \n",
      " [[0 0 0 0]\n",
      " [0 0 2 0]\n",
      " [0 0 0 0]\n",
      " [0 0 0 4]]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m step = \u001b[32m0\u001b[39m\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m terminal:\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     action, _ = \u001b[43magent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mperform_mcts_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     obs, rew, term, trun, info = env.step(action)\n\u001b[32m     21\u001b[39m     terminal = term \u001b[38;5;129;01mor\u001b[39;00m trun\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/gymcts/gymcts_agent.py:182\u001b[39m, in \u001b[36mGymctsAgent.perform_mcts_step\u001b[39m\u001b[34m(self, search_start_node, num_simulations, render_tree_after_step)\u001b[39m\n\u001b[32m    179\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m search_start_node \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    180\u001b[39m     search_start_node = \u001b[38;5;28mself\u001b[39m.search_root_node\n\u001b[32m--> \u001b[39m\u001b[32m182\u001b[39m action = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mvanilla_mcts_search\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    183\u001b[39m \u001b[43m    \u001b[49m\u001b[43msearch_start_node\u001b[49m\u001b[43m=\u001b[49m\u001b[43msearch_start_node\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    184\u001b[39m \u001b[43m    \u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m=\u001b[49m\u001b[43mnum_simulations\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    185\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    186\u001b[39m next_node = search_start_node.children[action]\n\u001b[32m    188\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.clear_mcts_tree_after_step:\n\u001b[32m    189\u001b[39m     \u001b[38;5;66;03m# to clear memory we need to remove all nodes except the current node\u001b[39;00m\n\u001b[32m    190\u001b[39m     \u001b[38;5;66;03m# this is done by setting the root node to the current node\u001b[39;00m\n\u001b[32m    191\u001b[39m     \u001b[38;5;66;03m# and setting the parent of the current node to None\u001b[39;00m\n\u001b[32m    192\u001b[39m     \u001b[38;5;66;03m# we also need to reset the children of the current node\u001b[39;00m\n\u001b[32m    193\u001b[39m     \u001b[38;5;66;03m# this is done by calling the reset method\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/gymcts/gymcts_agent.py:221\u001b[39m, in \u001b[36mGymctsAgent.vanilla_mcts_search\u001b[39m\u001b[34m(self, search_start_node, num_simulations)\u001b[39m\n\u001b[32m    218\u001b[39m \u001b[38;5;28mself\u001b[39m._load_state(leaf_node)\n\u001b[32m    220\u001b[39m \u001b[38;5;66;03m# rollout\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m221\u001b[39m episode_return = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrollout\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    222\u001b[39m \u001b[38;5;66;03m# self.env.render()\u001b[39;00m\n\u001b[32m    224\u001b[39m \u001b[38;5;28mself\u001b[39m.backpropagation(node=leaf_node, episode_return=episode_return)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/gymcts/gymcts_deepcopy_wrapper.py:166\u001b[39m, in \u001b[36mDeepCopyMCTSGymEnvWrapper.rollout\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    164\u001b[39m     action = random.choice(\u001b[38;5;28mself\u001b[39m.get_valid_actions())\n\u001b[32m    165\u001b[39m     \u001b[38;5;66;03m# print(f\"Valid actions: {self.get_valid_actions()}, selected action: {action}\")\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m166\u001b[39m     _obs, _reward, is_terminal_state, _truncated, info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    169\u001b[39m episode_return = info[\u001b[33m\"\u001b[39m\u001b[33mepisode\u001b[39m\u001b[33m\"\u001b[39m][\u001b[33m\"\u001b[39m\u001b[33mr\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    170\u001b[39m log.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mRollout return: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisode_return\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/gymcts/gymcts_deepcopy_wrapper.py:137\u001b[39m, in \u001b[36mDeepCopyMCTSGymEnvWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    127\u001b[39m         \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    128\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    129\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    130\u001b[39m \u001b[33;03m    Performs a step in the environment.\u001b[39;00m\n\u001b[32m    131\u001b[39m \u001b[33;03m    This method is used to update the wrapper with the new state and the new action, to realize the terminal state\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    135\u001b[39m \u001b[33;03m    :return: the step tuple of the environment (obs, reward, terminated, truncated, info)\u001b[39;00m\n\u001b[32m    136\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     step_tuple = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    139\u001b[39m     obs, reward, terminated, truncated, info = step_tuple\n\u001b[32m    140\u001b[39m     \u001b[38;5;28mself\u001b[39m._terminal_flag = terminated \u001b[38;5;129;01mor\u001b[39;00m truncated\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/gymnasium/wrappers/common.py:513\u001b[39m, in \u001b[36mRecordEpisodeStatistics.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    509\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    510\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: ActType\n\u001b[32m    511\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[ObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Steps through the environment, recording the episode statistics.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m513\u001b[39m     obs, reward, terminated, truncated, info = \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    515\u001b[39m     \u001b[38;5;28mself\u001b[39m.episode_returns += reward\n\u001b[32m    516\u001b[39m     \u001b[38;5;28mself\u001b[39m.episode_lengths += \u001b[32m1\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/gymnasium/core.py:327\u001b[39m, in \u001b[36mWrapper.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mstep\u001b[39m(\n\u001b[32m    324\u001b[39m     \u001b[38;5;28mself\u001b[39m, action: WrapperActType\n\u001b[32m    325\u001b[39m ) -> \u001b[38;5;28mtuple\u001b[39m[WrapperObsType, SupportsFloat, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mbool\u001b[39m, \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]]:\n\u001b[32m    326\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Uses the :meth:`step` of the :attr:`env` that can be overwritten to change the returned data.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43menv\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mTwentyFortyEightEnv.step\u001b[39m\u001b[34m(self, action)\u001b[39m\n\u001b[32m     50\u001b[39m     reward = \u001b[38;5;28mself\u001b[39m._move_left()\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01melif\u001b[39;00m action == \u001b[32m3\u001b[39m:\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m     reward = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_move_right\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# # Invalid move (board unchanged)\u001b[39;00m\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m np.array_equal(\u001b[38;5;28mself\u001b[39m.board, old_board):\n\u001b[32m     56\u001b[39m \n\u001b[32m     57\u001b[39m     \u001b[38;5;66;03m# done = True\u001b[39;00m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[17]\u001b[39m\u001b[32m, line 133\u001b[39m, in \u001b[36mTwentyFortyEightEnv._move_right\u001b[39m\u001b[34m(self, test_is_masked)\u001b[39m\n\u001b[32m    131\u001b[39m \u001b[38;5;28mself\u001b[39m.board = np.fliplr(\u001b[38;5;28mself\u001b[39m.board)\n\u001b[32m    132\u001b[39m reward = \u001b[38;5;28mself\u001b[39m._move_left(test_is_masked=test_is_masked)\n\u001b[32m--> \u001b[39m\u001b[32m133\u001b[39m \u001b[38;5;28mself\u001b[39m.board = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfliplr\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mboard\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    134\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m test_is_masked:\n\u001b[32m    135\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m reward\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.11/site-packages/numpy/lib/_twodim_base_impl.py:58\u001b[39m, in \u001b[36m_flip_dispatcher\u001b[39m\u001b[34m(m)\u001b[39m\n\u001b[32m     54\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m int32\n\u001b[32m     55\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m int64\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_flip_dispatcher\u001b[39m(m):\n\u001b[32m     59\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m (m,)\n\u001b[32m     62\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_flip_dispatcher)\n\u001b[32m     63\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mfliplr\u001b[39m(m):\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "import gymnasium as gym\n",
    "\n",
    "\n",
    "\n",
    "# Make your gym env\n",
    "env = TwentyFortyEightEnv()\n",
    "\n",
    "# Wrap it with gymct\n",
    "env = DeepCopyMCTSGymEnvWrapper(env)\n",
    "agent = GymctsAgent(\n",
    "        env=env,\n",
    "        number_of_simulations_per_step=100,\n",
    "        clear_mcts_tree_after_step=True,\n",
    "    )\n",
    "# 3. solve the environment\n",
    "terminal = False\n",
    "step = 0\n",
    "while not terminal:\n",
    "    action, _ = agent.perform_mcts_step()\n",
    "    obs, rew, term, trun, info = env.step(action)\n",
    "    terminal = term or trun\n",
    "\n",
    "    step += 1\n",
    "\n",
    "    # log to console every 10 steps\n",
    "    print(f\"step {step}: Reward {rew} state is \\n {obs}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
