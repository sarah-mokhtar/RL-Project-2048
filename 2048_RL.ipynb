{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "authorship_tag": "ABX9TyPSGOIGFwyr/NXtGbufY0EN",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sarah-mokhtar/RL-Project-2048/blob/main/2048_RL.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i0mfmUFGk6al",
        "outputId": "50eb097b-0dd2-476a-ea4b-db5cdf7cd6a0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Collecting stable-baselines3[extra]\n",
            "  Downloading stable_baselines3-2.7.0-py3-none-any.whl.metadata (4.8 kB)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.9.0+cu126)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (3.10.0)\n",
            "Requirement already satisfied: opencv-python in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (4.12.0.88)\n",
            "Requirement already satisfied: pygame in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.6.1)\n",
            "Requirement already satisfied: tensorboard>=2.9.1 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (2.19.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (4.67.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (13.9.4)\n",
            "Requirement already satisfied: ale-py>=0.9.0 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (0.11.2)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.12/dist-packages (from stable-baselines3[extra]) (11.3.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.76.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.10)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (5.29.5)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (75.2.0)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (1.17.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard>=2.9.1->stable-baselines3[extra]) (3.1.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.20.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3[extra]) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (1.4.9)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3[extra]) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3[extra]) (2025.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->stable-baselines3[extra]) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->stable-baselines3[extra]) (0.1.2)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3[extra]) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard>=2.9.1->stable-baselines3[extra]) (3.0.3)\n",
            "Downloading stable_baselines3-2.7.0-py3-none-any.whl (187 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m187.2/187.2 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: stable-baselines3\n",
            "Successfully installed stable-baselines3-2.7.0\n"
          ]
        }
      ],
      "source": [
        "!pip install stable-baselines3[extra] gymnasium numpy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "class Game2048Env(gym.Env):\n",
        "\n",
        "\n",
        "    metadata = {\"render_modes\": [\"ansi\"], \"render_fps\": 60}\n",
        "\n",
        "    def __init__(self, render_mode=None, target_tile=2048):\n",
        "        super().__init__()\n",
        "\n",
        "        self.board_size = 4\n",
        "        self.target_tile = target_tile\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0,\n",
        "            high=15,\n",
        "            shape=(self.board_size, self.board_size),\n",
        "            dtype=np.int32,\n",
        "        )\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.board = np.zeros((self.board_size, self.board_size), dtype=np.int32)\n",
        "        self.score = 0\n",
        "        self.rng = np.random.default_rng()\n",
        "\n",
        "    def _slide_and_merge_line(self, line):\n",
        "        \"\"\"\n",
        "        line: 1D np.array of exponents (0 = empty)\n",
        "        Returns: (new_line, reward_from_merges)\n",
        "        \"\"\"\n",
        "        non_zero = line[line != 0].tolist()\n",
        "        new = []\n",
        "        reward = 0\n",
        "        i = 0\n",
        "        while i < len(non_zero):\n",
        "            if i + 1 < len(non_zero) and non_zero[i] == non_zero[i + 1]:\n",
        "                exp = non_zero[i] + 1\n",
        "                new.append(exp)\n",
        "                reward += 2 ** exp\n",
        "                i += 2\n",
        "            else:\n",
        "                new.append(non_zero[i])\n",
        "                i += 1\n",
        "        # pad with zeros\n",
        "        new += [0] * (len(line) - len(new))\n",
        "        return np.array(new, dtype=np.int32), reward\n",
        "\n",
        "    def _add_random_tile(self):\n",
        "        empty_positions = list(zip(*np.where(self.board == 0)))\n",
        "        if not empty_positions:\n",
        "            return\n",
        "        row, col = empty_positions[self.rng.integers(len(empty_positions))]\n",
        "\n",
        "        if self.rng.random() < 0.9:\n",
        "            self.board[row, col] = 1\n",
        "        else:\n",
        "            self.board[row, col] = 2\n",
        "\n",
        "    def _can_move(self):\n",
        "        # If any cell empty -> can move\n",
        "        if np.any(self.board == 0):\n",
        "            return True\n",
        "        # If any horizontal merge possible\n",
        "        for i in range(self.board_size):\n",
        "            for j in range(self.board_size - 1):\n",
        "                if self.board[i, j] == self.board[i, j + 1]:\n",
        "                    return True\n",
        "        # If any vertical merge possible\n",
        "        for j in range(self.board_size):\n",
        "            for i in range(self.board_size - 1):\n",
        "                if self.board[i, j] == self.board[i + 1, j]:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def _get_max_tile(self):\n",
        "        exp = int(self.board.max())\n",
        "        return 0 if exp == 0 else 2 ** exp\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.board[:] = 0\n",
        "        self.score = 0\n",
        "\n",
        "        self._add_random_tile()\n",
        "        self._add_random_tile()\n",
        "\n",
        "        observation = self.board.copy()\n",
        "        info = {\"score\": self.score, \"max_tile\": self._get_max_tile()}\n",
        "        return observation, info\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action), \"Invalid action\"\n",
        "\n",
        "        old_board = self.board.copy()\n",
        "        reward = 0\n",
        "\n",
        "\n",
        "        if action == 0:  # up\n",
        "            for col in range(self.board_size):\n",
        "                line = self.board[:, col]\n",
        "                new_line, r = self._slide_and_merge_line(line)\n",
        "                self.board[:, col] = new_line\n",
        "                reward += r\n",
        "        elif action == 1:  # down\n",
        "            for col in range(self.board_size):\n",
        "                line = self.board[:, col][::-1]\n",
        "                new_line, r = self._slide_and_merge_line(line)\n",
        "                self.board[:, col] = new_line[::-1]\n",
        "                reward += r\n",
        "        elif action == 2:  # left\n",
        "            for row in range(self.board_size):\n",
        "                line = self.board[row, :]\n",
        "                new_line, r = self._slide_and_merge_line(line)\n",
        "                self.board[row, :] = new_line\n",
        "                reward += r\n",
        "        elif action == 3:  # right\n",
        "            for row in range(self.board_size):\n",
        "                line = self.board[row, :][::-1]\n",
        "                new_line, r = self._slide_and_merge_line(line)\n",
        "                self.board[row, :] = new_line[::-1]\n",
        "                reward += r\n",
        "\n",
        "        moved = not np.array_equal(old_board, self.board)\n",
        "\n",
        "        if not moved:\n",
        "            reward -= 1.0\n",
        "        else:\n",
        "            self._add_random_tile()\n",
        "\n",
        "        self.score += reward\n",
        "\n",
        "        max_tile = self._get_max_tile()\n",
        "        terminated = False\n",
        "        if not self._can_move():\n",
        "            terminated = True\n",
        "        if max_tile >= self.target_tile:\n",
        "            terminated = True\n",
        "\n",
        "        truncated = False\n",
        "\n",
        "        observation = self.board.copy()\n",
        "        info = {\"score\": self.score, \"max_tile\": max_tile}\n",
        "\n",
        "        return observation, reward, terminated, truncated, info\n",
        "\n",
        "    def render(self):\n",
        "        if self.render_mode == \"ansi\":\n",
        "            return self._board_to_string()\n",
        "        else:\n",
        "            print(self._board_to_string())\n",
        "\n",
        "    def _board_to_string(self):\n",
        "        display = []\n",
        "        for row in self.board:\n",
        "            display_row = []\n",
        "            for exp in row:\n",
        "                if exp == 0:\n",
        "                    display_row.append(\".\")\n",
        "                else:\n",
        "                    display_row.append(str(2 ** int(exp)))\n",
        "            display.append(\"\\t\".join(display_row))\n",
        "        return \"\\n\".join(display)\n"
      ],
      "metadata": {
        "id": "HLLA-2EjlU6p"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from stable_baselines3 import PPO\n",
        "from stable_baselines3.common.env_checker import check_env\n",
        "from stable_baselines3.common.vec_env import DummyVecEnv\n",
        "\n",
        "# We already defined Game2048Env above\n",
        "\n",
        "\n",
        "def make_env():\n",
        "    def _init():\n",
        "        env = Game2048Env()\n",
        "        return env\n",
        "    return _init\n",
        "\n",
        "\n",
        "# Create one env to check API\n",
        "env = Game2048Env()\n",
        "check_env(env, warn=True)\n",
        "\n",
        "# Vectorized env for PPO\n",
        "vec_env = DummyVecEnv([make_env()])\n",
        "\n",
        "model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    vec_env,\n",
        "    verbose=1,\n",
        "    learning_rate=3e-4,\n",
        "    n_steps=2048,\n",
        "    batch_size=256,\n",
        "    gamma=0.99,\n",
        "    gae_lambda=0.95,\n",
        "    clip_range=0.2,\n",
        "    ent_coef=0.01,\n",
        ")\n",
        "\n",
        "# üîÅ Training ‚Äì you can increase timesteps later\n",
        "model.learn(total_timesteps=2000000)\n",
        "\n",
        "model.save(\"ppo_2048\")\n",
        "print(\"Model saved as ppo_2048.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2CAcUx2lnB4",
        "outputId": "fa1e3018-e665-4c37-c8fc-a94ee7d3e57b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "|    total_timesteps      | 1433600      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018848357 |\n",
            "|    clip_fraction        | 0.0085       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.546       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.44e+03     |\n",
            "|    n_updates            | 6990         |\n",
            "|    policy_gradient_loss | -0.00285     |\n",
            "|    value_loss           | 2.21e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 701          |\n",
            "|    time_elapsed         | 2457         |\n",
            "|    total_timesteps      | 1435648      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014606611 |\n",
            "|    clip_fraction        | 0.00464      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.535       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.41e+03     |\n",
            "|    n_updates            | 7000         |\n",
            "|    policy_gradient_loss | -0.00308     |\n",
            "|    value_loss           | 2.35e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 702          |\n",
            "|    time_elapsed         | 2460         |\n",
            "|    total_timesteps      | 1437696      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041113095 |\n",
            "|    clip_fraction        | 0.0421       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.542       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.39e+03     |\n",
            "|    n_updates            | 7010         |\n",
            "|    policy_gradient_loss | -0.005       |\n",
            "|    value_loss           | 2.09e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 703          |\n",
            "|    time_elapsed         | 2464         |\n",
            "|    total_timesteps      | 1439744      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021986933 |\n",
            "|    clip_fraction        | 0.00508      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.575       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.31e+03     |\n",
            "|    n_updates            | 7020         |\n",
            "|    policy_gradient_loss | -0.00291     |\n",
            "|    value_loss           | 2.11e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 704          |\n",
            "|    time_elapsed         | 2467         |\n",
            "|    total_timesteps      | 1441792      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0142816035 |\n",
            "|    clip_fraction        | 0.0625       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.553       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.05e+04     |\n",
            "|    n_updates            | 7030         |\n",
            "|    policy_gradient_loss | -0.00177     |\n",
            "|    value_loss           | 2.3e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 705          |\n",
            "|    time_elapsed         | 2471         |\n",
            "|    total_timesteps      | 1443840      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005339472 |\n",
            "|    clip_fraction        | 0.0019       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.573       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.56e+03     |\n",
            "|    n_updates            | 7040         |\n",
            "|    policy_gradient_loss | -0.00136     |\n",
            "|    value_loss           | 2.26e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 706          |\n",
            "|    time_elapsed         | 2474         |\n",
            "|    total_timesteps      | 1445888      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012651911 |\n",
            "|    clip_fraction        | 0.013        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.571       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1e+04        |\n",
            "|    n_updates            | 7050         |\n",
            "|    policy_gradient_loss | -0.00202     |\n",
            "|    value_loss           | 2.12e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 707         |\n",
            "|    time_elapsed         | 2478        |\n",
            "|    total_timesteps      | 1447936     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002289161 |\n",
            "|    clip_fraction        | 0.0398      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.528      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.07e+04    |\n",
            "|    n_updates            | 7060        |\n",
            "|    policy_gradient_loss | -0.005      |\n",
            "|    value_loss           | 2.32e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 708          |\n",
            "|    time_elapsed         | 2481         |\n",
            "|    total_timesteps      | 1449984      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030338094 |\n",
            "|    clip_fraction        | 0.0302       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.543       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.1e+04      |\n",
            "|    n_updates            | 7070         |\n",
            "|    policy_gradient_loss | -0.00463     |\n",
            "|    value_loss           | 2.19e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 584        |\n",
            "|    iterations           | 709        |\n",
            "|    time_elapsed         | 2485       |\n",
            "|    total_timesteps      | 1452032    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00327591 |\n",
            "|    clip_fraction        | 0.0495     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.516     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 9.65e+03   |\n",
            "|    n_updates            | 7080       |\n",
            "|    policy_gradient_loss | -0.00584   |\n",
            "|    value_loss           | 2.06e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 710          |\n",
            "|    time_elapsed         | 2488         |\n",
            "|    total_timesteps      | 1454080      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070760627 |\n",
            "|    clip_fraction        | 0.106        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.591       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.03e+03     |\n",
            "|    n_updates            | 7090         |\n",
            "|    policy_gradient_loss | -0.000992    |\n",
            "|    value_loss           | 2.12e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 711         |\n",
            "|    time_elapsed         | 2492        |\n",
            "|    total_timesteps      | 1456128     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009745864 |\n",
            "|    clip_fraction        | 0.0456      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.567      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.09e+04    |\n",
            "|    n_updates            | 7100        |\n",
            "|    policy_gradient_loss | -0.00486    |\n",
            "|    value_loss           | 2.1e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 712         |\n",
            "|    time_elapsed         | 2495        |\n",
            "|    total_timesteps      | 1458176     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006391137 |\n",
            "|    clip_fraction        | 0.101       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.566      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.11e+04    |\n",
            "|    n_updates            | 7110        |\n",
            "|    policy_gradient_loss | -0.00509    |\n",
            "|    value_loss           | 2.12e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 713           |\n",
            "|    time_elapsed         | 2499          |\n",
            "|    total_timesteps      | 1460224       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013457594 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.578        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.02e+04      |\n",
            "|    n_updates            | 7120          |\n",
            "|    policy_gradient_loss | -0.00068      |\n",
            "|    value_loss           | 2.17e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 714          |\n",
            "|    time_elapsed         | 2502         |\n",
            "|    total_timesteps      | 1462272      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047863517 |\n",
            "|    clip_fraction        | 0.0396       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.581       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.23e+04     |\n",
            "|    n_updates            | 7130         |\n",
            "|    policy_gradient_loss | -0.00318     |\n",
            "|    value_loss           | 2.15e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 715          |\n",
            "|    time_elapsed         | 2506         |\n",
            "|    total_timesteps      | 1464320      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041372683 |\n",
            "|    clip_fraction        | 0.0464       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.569       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.27e+04     |\n",
            "|    n_updates            | 7140         |\n",
            "|    policy_gradient_loss | -0.00749     |\n",
            "|    value_loss           | 2.29e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 716           |\n",
            "|    time_elapsed         | 2509          |\n",
            "|    total_timesteps      | 1466368       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00066841906 |\n",
            "|    clip_fraction        | 9.77e-05      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.587        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.03e+04      |\n",
            "|    n_updates            | 7150          |\n",
            "|    policy_gradient_loss | -0.00149      |\n",
            "|    value_loss           | 1.98e+04      |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 717         |\n",
            "|    time_elapsed         | 2513        |\n",
            "|    total_timesteps      | 1468416     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003274487 |\n",
            "|    clip_fraction        | 0.0838      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.582      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.16e+04    |\n",
            "|    n_updates            | 7160        |\n",
            "|    policy_gradient_loss | -0.000493   |\n",
            "|    value_loss           | 2.16e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 718         |\n",
            "|    time_elapsed         | 2516        |\n",
            "|    total_timesteps      | 1470464     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008129628 |\n",
            "|    clip_fraction        | 0.0298      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.566      |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.13e+04    |\n",
            "|    n_updates            | 7170        |\n",
            "|    policy_gradient_loss | -0.000301   |\n",
            "|    value_loss           | 2.21e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 719          |\n",
            "|    time_elapsed         | 2520         |\n",
            "|    total_timesteps      | 1472512      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017819582 |\n",
            "|    clip_fraction        | 0.00508      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.549       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.21e+04     |\n",
            "|    n_updates            | 7180         |\n",
            "|    policy_gradient_loss | -0.00315     |\n",
            "|    value_loss           | 1.94e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 720          |\n",
            "|    time_elapsed         | 2523         |\n",
            "|    total_timesteps      | 1474560      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034948282 |\n",
            "|    clip_fraction        | 0.0345       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.578       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.25e+04     |\n",
            "|    n_updates            | 7190         |\n",
            "|    policy_gradient_loss | -0.00426     |\n",
            "|    value_loss           | 2.15e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 721          |\n",
            "|    time_elapsed         | 2527         |\n",
            "|    total_timesteps      | 1476608      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048420234 |\n",
            "|    clip_fraction        | 0.0789       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.579       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.09e+04     |\n",
            "|    n_updates            | 7200         |\n",
            "|    policy_gradient_loss | -0.00347     |\n",
            "|    value_loss           | 2.28e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 722         |\n",
            "|    time_elapsed         | 2530        |\n",
            "|    total_timesteps      | 1478656     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005022879 |\n",
            "|    clip_fraction        | 0.044       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.599      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.24e+04    |\n",
            "|    n_updates            | 7210        |\n",
            "|    policy_gradient_loss | -0.0059     |\n",
            "|    value_loss           | 2.11e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 723         |\n",
            "|    time_elapsed         | 2534        |\n",
            "|    total_timesteps      | 1480704     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004045033 |\n",
            "|    clip_fraction        | 0.0478      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.58       |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.83e+03    |\n",
            "|    n_updates            | 7220        |\n",
            "|    policy_gradient_loss | 0.00118     |\n",
            "|    value_loss           | 2.23e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 724          |\n",
            "|    time_elapsed         | 2537         |\n",
            "|    total_timesteps      | 1482752      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025072414 |\n",
            "|    clip_fraction        | 0.0476       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.604       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.38e+03     |\n",
            "|    n_updates            | 7230         |\n",
            "|    policy_gradient_loss | -0.00429     |\n",
            "|    value_loss           | 1.95e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 725         |\n",
            "|    time_elapsed         | 2541        |\n",
            "|    total_timesteps      | 1484800     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004231872 |\n",
            "|    clip_fraction        | 0.0544      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.611      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.07e+04    |\n",
            "|    n_updates            | 7240        |\n",
            "|    policy_gradient_loss | -0.0017     |\n",
            "|    value_loss           | 1.97e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 726          |\n",
            "|    time_elapsed         | 2544         |\n",
            "|    total_timesteps      | 1486848      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022444143 |\n",
            "|    clip_fraction        | 0.0041       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.645       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.49e+03     |\n",
            "|    n_updates            | 7250         |\n",
            "|    policy_gradient_loss | -0.00229     |\n",
            "|    value_loss           | 2.01e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 727         |\n",
            "|    time_elapsed         | 2548        |\n",
            "|    total_timesteps      | 1488896     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001008624 |\n",
            "|    clip_fraction        | 0.00181     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.605      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.11e+04    |\n",
            "|    n_updates            | 7260        |\n",
            "|    policy_gradient_loss | -0.00313    |\n",
            "|    value_loss           | 2.28e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 728          |\n",
            "|    time_elapsed         | 2551         |\n",
            "|    total_timesteps      | 1490944      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017798685 |\n",
            "|    clip_fraction        | 0.0118       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.602       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.1e+04      |\n",
            "|    n_updates            | 7270         |\n",
            "|    policy_gradient_loss | -0.00375     |\n",
            "|    value_loss           | 1.96e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 729          |\n",
            "|    time_elapsed         | 2555         |\n",
            "|    total_timesteps      | 1492992      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045749666 |\n",
            "|    clip_fraction        | 0.0139       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.63        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.31e+04     |\n",
            "|    n_updates            | 7280         |\n",
            "|    policy_gradient_loss | -0.00383     |\n",
            "|    value_loss           | 2.16e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 730          |\n",
            "|    time_elapsed         | 2558         |\n",
            "|    total_timesteps      | 1495040      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017099647 |\n",
            "|    clip_fraction        | 0.00234      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.596       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.34e+04     |\n",
            "|    n_updates            | 7290         |\n",
            "|    policy_gradient_loss | -0.0028      |\n",
            "|    value_loss           | 2.34e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 731          |\n",
            "|    time_elapsed         | 2562         |\n",
            "|    total_timesteps      | 1497088      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034033027 |\n",
            "|    clip_fraction        | 0.0383       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.592       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.14e+04     |\n",
            "|    n_updates            | 7300         |\n",
            "|    policy_gradient_loss | -0.00323     |\n",
            "|    value_loss           | 2.22e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 732          |\n",
            "|    time_elapsed         | 2565         |\n",
            "|    total_timesteps      | 1499136      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032295599 |\n",
            "|    clip_fraction        | 0.0241       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.626       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.14e+04     |\n",
            "|    n_updates            | 7310         |\n",
            "|    policy_gradient_loss | -0.00578     |\n",
            "|    value_loss           | 2.25e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 733         |\n",
            "|    time_elapsed         | 2569        |\n",
            "|    total_timesteps      | 1501184     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002349459 |\n",
            "|    clip_fraction        | 0.0153      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.605      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.98e+03    |\n",
            "|    n_updates            | 7320        |\n",
            "|    policy_gradient_loss | -0.00236    |\n",
            "|    value_loss           | 2.35e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 734          |\n",
            "|    time_elapsed         | 2572         |\n",
            "|    total_timesteps      | 1503232      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027168705 |\n",
            "|    clip_fraction        | 0.0215       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.636       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.09e+04     |\n",
            "|    n_updates            | 7330         |\n",
            "|    policy_gradient_loss | -0.0013      |\n",
            "|    value_loss           | 2.04e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 735          |\n",
            "|    time_elapsed         | 2576         |\n",
            "|    total_timesteps      | 1505280      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036433353 |\n",
            "|    clip_fraction        | 0.0572       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.578       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.92e+03     |\n",
            "|    n_updates            | 7340         |\n",
            "|    policy_gradient_loss | 0.000495     |\n",
            "|    value_loss           | 2.09e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 736           |\n",
            "|    time_elapsed         | 2579          |\n",
            "|    total_timesteps      | 1507328       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013388088 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.582        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.12e+04      |\n",
            "|    n_updates            | 7350          |\n",
            "|    policy_gradient_loss | -0.000816     |\n",
            "|    value_loss           | 1.96e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 737          |\n",
            "|    time_elapsed         | 2583         |\n",
            "|    total_timesteps      | 1509376      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048487717 |\n",
            "|    clip_fraction        | 0.0161       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.639       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.06e+04     |\n",
            "|    n_updates            | 7360         |\n",
            "|    policy_gradient_loss | -0.00226     |\n",
            "|    value_loss           | 2.06e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 738          |\n",
            "|    time_elapsed         | 2586         |\n",
            "|    total_timesteps      | 1511424      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019840966 |\n",
            "|    clip_fraction        | 0.00391      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.55        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.31e+03     |\n",
            "|    n_updates            | 7370         |\n",
            "|    policy_gradient_loss | -0.00293     |\n",
            "|    value_loss           | 1.87e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 739           |\n",
            "|    time_elapsed         | 2590          |\n",
            "|    total_timesteps      | 1513472       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00056713144 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.608        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.27e+04      |\n",
            "|    n_updates            | 7380          |\n",
            "|    policy_gradient_loss | 5.45e-05      |\n",
            "|    value_loss           | 2.29e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 740           |\n",
            "|    time_elapsed         | 2593          |\n",
            "|    total_timesteps      | 1515520       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00064449874 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.566        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.06e+04      |\n",
            "|    n_updates            | 7390          |\n",
            "|    policy_gradient_loss | -0.000723     |\n",
            "|    value_loss           | 2.24e+04      |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 741         |\n",
            "|    time_elapsed         | 2597        |\n",
            "|    total_timesteps      | 1517568     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009704782 |\n",
            "|    clip_fraction        | 0.0393      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.595      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.96e+03    |\n",
            "|    n_updates            | 7400        |\n",
            "|    policy_gradient_loss | -0.00183    |\n",
            "|    value_loss           | 2.07e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 742           |\n",
            "|    time_elapsed         | 2600          |\n",
            "|    total_timesteps      | 1519616       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00061955734 |\n",
            "|    clip_fraction        | 4.88e-05      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.597        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.06e+04      |\n",
            "|    n_updates            | 7410          |\n",
            "|    policy_gradient_loss | -0.00146      |\n",
            "|    value_loss           | 2.17e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 743           |\n",
            "|    time_elapsed         | 2604          |\n",
            "|    total_timesteps      | 1521664       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00039796458 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.608        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.95e+03      |\n",
            "|    n_updates            | 7420          |\n",
            "|    policy_gradient_loss | -0.00152      |\n",
            "|    value_loss           | 1.94e+04      |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 744         |\n",
            "|    time_elapsed         | 2607        |\n",
            "|    total_timesteps      | 1523712     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010334246 |\n",
            "|    clip_fraction        | 0.0558      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.573      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.45e+03    |\n",
            "|    n_updates            | 7430        |\n",
            "|    policy_gradient_loss | -0.00137    |\n",
            "|    value_loss           | 2.11e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 745          |\n",
            "|    time_elapsed         | 2611         |\n",
            "|    total_timesteps      | 1525760      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024405522 |\n",
            "|    clip_fraction        | 0.0365       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.632       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.05e+03     |\n",
            "|    n_updates            | 7440         |\n",
            "|    policy_gradient_loss | -0.00492     |\n",
            "|    value_loss           | 1.86e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 746          |\n",
            "|    time_elapsed         | 2614         |\n",
            "|    total_timesteps      | 1527808      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035045757 |\n",
            "|    clip_fraction        | 0.0212       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.589       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.23e+04     |\n",
            "|    n_updates            | 7450         |\n",
            "|    policy_gradient_loss | -0.0055      |\n",
            "|    value_loss           | 2.06e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 747          |\n",
            "|    time_elapsed         | 2618         |\n",
            "|    total_timesteps      | 1529856      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028394363 |\n",
            "|    clip_fraction        | 0.0122       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.587       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.31e+04     |\n",
            "|    n_updates            | 7460         |\n",
            "|    policy_gradient_loss | -0.00484     |\n",
            "|    value_loss           | 2.14e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 748          |\n",
            "|    time_elapsed         | 2621         |\n",
            "|    total_timesteps      | 1531904      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026765675 |\n",
            "|    clip_fraction        | 0.0504       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.599       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.15e+04     |\n",
            "|    n_updates            | 7470         |\n",
            "|    policy_gradient_loss | -0.00367     |\n",
            "|    value_loss           | 2.15e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 749          |\n",
            "|    time_elapsed         | 2625         |\n",
            "|    total_timesteps      | 1533952      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027432765 |\n",
            "|    clip_fraction        | 0.0378       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.581       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.58e+03     |\n",
            "|    n_updates            | 7480         |\n",
            "|    policy_gradient_loss | -0.00247     |\n",
            "|    value_loss           | 2.12e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 750          |\n",
            "|    time_elapsed         | 2628         |\n",
            "|    total_timesteps      | 1536000      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026528866 |\n",
            "|    clip_fraction        | 0.0141       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.598       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.12e+04     |\n",
            "|    n_updates            | 7490         |\n",
            "|    policy_gradient_loss | -0.00391     |\n",
            "|    value_loss           | 2.2e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 751          |\n",
            "|    time_elapsed         | 2632         |\n",
            "|    total_timesteps      | 1538048      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039179637 |\n",
            "|    clip_fraction        | 0.0397       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.629       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.2e+04      |\n",
            "|    n_updates            | 7500         |\n",
            "|    policy_gradient_loss | -0.00357     |\n",
            "|    value_loss           | 2.16e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 752         |\n",
            "|    time_elapsed         | 2635        |\n",
            "|    total_timesteps      | 1540096     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004553512 |\n",
            "|    clip_fraction        | 0.0572      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.624      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.58e+03    |\n",
            "|    n_updates            | 7510        |\n",
            "|    policy_gradient_loss | -0.00657    |\n",
            "|    value_loss           | 2.07e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 753         |\n",
            "|    time_elapsed         | 2639        |\n",
            "|    total_timesteps      | 1542144     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007990491 |\n",
            "|    clip_fraction        | 0.0316      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.63       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.06e+04    |\n",
            "|    n_updates            | 7520        |\n",
            "|    policy_gradient_loss | -0.00587    |\n",
            "|    value_loss           | 2.12e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 754         |\n",
            "|    time_elapsed         | 2642        |\n",
            "|    total_timesteps      | 1544192     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004200085 |\n",
            "|    clip_fraction        | 0.0253      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.649      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.07e+04    |\n",
            "|    n_updates            | 7530        |\n",
            "|    policy_gradient_loss | -0.00701    |\n",
            "|    value_loss           | 2.02e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 755           |\n",
            "|    time_elapsed         | 2646          |\n",
            "|    total_timesteps      | 1546240       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00068508624 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.661        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.36e+04      |\n",
            "|    n_updates            | 7540          |\n",
            "|    policy_gradient_loss | -0.000752     |\n",
            "|    value_loss           | 2.2e+04       |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 756          |\n",
            "|    time_elapsed         | 2649         |\n",
            "|    total_timesteps      | 1548288      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015905474 |\n",
            "|    clip_fraction        | 0.0222       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.668       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1e+04        |\n",
            "|    n_updates            | 7550         |\n",
            "|    policy_gradient_loss | -0.00402     |\n",
            "|    value_loss           | 2.08e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 757          |\n",
            "|    time_elapsed         | 2653         |\n",
            "|    total_timesteps      | 1550336      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025158734 |\n",
            "|    clip_fraction        | 0.0122       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.622       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.42e+04     |\n",
            "|    n_updates            | 7560         |\n",
            "|    policy_gradient_loss | -0.00389     |\n",
            "|    value_loss           | 2.41e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 758         |\n",
            "|    time_elapsed         | 2656        |\n",
            "|    total_timesteps      | 1552384     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002355074 |\n",
            "|    clip_fraction        | 0.0216      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.614      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.17e+04    |\n",
            "|    n_updates            | 7570        |\n",
            "|    policy_gradient_loss | -0.00309    |\n",
            "|    value_loss           | 2.27e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 759          |\n",
            "|    time_elapsed         | 2660         |\n",
            "|    total_timesteps      | 1554432      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017874232 |\n",
            "|    clip_fraction        | 0.0265       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.639       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.02e+04     |\n",
            "|    n_updates            | 7580         |\n",
            "|    policy_gradient_loss | -0.00101     |\n",
            "|    value_loss           | 2.15e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 760           |\n",
            "|    time_elapsed         | 2663          |\n",
            "|    total_timesteps      | 1556480       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00041527487 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.63         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.11e+03      |\n",
            "|    n_updates            | 7590          |\n",
            "|    policy_gradient_loss | -0.00152      |\n",
            "|    value_loss           | 2.31e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 761          |\n",
            "|    time_elapsed         | 2667         |\n",
            "|    total_timesteps      | 1558528      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054100878 |\n",
            "|    clip_fraction        | 0.0541       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.606       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.03e+04     |\n",
            "|    n_updates            | 7600         |\n",
            "|    policy_gradient_loss | -0.00599     |\n",
            "|    value_loss           | 2.24e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 762          |\n",
            "|    time_elapsed         | 2670         |\n",
            "|    total_timesteps      | 1560576      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019425538 |\n",
            "|    clip_fraction        | 0.0254       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.634       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.1e+04      |\n",
            "|    n_updates            | 7610         |\n",
            "|    policy_gradient_loss | -0.00333     |\n",
            "|    value_loss           | 2.25e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 763         |\n",
            "|    time_elapsed         | 2674        |\n",
            "|    total_timesteps      | 1562624     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004865854 |\n",
            "|    clip_fraction        | 0.0667      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.614      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.04e+04    |\n",
            "|    n_updates            | 7620        |\n",
            "|    policy_gradient_loss | -0.00206    |\n",
            "|    value_loss           | 2.31e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 764         |\n",
            "|    time_elapsed         | 2677        |\n",
            "|    total_timesteps      | 1564672     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011129973 |\n",
            "|    clip_fraction        | 0.0734      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.64       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.95e+03    |\n",
            "|    n_updates            | 7630        |\n",
            "|    policy_gradient_loss | -0.00518    |\n",
            "|    value_loss           | 2.09e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 765         |\n",
            "|    time_elapsed         | 2681        |\n",
            "|    total_timesteps      | 1566720     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005427701 |\n",
            "|    clip_fraction        | 0.0499      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.653      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.32e+04    |\n",
            "|    n_updates            | 7640        |\n",
            "|    policy_gradient_loss | -0.00128    |\n",
            "|    value_loss           | 2.1e+04     |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 766         |\n",
            "|    time_elapsed         | 2684        |\n",
            "|    total_timesteps      | 1568768     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002543995 |\n",
            "|    clip_fraction        | 0.0191      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.705      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.06e+04    |\n",
            "|    n_updates            | 7650        |\n",
            "|    policy_gradient_loss | -0.00371    |\n",
            "|    value_loss           | 2.09e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 767         |\n",
            "|    time_elapsed         | 2688        |\n",
            "|    total_timesteps      | 1570816     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005742478 |\n",
            "|    clip_fraction        | 0.037       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.671      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.24e+04    |\n",
            "|    n_updates            | 7660        |\n",
            "|    policy_gradient_loss | -0.0033     |\n",
            "|    value_loss           | 2.15e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 768          |\n",
            "|    time_elapsed         | 2691         |\n",
            "|    total_timesteps      | 1572864      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0053549404 |\n",
            "|    clip_fraction        | 0.0299       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.667       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.2e+04      |\n",
            "|    n_updates            | 7670         |\n",
            "|    policy_gradient_loss | -0.00274     |\n",
            "|    value_loss           | 2.28e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 769          |\n",
            "|    time_elapsed         | 2695         |\n",
            "|    total_timesteps      | 1574912      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040193647 |\n",
            "|    clip_fraction        | 0.13         |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.687       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.3e+04      |\n",
            "|    n_updates            | 7680         |\n",
            "|    policy_gradient_loss | 0.00441      |\n",
            "|    value_loss           | 2.18e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 770           |\n",
            "|    time_elapsed         | 2698          |\n",
            "|    total_timesteps      | 1576960       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00030967165 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.689        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.58e+03      |\n",
            "|    n_updates            | 7690          |\n",
            "|    policy_gradient_loss | -0.00107      |\n",
            "|    value_loss           | 2.16e+04      |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 771         |\n",
            "|    time_elapsed         | 2702        |\n",
            "|    total_timesteps      | 1579008     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002771562 |\n",
            "|    clip_fraction        | 0.0169      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.689      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.14e+03    |\n",
            "|    n_updates            | 7700        |\n",
            "|    policy_gradient_loss | -0.00384    |\n",
            "|    value_loss           | 2.2e+04     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 772          |\n",
            "|    time_elapsed         | 2705         |\n",
            "|    total_timesteps      | 1581056      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032728026 |\n",
            "|    clip_fraction        | 0.00835      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.658       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.13e+04     |\n",
            "|    n_updates            | 7710         |\n",
            "|    policy_gradient_loss | -0.00396     |\n",
            "|    value_loss           | 2e+04        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 773          |\n",
            "|    time_elapsed         | 2709         |\n",
            "|    total_timesteps      | 1583104      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054868665 |\n",
            "|    clip_fraction        | 0.0819       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.664       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.18e+04     |\n",
            "|    n_updates            | 7720         |\n",
            "|    policy_gradient_loss | -0.00269     |\n",
            "|    value_loss           | 2.19e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 774          |\n",
            "|    time_elapsed         | 2712         |\n",
            "|    total_timesteps      | 1585152      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0047264406 |\n",
            "|    clip_fraction        | 0.0543       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.645       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.05e+04     |\n",
            "|    n_updates            | 7730         |\n",
            "|    policy_gradient_loss | -0.00689     |\n",
            "|    value_loss           | 2.08e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 775         |\n",
            "|    time_elapsed         | 2716        |\n",
            "|    total_timesteps      | 1587200     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002060165 |\n",
            "|    clip_fraction        | 0.00518     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.646      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.21e+04    |\n",
            "|    n_updates            | 7740        |\n",
            "|    policy_gradient_loss | -0.00305    |\n",
            "|    value_loss           | 2.26e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 776          |\n",
            "|    time_elapsed         | 2719         |\n",
            "|    total_timesteps      | 1589248      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037139233 |\n",
            "|    clip_fraction        | 0.0597       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.657       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.2e+04      |\n",
            "|    n_updates            | 7750         |\n",
            "|    policy_gradient_loss | -0.00417     |\n",
            "|    value_loss           | 2.27e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 777           |\n",
            "|    time_elapsed         | 2723          |\n",
            "|    total_timesteps      | 1591296       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00041206396 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.696        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.12e+04      |\n",
            "|    n_updates            | 7760          |\n",
            "|    policy_gradient_loss | -0.00138      |\n",
            "|    value_loss           | 2.36e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 778          |\n",
            "|    time_elapsed         | 2726         |\n",
            "|    total_timesteps      | 1593344      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004285506 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.666       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.29e+04     |\n",
            "|    n_updates            | 7770         |\n",
            "|    policy_gradient_loss | -0.00122     |\n",
            "|    value_loss           | 2.29e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 779          |\n",
            "|    time_elapsed         | 2730         |\n",
            "|    total_timesteps      | 1595392      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0084744375 |\n",
            "|    clip_fraction        | 0.0358       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.661       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.06e+04     |\n",
            "|    n_updates            | 7780         |\n",
            "|    policy_gradient_loss | -0.00201     |\n",
            "|    value_loss           | 2.25e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 780           |\n",
            "|    time_elapsed         | 2733          |\n",
            "|    total_timesteps      | 1597440       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00064548914 |\n",
            "|    clip_fraction        | 0.000391      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.691        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.55e+03      |\n",
            "|    n_updates            | 7790          |\n",
            "|    policy_gradient_loss | -0.00166      |\n",
            "|    value_loss           | 2.11e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 781          |\n",
            "|    time_elapsed         | 2737         |\n",
            "|    total_timesteps      | 1599488      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040137926 |\n",
            "|    clip_fraction        | 0.126        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.688       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.21e+04     |\n",
            "|    n_updates            | 7800         |\n",
            "|    policy_gradient_loss | 0.00189      |\n",
            "|    value_loss           | 2.35e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 782          |\n",
            "|    time_elapsed         | 2740         |\n",
            "|    total_timesteps      | 1601536      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010040128 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.679       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.24e+04     |\n",
            "|    n_updates            | 7810         |\n",
            "|    policy_gradient_loss | -0.00233     |\n",
            "|    value_loss           | 2.45e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 783          |\n",
            "|    time_elapsed         | 2744         |\n",
            "|    total_timesteps      | 1603584      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033600698 |\n",
            "|    clip_fraction        | 0.0159       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.678       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.27e+04     |\n",
            "|    n_updates            | 7820         |\n",
            "|    policy_gradient_loss | -0.00261     |\n",
            "|    value_loss           | 2.33e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 784          |\n",
            "|    time_elapsed         | 2747         |\n",
            "|    total_timesteps      | 1605632      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006821725 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.679       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.08e+04     |\n",
            "|    n_updates            | 7830         |\n",
            "|    policy_gradient_loss | -0.00277     |\n",
            "|    value_loss           | 2.26e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 785          |\n",
            "|    time_elapsed         | 2751         |\n",
            "|    total_timesteps      | 1607680      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039479984 |\n",
            "|    clip_fraction        | 0.00957      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.654       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.24e+04     |\n",
            "|    n_updates            | 7840         |\n",
            "|    policy_gradient_loss | -0.00187     |\n",
            "|    value_loss           | 2.32e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 786          |\n",
            "|    time_elapsed         | 2754         |\n",
            "|    total_timesteps      | 1609728      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022132332 |\n",
            "|    clip_fraction        | 0.0208       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.66        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.04e+04     |\n",
            "|    n_updates            | 7850         |\n",
            "|    policy_gradient_loss | -0.00716     |\n",
            "|    value_loss           | 2.28e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 787         |\n",
            "|    time_elapsed         | 2758        |\n",
            "|    total_timesteps      | 1611776     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014541523 |\n",
            "|    clip_fraction        | 0.0449      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.688      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.14e+04    |\n",
            "|    n_updates            | 7860        |\n",
            "|    policy_gradient_loss | -0.00013    |\n",
            "|    value_loss           | 2.36e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 788         |\n",
            "|    time_elapsed         | 2761        |\n",
            "|    total_timesteps      | 1613824     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002753946 |\n",
            "|    clip_fraction        | 0.00933     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.716      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.17e+04    |\n",
            "|    n_updates            | 7870        |\n",
            "|    policy_gradient_loss | -0.00321    |\n",
            "|    value_loss           | 2.11e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 789          |\n",
            "|    time_elapsed         | 2765         |\n",
            "|    total_timesteps      | 1615872      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035805213 |\n",
            "|    clip_fraction        | 0.0663       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.686       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.14e+04     |\n",
            "|    n_updates            | 7880         |\n",
            "|    policy_gradient_loss | -0.00263     |\n",
            "|    value_loss           | 2.12e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 790         |\n",
            "|    time_elapsed         | 2768        |\n",
            "|    total_timesteps      | 1617920     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003587109 |\n",
            "|    clip_fraction        | 0.0246      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.714      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.21e+04    |\n",
            "|    n_updates            | 7890        |\n",
            "|    policy_gradient_loss | -0.00493    |\n",
            "|    value_loss           | 2.51e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 791         |\n",
            "|    time_elapsed         | 2772        |\n",
            "|    total_timesteps      | 1619968     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004243497 |\n",
            "|    clip_fraction        | 0.0529      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.668      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.02e+04    |\n",
            "|    n_updates            | 7900        |\n",
            "|    policy_gradient_loss | -0.00809    |\n",
            "|    value_loss           | 2.01e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 792          |\n",
            "|    time_elapsed         | 2775         |\n",
            "|    total_timesteps      | 1622016      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076044113 |\n",
            "|    clip_fraction        | 0.0546       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.692       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.78e+03     |\n",
            "|    n_updates            | 7910         |\n",
            "|    policy_gradient_loss | -0.00725     |\n",
            "|    value_loss           | 2.32e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 793         |\n",
            "|    time_elapsed         | 2779        |\n",
            "|    total_timesteps      | 1624064     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008353002 |\n",
            "|    clip_fraction        | 0.139       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.704      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.13e+04    |\n",
            "|    n_updates            | 7920        |\n",
            "|    policy_gradient_loss | -0.00362    |\n",
            "|    value_loss           | 2.05e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 794          |\n",
            "|    time_elapsed         | 2782         |\n",
            "|    total_timesteps      | 1626112      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009676294 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.659       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.33e+04     |\n",
            "|    n_updates            | 7930         |\n",
            "|    policy_gradient_loss | -0.0014      |\n",
            "|    value_loss           | 2.32e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 795          |\n",
            "|    time_elapsed         | 2786         |\n",
            "|    total_timesteps      | 1628160      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023131487 |\n",
            "|    clip_fraction        | 0.0155       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.679       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.98e+03     |\n",
            "|    n_updates            | 7940         |\n",
            "|    policy_gradient_loss | -0.00368     |\n",
            "|    value_loss           | 2.28e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 796           |\n",
            "|    time_elapsed         | 2789          |\n",
            "|    total_timesteps      | 1630208       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00058861944 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.667        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1e+04         |\n",
            "|    n_updates            | 7950          |\n",
            "|    policy_gradient_loss | -0.0016       |\n",
            "|    value_loss           | 2.39e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 797          |\n",
            "|    time_elapsed         | 2793         |\n",
            "|    total_timesteps      | 1632256      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025542774 |\n",
            "|    clip_fraction        | 0.00293      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.698       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.2e+04      |\n",
            "|    n_updates            | 7960         |\n",
            "|    policy_gradient_loss | -0.00276     |\n",
            "|    value_loss           | 1.96e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 798         |\n",
            "|    time_elapsed         | 2796        |\n",
            "|    total_timesteps      | 1634304     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004413519 |\n",
            "|    clip_fraction        | 0.0128      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.635      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.89e+03    |\n",
            "|    n_updates            | 7970        |\n",
            "|    policy_gradient_loss | -0.00312    |\n",
            "|    value_loss           | 2.36e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 799          |\n",
            "|    time_elapsed         | 2800         |\n",
            "|    total_timesteps      | 1636352      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023679843 |\n",
            "|    clip_fraction        | 0.0137       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.687       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.41e+03     |\n",
            "|    n_updates            | 7980         |\n",
            "|    policy_gradient_loss | -0.0035      |\n",
            "|    value_loss           | 1.83e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 800         |\n",
            "|    time_elapsed         | 2803        |\n",
            "|    total_timesteps      | 1638400     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006788017 |\n",
            "|    clip_fraction        | 0.0545      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.684      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.68e+03    |\n",
            "|    n_updates            | 7990        |\n",
            "|    policy_gradient_loss | -0.00251    |\n",
            "|    value_loss           | 2.1e+04     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 801          |\n",
            "|    time_elapsed         | 2807         |\n",
            "|    total_timesteps      | 1640448      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0056396006 |\n",
            "|    clip_fraction        | 0.104        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.655       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.22e+04     |\n",
            "|    n_updates            | 8000         |\n",
            "|    policy_gradient_loss | 0.000868     |\n",
            "|    value_loss           | 2.15e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 802         |\n",
            "|    time_elapsed         | 2810        |\n",
            "|    total_timesteps      | 1642496     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011893575 |\n",
            "|    clip_fraction        | 0.0575      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.688      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.17e+04    |\n",
            "|    n_updates            | 8010        |\n",
            "|    policy_gradient_loss | 0.000801    |\n",
            "|    value_loss           | 2.04e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 803           |\n",
            "|    time_elapsed         | 2814          |\n",
            "|    total_timesteps      | 1644544       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00081340654 |\n",
            "|    clip_fraction        | 0.000195      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.629        |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.79e+03      |\n",
            "|    n_updates            | 8020          |\n",
            "|    policy_gradient_loss | -0.000109     |\n",
            "|    value_loss           | 2.08e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 804           |\n",
            "|    time_elapsed         | 2817          |\n",
            "|    total_timesteps      | 1646592       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00040615065 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.7          |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.25e+04      |\n",
            "|    n_updates            | 8030          |\n",
            "|    policy_gradient_loss | -0.00104      |\n",
            "|    value_loss           | 2.14e+04      |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 805         |\n",
            "|    time_elapsed         | 2821        |\n",
            "|    total_timesteps      | 1648640     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.014034481 |\n",
            "|    clip_fraction        | 0.0189      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.629      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.21e+03    |\n",
            "|    n_updates            | 8040        |\n",
            "|    policy_gradient_loss | -0.00024    |\n",
            "|    value_loss           | 2.14e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 806          |\n",
            "|    time_elapsed         | 2824         |\n",
            "|    total_timesteps      | 1650688      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0060434565 |\n",
            "|    clip_fraction        | 0.082        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.623       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.09e+04     |\n",
            "|    n_updates            | 8050         |\n",
            "|    policy_gradient_loss | -0.000784    |\n",
            "|    value_loss           | 2.19e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 807         |\n",
            "|    time_elapsed         | 2828        |\n",
            "|    total_timesteps      | 1652736     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.017433658 |\n",
            "|    clip_fraction        | 0.0951      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.701      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.18e+04    |\n",
            "|    n_updates            | 8060        |\n",
            "|    policy_gradient_loss | -0.00243    |\n",
            "|    value_loss           | 2.11e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 808          |\n",
            "|    time_elapsed         | 2831         |\n",
            "|    total_timesteps      | 1654784      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011227427 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.695       |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.24e+03     |\n",
            "|    n_updates            | 8070         |\n",
            "|    policy_gradient_loss | -0.0014      |\n",
            "|    value_loss           | 2e+04        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 809          |\n",
            "|    time_elapsed         | 2835         |\n",
            "|    total_timesteps      | 1656832      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070534423 |\n",
            "|    clip_fraction        | 0.0535       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.687       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.24e+04     |\n",
            "|    n_updates            | 8080         |\n",
            "|    policy_gradient_loss | -0.00454     |\n",
            "|    value_loss           | 2.21e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 810          |\n",
            "|    time_elapsed         | 2838         |\n",
            "|    total_timesteps      | 1658880      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012350093 |\n",
            "|    clip_fraction        | 0.00195      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.637       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.69e+03     |\n",
            "|    n_updates            | 8090         |\n",
            "|    policy_gradient_loss | -0.00285     |\n",
            "|    value_loss           | 2.22e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 811          |\n",
            "|    time_elapsed         | 2842         |\n",
            "|    total_timesteps      | 1660928      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038049982 |\n",
            "|    clip_fraction        | 0.0277       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.674       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.08e+04     |\n",
            "|    n_updates            | 8100         |\n",
            "|    policy_gradient_loss | -0.00666     |\n",
            "|    value_loss           | 2.36e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 812          |\n",
            "|    time_elapsed         | 2845         |\n",
            "|    total_timesteps      | 1662976      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.380918e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.658       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.13e+03     |\n",
            "|    n_updates            | 8110         |\n",
            "|    policy_gradient_loss | -0.000554    |\n",
            "|    value_loss           | 1.96e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 813          |\n",
            "|    time_elapsed         | 2849         |\n",
            "|    total_timesteps      | 1665024      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034463047 |\n",
            "|    clip_fraction        | 0.00991      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.594       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.23e+04     |\n",
            "|    n_updates            | 8120         |\n",
            "|    policy_gradient_loss | -0.00628     |\n",
            "|    value_loss           | 2.22e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 814          |\n",
            "|    time_elapsed         | 2852         |\n",
            "|    total_timesteps      | 1667072      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070475517 |\n",
            "|    clip_fraction        | 0.0419       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.629       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.87e+03     |\n",
            "|    n_updates            | 8130         |\n",
            "|    policy_gradient_loss | -0.00482     |\n",
            "|    value_loss           | 2.19e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 815          |\n",
            "|    time_elapsed         | 2856         |\n",
            "|    total_timesteps      | 1669120      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0076221535 |\n",
            "|    clip_fraction        | 0.0286       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.641       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.15e+04     |\n",
            "|    n_updates            | 8140         |\n",
            "|    policy_gradient_loss | -0.00279     |\n",
            "|    value_loss           | 2.34e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 816          |\n",
            "|    time_elapsed         | 2859         |\n",
            "|    total_timesteps      | 1671168      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025657215 |\n",
            "|    clip_fraction        | 0.00713      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.608       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.01e+04     |\n",
            "|    n_updates            | 8150         |\n",
            "|    policy_gradient_loss | -0.00615     |\n",
            "|    value_loss           | 2.29e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 817          |\n",
            "|    time_elapsed         | 2863         |\n",
            "|    total_timesteps      | 1673216      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038051638 |\n",
            "|    clip_fraction        | 0.0349       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.614       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.25e+04     |\n",
            "|    n_updates            | 8160         |\n",
            "|    policy_gradient_loss | -0.00604     |\n",
            "|    value_loss           | 2.26e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 818         |\n",
            "|    time_elapsed         | 2866        |\n",
            "|    total_timesteps      | 1675264     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 5.47065e-05 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.642      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.25e+04    |\n",
            "|    n_updates            | 8170        |\n",
            "|    policy_gradient_loss | -8.46e-05   |\n",
            "|    value_loss           | 2.21e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 819         |\n",
            "|    time_elapsed         | 2870        |\n",
            "|    total_timesteps      | 1677312     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008962976 |\n",
            "|    clip_fraction        | 0.0456      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.575      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.06e+04    |\n",
            "|    n_updates            | 8180        |\n",
            "|    policy_gradient_loss | -0.00262    |\n",
            "|    value_loss           | 1.94e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 820          |\n",
            "|    time_elapsed         | 2873         |\n",
            "|    total_timesteps      | 1679360      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0064172237 |\n",
            "|    clip_fraction        | 0.0347       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.598       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.07e+04     |\n",
            "|    n_updates            | 8190         |\n",
            "|    policy_gradient_loss | -0.00284     |\n",
            "|    value_loss           | 2.3e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 821          |\n",
            "|    time_elapsed         | 2877         |\n",
            "|    total_timesteps      | 1681408      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035843118 |\n",
            "|    clip_fraction        | 0.0303       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.643       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.1e+04      |\n",
            "|    n_updates            | 8200         |\n",
            "|    policy_gradient_loss | -0.00377     |\n",
            "|    value_loss           | 2.31e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 822           |\n",
            "|    time_elapsed         | 2880          |\n",
            "|    total_timesteps      | 1683456       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021075865 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.679        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.07e+03      |\n",
            "|    n_updates            | 8210          |\n",
            "|    policy_gradient_loss | -0.000785     |\n",
            "|    value_loss           | 2.11e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 823          |\n",
            "|    time_elapsed         | 2884         |\n",
            "|    total_timesteps      | 1685504      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020619174 |\n",
            "|    clip_fraction        | 0.00991      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.651       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.43e+03     |\n",
            "|    n_updates            | 8220         |\n",
            "|    policy_gradient_loss | -0.00416     |\n",
            "|    value_loss           | 2.15e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 824          |\n",
            "|    time_elapsed         | 2888         |\n",
            "|    total_timesteps      | 1687552      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0045072655 |\n",
            "|    clip_fraction        | 0.0126       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.641       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.92e+03     |\n",
            "|    n_updates            | 8230         |\n",
            "|    policy_gradient_loss | -0.0031      |\n",
            "|    value_loss           | 2.19e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 825          |\n",
            "|    time_elapsed         | 2891         |\n",
            "|    total_timesteps      | 1689600      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022135333 |\n",
            "|    clip_fraction        | 0.0108       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.617       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.22e+04     |\n",
            "|    n_updates            | 8240         |\n",
            "|    policy_gradient_loss | -0.00436     |\n",
            "|    value_loss           | 2.36e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 826          |\n",
            "|    time_elapsed         | 2895         |\n",
            "|    total_timesteps      | 1691648      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020640919 |\n",
            "|    clip_fraction        | 0.0134       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.625       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.02e+04     |\n",
            "|    n_updates            | 8250         |\n",
            "|    policy_gradient_loss | -0.00485     |\n",
            "|    value_loss           | 2.3e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 827          |\n",
            "|    time_elapsed         | 2898         |\n",
            "|    total_timesteps      | 1693696      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0068116845 |\n",
            "|    clip_fraction        | 0.0386       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.628       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.08e+04     |\n",
            "|    n_updates            | 8260         |\n",
            "|    policy_gradient_loss | -0.0015      |\n",
            "|    value_loss           | 2.16e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 584        |\n",
            "|    iterations           | 828        |\n",
            "|    time_elapsed         | 2902       |\n",
            "|    total_timesteps      | 1695744    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02011433 |\n",
            "|    clip_fraction        | 0.0806     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.594     |\n",
            "|    explained_variance   | 5.96e-08   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.19e+04   |\n",
            "|    n_updates            | 8270       |\n",
            "|    policy_gradient_loss | 0.00022    |\n",
            "|    value_loss           | 2.33e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 829         |\n",
            "|    time_elapsed         | 2905        |\n",
            "|    total_timesteps      | 1697792     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008523356 |\n",
            "|    clip_fraction        | 0.0293      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.592      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.07e+04    |\n",
            "|    n_updates            | 8280        |\n",
            "|    policy_gradient_loss | 0.000696    |\n",
            "|    value_loss           | 2.1e+04     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 830          |\n",
            "|    time_elapsed         | 2909         |\n",
            "|    total_timesteps      | 1699840      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010035076 |\n",
            "|    clip_fraction        | 0.00415      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.596       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.81e+03     |\n",
            "|    n_updates            | 8290         |\n",
            "|    policy_gradient_loss | -0.00331     |\n",
            "|    value_loss           | 2.11e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 584        |\n",
            "|    iterations           | 831        |\n",
            "|    time_elapsed         | 2912       |\n",
            "|    total_timesteps      | 1701888    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00300392 |\n",
            "|    clip_fraction        | 0.079      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.557     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.15e+04   |\n",
            "|    n_updates            | 8300       |\n",
            "|    policy_gradient_loss | 0.000916   |\n",
            "|    value_loss           | 2e+04      |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 832          |\n",
            "|    time_elapsed         | 2916         |\n",
            "|    total_timesteps      | 1703936      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0059031607 |\n",
            "|    clip_fraction        | 0.0697       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.587       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.04e+04     |\n",
            "|    n_updates            | 8310         |\n",
            "|    policy_gradient_loss | -0.00228     |\n",
            "|    value_loss           | 1.94e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 833          |\n",
            "|    time_elapsed         | 2919         |\n",
            "|    total_timesteps      | 1705984      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001139449 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.574       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.27e+04     |\n",
            "|    n_updates            | 8320         |\n",
            "|    policy_gradient_loss | -6.21e-05    |\n",
            "|    value_loss           | 2.24e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 834          |\n",
            "|    time_elapsed         | 2923         |\n",
            "|    total_timesteps      | 1708032      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038337943 |\n",
            "|    clip_fraction        | 0.0479       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.595       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.48e+03     |\n",
            "|    n_updates            | 8330         |\n",
            "|    policy_gradient_loss | -0.00107     |\n",
            "|    value_loss           | 1.94e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 835          |\n",
            "|    time_elapsed         | 2926         |\n",
            "|    total_timesteps      | 1710080      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002514322 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.606       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.08e+03     |\n",
            "|    n_updates            | 8340         |\n",
            "|    policy_gradient_loss | -0.000615    |\n",
            "|    value_loss           | 2.06e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 836           |\n",
            "|    time_elapsed         | 2930          |\n",
            "|    total_timesteps      | 1712128       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010154102 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.593        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.2e+04       |\n",
            "|    n_updates            | 8350          |\n",
            "|    policy_gradient_loss | -0.000927     |\n",
            "|    value_loss           | 2.12e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 837           |\n",
            "|    time_elapsed         | 2933          |\n",
            "|    total_timesteps      | 1714176       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.7345704e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.573        |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.28e+04      |\n",
            "|    n_updates            | 8360          |\n",
            "|    policy_gradient_loss | -0.000397     |\n",
            "|    value_loss           | 2.2e+04       |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 838         |\n",
            "|    time_elapsed         | 2937        |\n",
            "|    total_timesteps      | 1716224     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002189909 |\n",
            "|    clip_fraction        | 0.0181      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.62       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9e+03       |\n",
            "|    n_updates            | 8370        |\n",
            "|    policy_gradient_loss | -0.00516    |\n",
            "|    value_loss           | 1.96e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 839          |\n",
            "|    time_elapsed         | 2940         |\n",
            "|    total_timesteps      | 1718272      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020003908 |\n",
            "|    clip_fraction        | 0.0103       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.626       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.59e+03     |\n",
            "|    n_updates            | 8380         |\n",
            "|    policy_gradient_loss | -0.00183     |\n",
            "|    value_loss           | 1.9e+04      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 840         |\n",
            "|    time_elapsed         | 2944        |\n",
            "|    total_timesteps      | 1720320     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.016024493 |\n",
            "|    clip_fraction        | 0.0469      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.594      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.14e+04    |\n",
            "|    n_updates            | 8390        |\n",
            "|    policy_gradient_loss | -0.000917   |\n",
            "|    value_loss           | 2.23e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 841          |\n",
            "|    time_elapsed         | 2947         |\n",
            "|    total_timesteps      | 1722368      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022987616 |\n",
            "|    clip_fraction        | 0.00679      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.607       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.64e+03     |\n",
            "|    n_updates            | 8400         |\n",
            "|    policy_gradient_loss | -0.00226     |\n",
            "|    value_loss           | 1.81e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 842         |\n",
            "|    time_elapsed         | 2951        |\n",
            "|    total_timesteps      | 1724416     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005760012 |\n",
            "|    clip_fraction        | 0.0574      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.635      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.96e+03    |\n",
            "|    n_updates            | 8410        |\n",
            "|    policy_gradient_loss | -0.000887   |\n",
            "|    value_loss           | 1.96e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 843          |\n",
            "|    time_elapsed         | 2954         |\n",
            "|    total_timesteps      | 1726464      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025172443 |\n",
            "|    clip_fraction        | 0.00664      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.627       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.01e+04     |\n",
            "|    n_updates            | 8420         |\n",
            "|    policy_gradient_loss | -0.00261     |\n",
            "|    value_loss           | 2.15e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 844          |\n",
            "|    time_elapsed         | 2958         |\n",
            "|    total_timesteps      | 1728512      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057669664 |\n",
            "|    clip_fraction        | 0.049        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.64        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.03e+03     |\n",
            "|    n_updates            | 8430         |\n",
            "|    policy_gradient_loss | -0.00553     |\n",
            "|    value_loss           | 2.02e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 845          |\n",
            "|    time_elapsed         | 2961         |\n",
            "|    total_timesteps      | 1730560      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032844213 |\n",
            "|    clip_fraction        | 0.0356       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.638       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.94e+03     |\n",
            "|    n_updates            | 8440         |\n",
            "|    policy_gradient_loss | -0.00485     |\n",
            "|    value_loss           | 1.98e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 846          |\n",
            "|    time_elapsed         | 2965         |\n",
            "|    total_timesteps      | 1732608      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033174888 |\n",
            "|    clip_fraction        | 0.0249       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.645       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1e+04        |\n",
            "|    n_updates            | 8450         |\n",
            "|    policy_gradient_loss | -0.00381     |\n",
            "|    value_loss           | 2e+04        |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 847         |\n",
            "|    time_elapsed         | 2968        |\n",
            "|    total_timesteps      | 1734656     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004799539 |\n",
            "|    clip_fraction        | 0.039       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.643      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.34e+03    |\n",
            "|    n_updates            | 8460        |\n",
            "|    policy_gradient_loss | -0.00653    |\n",
            "|    value_loss           | 1.94e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 848           |\n",
            "|    time_elapsed         | 2972          |\n",
            "|    total_timesteps      | 1736704       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010448575 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.627        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.32e+03      |\n",
            "|    n_updates            | 8470          |\n",
            "|    policy_gradient_loss | -0.000483     |\n",
            "|    value_loss           | 2.08e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 849          |\n",
            "|    time_elapsed         | 2975         |\n",
            "|    total_timesteps      | 1738752      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002568437 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.642       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.77e+03     |\n",
            "|    n_updates            | 8480         |\n",
            "|    policy_gradient_loss | -0.000941    |\n",
            "|    value_loss           | 2.03e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 850          |\n",
            "|    time_elapsed         | 2979         |\n",
            "|    total_timesteps      | 1740800      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054903133 |\n",
            "|    clip_fraction        | 0.0697       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.642       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.31e+03     |\n",
            "|    n_updates            | 8490         |\n",
            "|    policy_gradient_loss | -0.00347     |\n",
            "|    value_loss           | 1.8e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 851          |\n",
            "|    time_elapsed         | 2982         |\n",
            "|    total_timesteps      | 1742848      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0052282955 |\n",
            "|    clip_fraction        | 0.0544       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.664       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.37e+03     |\n",
            "|    n_updates            | 8500         |\n",
            "|    policy_gradient_loss | -0.002       |\n",
            "|    value_loss           | 2.03e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 852          |\n",
            "|    time_elapsed         | 2986         |\n",
            "|    total_timesteps      | 1744896      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009842045 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.649       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.34e+03     |\n",
            "|    n_updates            | 8510         |\n",
            "|    policy_gradient_loss | -0.00103     |\n",
            "|    value_loss           | 1.83e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 853         |\n",
            "|    time_elapsed         | 2989        |\n",
            "|    total_timesteps      | 1746944     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004211205 |\n",
            "|    clip_fraction        | 0.0424      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.653      |\n",
            "|    explained_variance   | -2.38e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.67e+03    |\n",
            "|    n_updates            | 8520        |\n",
            "|    policy_gradient_loss | -0.00366    |\n",
            "|    value_loss           | 1.77e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 854          |\n",
            "|    time_elapsed         | 2993         |\n",
            "|    total_timesteps      | 1748992      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023256857 |\n",
            "|    clip_fraction        | 0.00684      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.654       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.69e+03     |\n",
            "|    n_updates            | 8530         |\n",
            "|    policy_gradient_loss | -0.00292     |\n",
            "|    value_loss           | 1.85e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 584        |\n",
            "|    iterations           | 855        |\n",
            "|    time_elapsed         | 2996       |\n",
            "|    total_timesteps      | 1751040    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00670239 |\n",
            "|    clip_fraction        | 0.00947    |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.642     |\n",
            "|    explained_variance   | 1.19e-07   |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 9.72e+03   |\n",
            "|    n_updates            | 8540       |\n",
            "|    policy_gradient_loss | -0.00374   |\n",
            "|    value_loss           | 2.03e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 856         |\n",
            "|    time_elapsed         | 3000        |\n",
            "|    total_timesteps      | 1753088     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005245576 |\n",
            "|    clip_fraction        | 0.113       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.714      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.8e+03     |\n",
            "|    n_updates            | 8550        |\n",
            "|    policy_gradient_loss | -0.00397    |\n",
            "|    value_loss           | 1.96e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 857          |\n",
            "|    time_elapsed         | 3003         |\n",
            "|    total_timesteps      | 1755136      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022572167 |\n",
            "|    clip_fraction        | 0.00845      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.644       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.01e+04     |\n",
            "|    n_updates            | 8560         |\n",
            "|    policy_gradient_loss | -0.00283     |\n",
            "|    value_loss           | 2.15e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 858          |\n",
            "|    time_elapsed         | 3007         |\n",
            "|    total_timesteps      | 1757184      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0057522077 |\n",
            "|    clip_fraction        | 0.0529       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.704       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.06e+03     |\n",
            "|    n_updates            | 8570         |\n",
            "|    policy_gradient_loss | -0.00641     |\n",
            "|    value_loss           | 1.64e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 859         |\n",
            "|    time_elapsed         | 3010        |\n",
            "|    total_timesteps      | 1759232     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001970725 |\n",
            "|    clip_fraction        | 0.0216      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.674      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.1e+03     |\n",
            "|    n_updates            | 8580        |\n",
            "|    policy_gradient_loss | 0.00196     |\n",
            "|    value_loss           | 2.04e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 860          |\n",
            "|    time_elapsed         | 3014         |\n",
            "|    total_timesteps      | 1761280      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026996043 |\n",
            "|    clip_fraction        | 0.0281       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.676       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.99e+03     |\n",
            "|    n_updates            | 8590         |\n",
            "|    policy_gradient_loss | -0.00352     |\n",
            "|    value_loss           | 1.81e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 861          |\n",
            "|    time_elapsed         | 3017         |\n",
            "|    total_timesteps      | 1763328      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031436053 |\n",
            "|    clip_fraction        | 0.0705       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.654       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.41e+03     |\n",
            "|    n_updates            | 8600         |\n",
            "|    policy_gradient_loss | -0.00108     |\n",
            "|    value_loss           | 2e+04        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 862          |\n",
            "|    time_elapsed         | 3021         |\n",
            "|    total_timesteps      | 1765376      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008574588 |\n",
            "|    clip_fraction        | 0.000244     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.662       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.13e+03     |\n",
            "|    n_updates            | 8610         |\n",
            "|    policy_gradient_loss | -0.00154     |\n",
            "|    value_loss           | 1.83e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 863          |\n",
            "|    time_elapsed         | 3024         |\n",
            "|    total_timesteps      | 1767424      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0040714596 |\n",
            "|    clip_fraction        | 0.0524       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.701       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.03e+04     |\n",
            "|    n_updates            | 8620         |\n",
            "|    policy_gradient_loss | -0.0064      |\n",
            "|    value_loss           | 1.9e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 864          |\n",
            "|    time_elapsed         | 3028         |\n",
            "|    total_timesteps      | 1769472      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017724993 |\n",
            "|    clip_fraction        | 0.00396      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.659       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.99e+03     |\n",
            "|    n_updates            | 8630         |\n",
            "|    policy_gradient_loss | -0.00309     |\n",
            "|    value_loss           | 1.96e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 865          |\n",
            "|    time_elapsed         | 3031         |\n",
            "|    total_timesteps      | 1771520      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026663125 |\n",
            "|    clip_fraction        | 0.00269      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.725       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.09e+04     |\n",
            "|    n_updates            | 8640         |\n",
            "|    policy_gradient_loss | -0.00266     |\n",
            "|    value_loss           | 1.95e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 866          |\n",
            "|    time_elapsed         | 3035         |\n",
            "|    total_timesteps      | 1773568      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024955273 |\n",
            "|    clip_fraction        | 0.0235       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.693       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.85e+03     |\n",
            "|    n_updates            | 8650         |\n",
            "|    policy_gradient_loss | -0.00424     |\n",
            "|    value_loss           | 1.77e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 867          |\n",
            "|    time_elapsed         | 3038         |\n",
            "|    total_timesteps      | 1775616      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0048683435 |\n",
            "|    clip_fraction        | 0.0183       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.631       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.13e+03     |\n",
            "|    n_updates            | 8660         |\n",
            "|    policy_gradient_loss | -0.00566     |\n",
            "|    value_loss           | 2.03e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 868          |\n",
            "|    time_elapsed         | 3042         |\n",
            "|    total_timesteps      | 1777664      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029848646 |\n",
            "|    clip_fraction        | 0.00649      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.664       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.07e+04     |\n",
            "|    n_updates            | 8670         |\n",
            "|    policy_gradient_loss | -0.00159     |\n",
            "|    value_loss           | 1.94e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 869          |\n",
            "|    time_elapsed         | 3045         |\n",
            "|    total_timesteps      | 1779712      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.136816e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.629       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.33e+03     |\n",
            "|    n_updates            | 8680         |\n",
            "|    policy_gradient_loss | -0.00056     |\n",
            "|    value_loss           | 2.1e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 870          |\n",
            "|    time_elapsed         | 3049         |\n",
            "|    total_timesteps      | 1781760      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014212246 |\n",
            "|    clip_fraction        | 0.00195      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.642       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.01e+04     |\n",
            "|    n_updates            | 8690         |\n",
            "|    policy_gradient_loss | -0.00287     |\n",
            "|    value_loss           | 1.84e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 871         |\n",
            "|    time_elapsed         | 3052        |\n",
            "|    total_timesteps      | 1783808     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011714271 |\n",
            "|    clip_fraction        | 0.0416      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.705      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.73e+03    |\n",
            "|    n_updates            | 8700        |\n",
            "|    policy_gradient_loss | -0.00379    |\n",
            "|    value_loss           | 1.72e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 872          |\n",
            "|    time_elapsed         | 3056         |\n",
            "|    total_timesteps      | 1785856      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025732294 |\n",
            "|    clip_fraction        | 0.0757       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.622       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.04e+04     |\n",
            "|    n_updates            | 8710         |\n",
            "|    policy_gradient_loss | -0.00367     |\n",
            "|    value_loss           | 2.06e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 584        |\n",
            "|    iterations           | 873        |\n",
            "|    time_elapsed         | 3059       |\n",
            "|    total_timesteps      | 1787904    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.01812575 |\n",
            "|    clip_fraction        | 0.127      |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.633     |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.06e+04   |\n",
            "|    n_updates            | 8720       |\n",
            "|    policy_gradient_loss | 0.00364    |\n",
            "|    value_loss           | 2.12e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 874          |\n",
            "|    time_elapsed         | 3063         |\n",
            "|    total_timesteps      | 1789952      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020404682 |\n",
            "|    clip_fraction        | 0.00435      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.64        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.01e+04     |\n",
            "|    n_updates            | 8730         |\n",
            "|    policy_gradient_loss | -0.00156     |\n",
            "|    value_loss           | 2.17e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 875           |\n",
            "|    time_elapsed         | 3066          |\n",
            "|    total_timesteps      | 1792000       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00024533545 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.619        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.1e+04       |\n",
            "|    n_updates            | 8740          |\n",
            "|    policy_gradient_loss | -0.000294     |\n",
            "|    value_loss           | 2.06e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 876          |\n",
            "|    time_elapsed         | 3070         |\n",
            "|    total_timesteps      | 1794048      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024070414 |\n",
            "|    clip_fraction        | 0.0429       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.615       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.46e+03     |\n",
            "|    n_updates            | 8750         |\n",
            "|    policy_gradient_loss | -0.00416     |\n",
            "|    value_loss           | 1.94e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 877          |\n",
            "|    time_elapsed         | 3073         |\n",
            "|    total_timesteps      | 1796096      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0055981297 |\n",
            "|    clip_fraction        | 0.025        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.645       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.14e+04     |\n",
            "|    n_updates            | 8760         |\n",
            "|    policy_gradient_loss | -0.00439     |\n",
            "|    value_loss           | 2.01e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 584        |\n",
            "|    iterations           | 878        |\n",
            "|    time_elapsed         | 3077       |\n",
            "|    total_timesteps      | 1798144    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00477421 |\n",
            "|    clip_fraction        | 0.0916     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.629     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 9.67e+03   |\n",
            "|    n_updates            | 8770       |\n",
            "|    policy_gradient_loss | -0.000307  |\n",
            "|    value_loss           | 1.91e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 879          |\n",
            "|    time_elapsed         | 3080         |\n",
            "|    total_timesteps      | 1800192      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028837475 |\n",
            "|    clip_fraction        | 0.0194       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.621       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.2e+04      |\n",
            "|    n_updates            | 8780         |\n",
            "|    policy_gradient_loss | -0.00537     |\n",
            "|    value_loss           | 2.06e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 880          |\n",
            "|    time_elapsed         | 3084         |\n",
            "|    total_timesteps      | 1802240      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010566628 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.639       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.14e+04     |\n",
            "|    n_updates            | 8790         |\n",
            "|    policy_gradient_loss | -0.00167     |\n",
            "|    value_loss           | 2.09e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 881          |\n",
            "|    time_elapsed         | 3087         |\n",
            "|    total_timesteps      | 1804288      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011595059 |\n",
            "|    clip_fraction        | 0.000586     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.625       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.91e+03     |\n",
            "|    n_updates            | 8800         |\n",
            "|    policy_gradient_loss | -0.00135     |\n",
            "|    value_loss           | 1.77e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 882          |\n",
            "|    time_elapsed         | 3091         |\n",
            "|    total_timesteps      | 1806336      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0061055883 |\n",
            "|    clip_fraction        | 0.0543       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.632       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.38e+03     |\n",
            "|    n_updates            | 8810         |\n",
            "|    policy_gradient_loss | -0.00176     |\n",
            "|    value_loss           | 2.04e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 883         |\n",
            "|    time_elapsed         | 3094        |\n",
            "|    total_timesteps      | 1808384     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.010161805 |\n",
            "|    clip_fraction        | 0.0448      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.552      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.78e+03    |\n",
            "|    n_updates            | 8820        |\n",
            "|    policy_gradient_loss | -0.00129    |\n",
            "|    value_loss           | 1.96e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 884          |\n",
            "|    time_elapsed         | 3098         |\n",
            "|    total_timesteps      | 1810432      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019493066 |\n",
            "|    clip_fraction        | 0.0042       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.594       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.89e+03     |\n",
            "|    n_updates            | 8830         |\n",
            "|    policy_gradient_loss | -0.00364     |\n",
            "|    value_loss           | 2e+04        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 885          |\n",
            "|    time_elapsed         | 3101         |\n",
            "|    total_timesteps      | 1812480      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030008946 |\n",
            "|    clip_fraction        | 0.0392       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.598       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.68e+03     |\n",
            "|    n_updates            | 8840         |\n",
            "|    policy_gradient_loss | -0.00118     |\n",
            "|    value_loss           | 1.79e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 886          |\n",
            "|    time_elapsed         | 3105         |\n",
            "|    total_timesteps      | 1814528      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010863123 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.597       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.07e+04     |\n",
            "|    n_updates            | 8850         |\n",
            "|    policy_gradient_loss | -0.000288    |\n",
            "|    value_loss           | 1.79e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 887           |\n",
            "|    time_elapsed         | 3108          |\n",
            "|    total_timesteps      | 1816576       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018310169 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.59         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.08e+04      |\n",
            "|    n_updates            | 8860          |\n",
            "|    policy_gradient_loss | -0.000761     |\n",
            "|    value_loss           | 1.81e+04      |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 888         |\n",
            "|    time_elapsed         | 3112        |\n",
            "|    total_timesteps      | 1818624     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007376807 |\n",
            "|    clip_fraction        | 0.0327      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.634      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.91e+03    |\n",
            "|    n_updates            | 8870        |\n",
            "|    policy_gradient_loss | -0.00443    |\n",
            "|    value_loss           | 2.1e+04     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 889          |\n",
            "|    time_elapsed         | 3115         |\n",
            "|    total_timesteps      | 1820672      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0030131992 |\n",
            "|    clip_fraction        | 0.0219       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.64        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.1e+04      |\n",
            "|    n_updates            | 8880         |\n",
            "|    policy_gradient_loss | -0.00505     |\n",
            "|    value_loss           | 1.95e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 890         |\n",
            "|    time_elapsed         | 3119        |\n",
            "|    total_timesteps      | 1822720     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008461458 |\n",
            "|    clip_fraction        | 0.0447      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.623      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 7.95e+03    |\n",
            "|    n_updates            | 8890        |\n",
            "|    policy_gradient_loss | -0.00343    |\n",
            "|    value_loss           | 1.77e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 891          |\n",
            "|    time_elapsed         | 3122         |\n",
            "|    total_timesteps      | 1824768      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041301893 |\n",
            "|    clip_fraction        | 0.0144       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.62        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.1e+04      |\n",
            "|    n_updates            | 8900         |\n",
            "|    policy_gradient_loss | -0.00377     |\n",
            "|    value_loss           | 1.91e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 892          |\n",
            "|    time_elapsed         | 3126         |\n",
            "|    total_timesteps      | 1826816      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025779004 |\n",
            "|    clip_fraction        | 0.0267       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.615       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.97e+03     |\n",
            "|    n_updates            | 8910         |\n",
            "|    policy_gradient_loss | -0.00482     |\n",
            "|    value_loss           | 2.02e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 893         |\n",
            "|    time_elapsed         | 3129        |\n",
            "|    total_timesteps      | 1828864     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005910491 |\n",
            "|    clip_fraction        | 0.0204      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.607      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.62e+03    |\n",
            "|    n_updates            | 8920        |\n",
            "|    policy_gradient_loss | -0.00517    |\n",
            "|    value_loss           | 1.79e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 894          |\n",
            "|    time_elapsed         | 3133         |\n",
            "|    total_timesteps      | 1830912      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0042056246 |\n",
            "|    clip_fraction        | 0.036        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.634       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.11e+04     |\n",
            "|    n_updates            | 8930         |\n",
            "|    policy_gradient_loss | -0.00314     |\n",
            "|    value_loss           | 1.91e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 895          |\n",
            "|    time_elapsed         | 3136         |\n",
            "|    total_timesteps      | 1832960      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033104673 |\n",
            "|    clip_fraction        | 0.0152       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.632       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.79e+03     |\n",
            "|    n_updates            | 8940         |\n",
            "|    policy_gradient_loss | -0.00655     |\n",
            "|    value_loss           | 1.78e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 896          |\n",
            "|    time_elapsed         | 3140         |\n",
            "|    total_timesteps      | 1835008      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004535638 |\n",
            "|    clip_fraction        | 4.88e-05     |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.611       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.69e+03     |\n",
            "|    n_updates            | 8950         |\n",
            "|    policy_gradient_loss | -0.00149     |\n",
            "|    value_loss           | 2.01e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 897         |\n",
            "|    time_elapsed         | 3143        |\n",
            "|    total_timesteps      | 1837056     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001436985 |\n",
            "|    clip_fraction        | 0.00522     |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.63       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.03e+04    |\n",
            "|    n_updates            | 8960        |\n",
            "|    policy_gradient_loss | -0.00297    |\n",
            "|    value_loss           | 1.97e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 898          |\n",
            "|    time_elapsed         | 3147         |\n",
            "|    total_timesteps      | 1839104      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028544879 |\n",
            "|    clip_fraction        | 0.0117       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.575       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.21e+04     |\n",
            "|    n_updates            | 8970         |\n",
            "|    policy_gradient_loss | -0.00297     |\n",
            "|    value_loss           | 1.86e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 899          |\n",
            "|    time_elapsed         | 3150         |\n",
            "|    total_timesteps      | 1841152      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0077282684 |\n",
            "|    clip_fraction        | 0.0737       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.59        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.58e+03     |\n",
            "|    n_updates            | 8980         |\n",
            "|    policy_gradient_loss | -0.00253     |\n",
            "|    value_loss           | 2.26e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 900           |\n",
            "|    time_elapsed         | 3154          |\n",
            "|    total_timesteps      | 1843200       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015935497 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.588        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.18e+04      |\n",
            "|    n_updates            | 8990          |\n",
            "|    policy_gradient_loss | -0.000423     |\n",
            "|    value_loss           | 2.21e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 901          |\n",
            "|    time_elapsed         | 3157         |\n",
            "|    total_timesteps      | 1845248      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.552287e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.656       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.03e+03     |\n",
            "|    n_updates            | 9000         |\n",
            "|    policy_gradient_loss | -0.000711    |\n",
            "|    value_loss           | 2e+04        |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 902          |\n",
            "|    time_elapsed         | 3161         |\n",
            "|    total_timesteps      | 1847296      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0051775044 |\n",
            "|    clip_fraction        | 0.035        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.571       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.25e+03     |\n",
            "|    n_updates            | 9010         |\n",
            "|    policy_gradient_loss | -0.00386     |\n",
            "|    value_loss           | 2.12e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 903         |\n",
            "|    time_elapsed         | 3164        |\n",
            "|    total_timesteps      | 1849344     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.013534174 |\n",
            "|    clip_fraction        | 0.0811      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.625      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.96e+03    |\n",
            "|    n_updates            | 9020        |\n",
            "|    policy_gradient_loss | 0.00123     |\n",
            "|    value_loss           | 1.92e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 904         |\n",
            "|    time_elapsed         | 3168        |\n",
            "|    total_timesteps      | 1851392     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006151116 |\n",
            "|    clip_fraction        | 0.025       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.563      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.1e+04     |\n",
            "|    n_updates            | 9030        |\n",
            "|    policy_gradient_loss | -0.00354    |\n",
            "|    value_loss           | 2.19e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 905          |\n",
            "|    time_elapsed         | 3171         |\n",
            "|    total_timesteps      | 1853440      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0070407335 |\n",
            "|    clip_fraction        | 0.0679       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.606       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.32e+03     |\n",
            "|    n_updates            | 9040         |\n",
            "|    policy_gradient_loss | -0.00753     |\n",
            "|    value_loss           | 1.9e+04      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 906         |\n",
            "|    time_elapsed         | 3175        |\n",
            "|    total_timesteps      | 1855488     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.015278995 |\n",
            "|    clip_fraction        | 0.0715      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.58       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.12e+03    |\n",
            "|    n_updates            | 9050        |\n",
            "|    policy_gradient_loss | -0.00139    |\n",
            "|    value_loss           | 1.98e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 907          |\n",
            "|    time_elapsed         | 3178         |\n",
            "|    total_timesteps      | 1857536      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020293146 |\n",
            "|    clip_fraction        | 0.0111       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.622       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.09e+03     |\n",
            "|    n_updates            | 9060         |\n",
            "|    policy_gradient_loss | -0.00461     |\n",
            "|    value_loss           | 1.86e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 908           |\n",
            "|    time_elapsed         | 3182          |\n",
            "|    total_timesteps      | 1859584       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00051681953 |\n",
            "|    clip_fraction        | 4.88e-05      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.64         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.01e+04      |\n",
            "|    n_updates            | 9070          |\n",
            "|    policy_gradient_loss | -0.00158      |\n",
            "|    value_loss           | 2.01e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 909          |\n",
            "|    time_elapsed         | 3185         |\n",
            "|    total_timesteps      | 1861632      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0054836893 |\n",
            "|    clip_fraction        | 0.00874      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.626       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.09e+04     |\n",
            "|    n_updates            | 9080         |\n",
            "|    policy_gradient_loss | -0.0046      |\n",
            "|    value_loss           | 2.04e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 910         |\n",
            "|    time_elapsed         | 3189        |\n",
            "|    total_timesteps      | 1863680     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001770235 |\n",
            "|    clip_fraction        | 0.0121      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.586      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.11e+04    |\n",
            "|    n_updates            | 9090        |\n",
            "|    policy_gradient_loss | -0.00501    |\n",
            "|    value_loss           | 2.08e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 911          |\n",
            "|    time_elapsed         | 3192         |\n",
            "|    total_timesteps      | 1865728      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020097601 |\n",
            "|    clip_fraction        | 0.053        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.639       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.65e+03     |\n",
            "|    n_updates            | 9100         |\n",
            "|    policy_gradient_loss | -0.00327     |\n",
            "|    value_loss           | 1.72e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 912           |\n",
            "|    time_elapsed         | 3196          |\n",
            "|    total_timesteps      | 1867776       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00068092195 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.623        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.13e+04      |\n",
            "|    n_updates            | 9110          |\n",
            "|    policy_gradient_loss | -0.00091      |\n",
            "|    value_loss           | 2.07e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 913           |\n",
            "|    time_elapsed         | 3199          |\n",
            "|    total_timesteps      | 1869824       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013303655 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.619        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.26e+04      |\n",
            "|    n_updates            | 9120          |\n",
            "|    policy_gradient_loss | -0.000988     |\n",
            "|    value_loss           | 2.31e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 914          |\n",
            "|    time_elapsed         | 3203         |\n",
            "|    total_timesteps      | 1871872      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008906777 |\n",
            "|    clip_fraction        | 0.00137      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.612       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.02e+04     |\n",
            "|    n_updates            | 9130         |\n",
            "|    policy_gradient_loss | -0.00225     |\n",
            "|    value_loss           | 2.07e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 915          |\n",
            "|    time_elapsed         | 3206         |\n",
            "|    total_timesteps      | 1873920      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0044694766 |\n",
            "|    clip_fraction        | 0.00645      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.629       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.17e+04     |\n",
            "|    n_updates            | 9140         |\n",
            "|    policy_gradient_loss | -0.000664    |\n",
            "|    value_loss           | 2.1e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 916          |\n",
            "|    time_elapsed         | 3210         |\n",
            "|    total_timesteps      | 1875968      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005766946 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.608       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.06e+04     |\n",
            "|    n_updates            | 9150         |\n",
            "|    policy_gradient_loss | -0.00151     |\n",
            "|    value_loss           | 1.98e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 917          |\n",
            "|    time_elapsed         | 3213         |\n",
            "|    total_timesteps      | 1878016      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032364351 |\n",
            "|    clip_fraction        | 0.0723       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.613       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.7e+03      |\n",
            "|    n_updates            | 9160         |\n",
            "|    policy_gradient_loss | -0.000803    |\n",
            "|    value_loss           | 2e+04        |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 918           |\n",
            "|    time_elapsed         | 3217          |\n",
            "|    total_timesteps      | 1880064       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00072660414 |\n",
            "|    clip_fraction        | 0.000342      |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.602        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.67e+03      |\n",
            "|    n_updates            | 9170          |\n",
            "|    policy_gradient_loss | -0.00174      |\n",
            "|    value_loss           | 2.12e+04      |\n",
            "-------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 584        |\n",
            "|    iterations           | 919        |\n",
            "|    time_elapsed         | 3220       |\n",
            "|    total_timesteps      | 1882112    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00417192 |\n",
            "|    clip_fraction        | 0.0328     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.571     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 9.94e+03   |\n",
            "|    n_updates            | 9180       |\n",
            "|    policy_gradient_loss | -0.00839   |\n",
            "|    value_loss           | 2.11e+04   |\n",
            "----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 584        |\n",
            "|    iterations           | 920        |\n",
            "|    time_elapsed         | 3224       |\n",
            "|    total_timesteps      | 1884160    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00269834 |\n",
            "|    clip_fraction        | 0.0108     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.601     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 8.33e+03   |\n",
            "|    n_updates            | 9190       |\n",
            "|    policy_gradient_loss | -0.00357   |\n",
            "|    value_loss           | 1.86e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 921          |\n",
            "|    time_elapsed         | 3227         |\n",
            "|    total_timesteps      | 1886208      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0036192331 |\n",
            "|    clip_fraction        | 0.0271       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.608       |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.3e+04      |\n",
            "|    n_updates            | 9200         |\n",
            "|    policy_gradient_loss | -0.00281     |\n",
            "|    value_loss           | 2.16e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 922         |\n",
            "|    time_elapsed         | 3231        |\n",
            "|    total_timesteps      | 1888256     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.012306191 |\n",
            "|    clip_fraction        | 0.0544      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.619      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.12e+03    |\n",
            "|    n_updates            | 9210        |\n",
            "|    policy_gradient_loss | -0.00489    |\n",
            "|    value_loss           | 2.22e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 584        |\n",
            "|    iterations           | 923        |\n",
            "|    time_elapsed         | 3234       |\n",
            "|    total_timesteps      | 1890304    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00709283 |\n",
            "|    clip_fraction        | 0.0655     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.64      |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.15e+04   |\n",
            "|    n_updates            | 9220       |\n",
            "|    policy_gradient_loss | -0.00325   |\n",
            "|    value_loss           | 2.23e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 924         |\n",
            "|    time_elapsed         | 3238        |\n",
            "|    total_timesteps      | 1892352     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005537887 |\n",
            "|    clip_fraction        | 0.0851      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.659      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.54e+03    |\n",
            "|    n_updates            | 9230        |\n",
            "|    policy_gradient_loss | -0.0024     |\n",
            "|    value_loss           | 1.99e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 925          |\n",
            "|    time_elapsed         | 3241         |\n",
            "|    total_timesteps      | 1894400      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022786225 |\n",
            "|    clip_fraction        | 0.0102       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.659       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.1e+04      |\n",
            "|    n_updates            | 9240         |\n",
            "|    policy_gradient_loss | -0.0049      |\n",
            "|    value_loss           | 2.15e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 926          |\n",
            "|    time_elapsed         | 3245         |\n",
            "|    total_timesteps      | 1896448      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019599604 |\n",
            "|    clip_fraction        | 0.00479      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.603       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.06e+04     |\n",
            "|    n_updates            | 9250         |\n",
            "|    policy_gradient_loss | -0.00291     |\n",
            "|    value_loss           | 2.32e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 927          |\n",
            "|    time_elapsed         | 3248         |\n",
            "|    total_timesteps      | 1898496      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034981673 |\n",
            "|    clip_fraction        | 0.0298       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.612       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.63e+03     |\n",
            "|    n_updates            | 9260         |\n",
            "|    policy_gradient_loss | -0.00301     |\n",
            "|    value_loss           | 2.13e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 928          |\n",
            "|    time_elapsed         | 3252         |\n",
            "|    total_timesteps      | 1900544      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023543178 |\n",
            "|    clip_fraction        | 0.00195      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.639       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.61e+03     |\n",
            "|    n_updates            | 9270         |\n",
            "|    policy_gradient_loss | -0.00257     |\n",
            "|    value_loss           | 2.23e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 929          |\n",
            "|    time_elapsed         | 3255         |\n",
            "|    total_timesteps      | 1902592      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0024768612 |\n",
            "|    clip_fraction        | 0.0349       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.633       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.15e+04     |\n",
            "|    n_updates            | 9280         |\n",
            "|    policy_gradient_loss | -0.00148     |\n",
            "|    value_loss           | 2.25e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 930          |\n",
            "|    time_elapsed         | 3259         |\n",
            "|    total_timesteps      | 1904640      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035939512 |\n",
            "|    clip_fraction        | 0.0122       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.63        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.04e+04     |\n",
            "|    n_updates            | 9290         |\n",
            "|    policy_gradient_loss | -0.00525     |\n",
            "|    value_loss           | 2.06e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 931         |\n",
            "|    time_elapsed         | 3262        |\n",
            "|    total_timesteps      | 1906688     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004175498 |\n",
            "|    clip_fraction        | 0.0353      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.603      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.04e+04    |\n",
            "|    n_updates            | 9300        |\n",
            "|    policy_gradient_loss | -0.00264    |\n",
            "|    value_loss           | 2.29e+04    |\n",
            "-----------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 584        |\n",
            "|    iterations           | 932        |\n",
            "|    time_elapsed         | 3266       |\n",
            "|    total_timesteps      | 1908736    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.02119669 |\n",
            "|    clip_fraction        | 0.0605     |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.611     |\n",
            "|    explained_variance   | 0          |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.1e+04    |\n",
            "|    n_updates            | 9310       |\n",
            "|    policy_gradient_loss | 0.00165    |\n",
            "|    value_loss           | 2.27e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 933          |\n",
            "|    time_elapsed         | 3269         |\n",
            "|    total_timesteps      | 1910784      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0138860205 |\n",
            "|    clip_fraction        | 0.181        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.651       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.63e+03     |\n",
            "|    n_updates            | 9320         |\n",
            "|    policy_gradient_loss | 0.0104       |\n",
            "|    value_loss           | 2.12e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 934         |\n",
            "|    time_elapsed         | 3273        |\n",
            "|    total_timesteps      | 1912832     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004072859 |\n",
            "|    clip_fraction        | 0.0313      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.664      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.14e+04    |\n",
            "|    n_updates            | 9330        |\n",
            "|    policy_gradient_loss | -0.00395    |\n",
            "|    value_loss           | 2.09e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 935           |\n",
            "|    time_elapsed         | 3276          |\n",
            "|    total_timesteps      | 1914880       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.5881945e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.632        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 1.21e+04      |\n",
            "|    n_updates            | 9340          |\n",
            "|    policy_gradient_loss | -0.000433     |\n",
            "|    value_loss           | 2.27e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 936          |\n",
            "|    time_elapsed         | 3280         |\n",
            "|    total_timesteps      | 1916928      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0038589989 |\n",
            "|    clip_fraction        | 0.0118       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.676       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.02e+04     |\n",
            "|    n_updates            | 9350         |\n",
            "|    policy_gradient_loss | -0.00364     |\n",
            "|    value_loss           | 2.04e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 937         |\n",
            "|    time_elapsed         | 3283        |\n",
            "|    total_timesteps      | 1918976     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.022768475 |\n",
            "|    clip_fraction        | 0.0617      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.707      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.16e+04    |\n",
            "|    n_updates            | 9360        |\n",
            "|    policy_gradient_loss | 0.00187     |\n",
            "|    value_loss           | 2.24e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 938          |\n",
            "|    time_elapsed         | 3287         |\n",
            "|    total_timesteps      | 1921024      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041336026 |\n",
            "|    clip_fraction        | 0.0202       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.718       |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.24e+04     |\n",
            "|    n_updates            | 9370         |\n",
            "|    policy_gradient_loss | -0.00441     |\n",
            "|    value_loss           | 2.28e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 939          |\n",
            "|    time_elapsed         | 3290         |\n",
            "|    total_timesteps      | 1923072      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031297426 |\n",
            "|    clip_fraction        | 0.0223       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.704       |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.39e+03     |\n",
            "|    n_updates            | 9380         |\n",
            "|    policy_gradient_loss | -0.00507     |\n",
            "|    value_loss           | 1.97e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 940          |\n",
            "|    time_elapsed         | 3294         |\n",
            "|    total_timesteps      | 1925120      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027885307 |\n",
            "|    clip_fraction        | 0.00991      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.723       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.39e+03     |\n",
            "|    n_updates            | 9390         |\n",
            "|    policy_gradient_loss | -0.00343     |\n",
            "|    value_loss           | 2.07e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 941          |\n",
            "|    time_elapsed         | 3297         |\n",
            "|    total_timesteps      | 1927168      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027167178 |\n",
            "|    clip_fraction        | 0.0763       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.71        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.34e+04     |\n",
            "|    n_updates            | 9400         |\n",
            "|    policy_gradient_loss | -0.000168    |\n",
            "|    value_loss           | 2.36e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 942          |\n",
            "|    time_elapsed         | 3301         |\n",
            "|    total_timesteps      | 1929216      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0028308465 |\n",
            "|    clip_fraction        | 0.00625      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.733       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.55e+03     |\n",
            "|    n_updates            | 9410         |\n",
            "|    policy_gradient_loss | -0.00371     |\n",
            "|    value_loss           | 2.2e+04      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 943         |\n",
            "|    time_elapsed         | 3304        |\n",
            "|    total_timesteps      | 1931264     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002920021 |\n",
            "|    clip_fraction        | 0.0331      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.691      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.11e+04    |\n",
            "|    n_updates            | 9420        |\n",
            "|    policy_gradient_loss | -0.00367    |\n",
            "|    value_loss           | 2.16e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 944         |\n",
            "|    time_elapsed         | 3308        |\n",
            "|    total_timesteps      | 1933312     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004195706 |\n",
            "|    clip_fraction        | 0.0412      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.689      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.11e+04    |\n",
            "|    n_updates            | 9430        |\n",
            "|    policy_gradient_loss | -0.00921    |\n",
            "|    value_loss           | 2.12e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| time/                   |               |\n",
            "|    fps                  | 584           |\n",
            "|    iterations           | 945           |\n",
            "|    time_elapsed         | 3311          |\n",
            "|    total_timesteps      | 1935360       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00035601293 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.2           |\n",
            "|    entropy_loss         | -0.734        |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.93e+03      |\n",
            "|    n_updates            | 9440          |\n",
            "|    policy_gradient_loss | -0.00125      |\n",
            "|    value_loss           | 2.08e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 946          |\n",
            "|    time_elapsed         | 3315         |\n",
            "|    total_timesteps      | 1937408      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039693965 |\n",
            "|    clip_fraction        | 0.0427       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.711       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.06e+04     |\n",
            "|    n_updates            | 9450         |\n",
            "|    policy_gradient_loss | -0.00441     |\n",
            "|    value_loss           | 2.22e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 947         |\n",
            "|    time_elapsed         | 3318        |\n",
            "|    total_timesteps      | 1939456     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.027742201 |\n",
            "|    clip_fraction        | 0.105       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.703      |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.59e+03    |\n",
            "|    n_updates            | 9460        |\n",
            "|    policy_gradient_loss | 0.00242     |\n",
            "|    value_loss           | 2.25e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 948          |\n",
            "|    time_elapsed         | 3322         |\n",
            "|    total_timesteps      | 1941504      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020916476 |\n",
            "|    clip_fraction        | 0.00122      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.706       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.3e+04      |\n",
            "|    n_updates            | 9470         |\n",
            "|    policy_gradient_loss | -0.00243     |\n",
            "|    value_loss           | 2.38e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 949         |\n",
            "|    time_elapsed         | 3326        |\n",
            "|    total_timesteps      | 1943552     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011571038 |\n",
            "|    clip_fraction        | 0.0891      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.693      |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.06e+04    |\n",
            "|    n_updates            | 9480        |\n",
            "|    policy_gradient_loss | 0.0027      |\n",
            "|    value_loss           | 2.07e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 950         |\n",
            "|    time_elapsed         | 3329        |\n",
            "|    total_timesteps      | 1945600     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005775258 |\n",
            "|    clip_fraction        | 0.0757      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.723      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.28e+04    |\n",
            "|    n_updates            | 9490        |\n",
            "|    policy_gradient_loss | -0.00882    |\n",
            "|    value_loss           | 2.08e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 951          |\n",
            "|    time_elapsed         | 3333         |\n",
            "|    total_timesteps      | 1947648      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022175196 |\n",
            "|    clip_fraction        | 0.00991      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.696       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.08e+04     |\n",
            "|    n_updates            | 9500         |\n",
            "|    policy_gradient_loss | -0.00466     |\n",
            "|    value_loss           | 2.12e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 952         |\n",
            "|    time_elapsed         | 3336        |\n",
            "|    total_timesteps      | 1949696     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004625907 |\n",
            "|    clip_fraction        | 0.0259      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.719      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.13e+04    |\n",
            "|    n_updates            | 9510        |\n",
            "|    policy_gradient_loss | -0.00337    |\n",
            "|    value_loss           | 2.07e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 953          |\n",
            "|    time_elapsed         | 3340         |\n",
            "|    total_timesteps      | 1951744      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0037449754 |\n",
            "|    clip_fraction        | 0.0177       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.636       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.8e+03      |\n",
            "|    n_updates            | 9520         |\n",
            "|    policy_gradient_loss | -0.00644     |\n",
            "|    value_loss           | 2.19e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 954          |\n",
            "|    time_elapsed         | 3343         |\n",
            "|    total_timesteps      | 1953792      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0031808831 |\n",
            "|    clip_fraction        | 0.0507       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.631       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.44e+04     |\n",
            "|    n_updates            | 9530         |\n",
            "|    policy_gradient_loss | -0.0037      |\n",
            "|    value_loss           | 2.31e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 955         |\n",
            "|    time_elapsed         | 3347        |\n",
            "|    total_timesteps      | 1955840     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002209004 |\n",
            "|    clip_fraction        | 0.0127      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.635      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.19e+04    |\n",
            "|    n_updates            | 9540        |\n",
            "|    policy_gradient_loss | -0.0051     |\n",
            "|    value_loss           | 2.46e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 956         |\n",
            "|    time_elapsed         | 3350        |\n",
            "|    total_timesteps      | 1957888     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004099597 |\n",
            "|    clip_fraction        | 0.0385      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.642      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.57e+03    |\n",
            "|    n_updates            | 9550        |\n",
            "|    policy_gradient_loss | -0.00808    |\n",
            "|    value_loss           | 2.12e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 957          |\n",
            "|    time_elapsed         | 3354         |\n",
            "|    total_timesteps      | 1959936      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009776168 |\n",
            "|    clip_fraction        | 0.00186      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.696       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.44e+03     |\n",
            "|    n_updates            | 9560         |\n",
            "|    policy_gradient_loss | -0.00179     |\n",
            "|    value_loss           | 2.11e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 958          |\n",
            "|    time_elapsed         | 3357         |\n",
            "|    total_timesteps      | 1961984      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027942066 |\n",
            "|    clip_fraction        | 0.0115       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.654       |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.18e+04     |\n",
            "|    n_updates            | 9570         |\n",
            "|    policy_gradient_loss | -0.00507     |\n",
            "|    value_loss           | 2.3e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 959          |\n",
            "|    time_elapsed         | 3361         |\n",
            "|    total_timesteps      | 1964032      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021771863 |\n",
            "|    clip_fraction        | 0.00156      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.621       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.18e+04     |\n",
            "|    n_updates            | 9580         |\n",
            "|    policy_gradient_loss | -0.00299     |\n",
            "|    value_loss           | 2.33e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 960         |\n",
            "|    time_elapsed         | 3364        |\n",
            "|    total_timesteps      | 1966080     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.007563484 |\n",
            "|    clip_fraction        | 0.0571      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.653      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.13e+04    |\n",
            "|    n_updates            | 9590        |\n",
            "|    policy_gradient_loss | -0.00278    |\n",
            "|    value_loss           | 2.19e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 961          |\n",
            "|    time_elapsed         | 3368         |\n",
            "|    total_timesteps      | 1968128      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011040458 |\n",
            "|    clip_fraction        | 0.00278      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.654       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1e+04        |\n",
            "|    n_updates            | 9600         |\n",
            "|    policy_gradient_loss | 0.0007       |\n",
            "|    value_loss           | 2.45e+04     |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| time/                   |            |\n",
            "|    fps                  | 584        |\n",
            "|    iterations           | 962        |\n",
            "|    time_elapsed         | 3371       |\n",
            "|    total_timesteps      | 1970176    |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00198807 |\n",
            "|    clip_fraction        | 0.00596    |\n",
            "|    clip_range           | 0.2        |\n",
            "|    entropy_loss         | -0.638     |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 1.26e+04   |\n",
            "|    n_updates            | 9610       |\n",
            "|    policy_gradient_loss | -0.00354   |\n",
            "|    value_loss           | 2.23e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 963         |\n",
            "|    time_elapsed         | 3375        |\n",
            "|    total_timesteps      | 1972224     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.006286617 |\n",
            "|    clip_fraction        | 0.0336      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.653      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.77e+03    |\n",
            "|    n_updates            | 9620        |\n",
            "|    policy_gradient_loss | -0.00538    |\n",
            "|    value_loss           | 2.17e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 964          |\n",
            "|    time_elapsed         | 3378         |\n",
            "|    total_timesteps      | 1974272      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022261748 |\n",
            "|    clip_fraction        | 0.135        |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.652       |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.36e+03     |\n",
            "|    n_updates            | 9630         |\n",
            "|    policy_gradient_loss | -0.000162    |\n",
            "|    value_loss           | 2.16e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 965         |\n",
            "|    time_elapsed         | 3382        |\n",
            "|    total_timesteps      | 1976320     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001969579 |\n",
            "|    clip_fraction        | 0.0224      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.639      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.17e+04    |\n",
            "|    n_updates            | 9640        |\n",
            "|    policy_gradient_loss | -0.00482    |\n",
            "|    value_loss           | 2.13e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 966          |\n",
            "|    time_elapsed         | 3385         |\n",
            "|    total_timesteps      | 1978368      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022698073 |\n",
            "|    clip_fraction        | 0.00542      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.666       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.11e+04     |\n",
            "|    n_updates            | 9650         |\n",
            "|    policy_gradient_loss | -0.0038      |\n",
            "|    value_loss           | 2.23e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 967         |\n",
            "|    time_elapsed         | 3389        |\n",
            "|    total_timesteps      | 1980416     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.011480248 |\n",
            "|    clip_fraction        | 0.121       |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.641      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.23e+04    |\n",
            "|    n_updates            | 9660        |\n",
            "|    policy_gradient_loss | 0.000427    |\n",
            "|    value_loss           | 2.27e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 968         |\n",
            "|    time_elapsed         | 3392        |\n",
            "|    total_timesteps      | 1982464     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.005219204 |\n",
            "|    clip_fraction        | 0.0232      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.653      |\n",
            "|    explained_variance   | 5.96e-08    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1e+04       |\n",
            "|    n_updates            | 9670        |\n",
            "|    policy_gradient_loss | -0.00576    |\n",
            "|    value_loss           | 2.33e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 969          |\n",
            "|    time_elapsed         | 3396         |\n",
            "|    total_timesteps      | 1984512      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0039750105 |\n",
            "|    clip_fraction        | 0.00796      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.617       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.26e+04     |\n",
            "|    n_updates            | 9680         |\n",
            "|    policy_gradient_loss | -0.00251     |\n",
            "|    value_loss           | 2.36e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 970          |\n",
            "|    time_elapsed         | 3399         |\n",
            "|    total_timesteps      | 1986560      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018934038 |\n",
            "|    clip_fraction        | 0.00117      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.659       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.18e+04     |\n",
            "|    n_updates            | 9690         |\n",
            "|    policy_gradient_loss | -0.00309     |\n",
            "|    value_loss           | 2.19e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 971          |\n",
            "|    time_elapsed         | 3403         |\n",
            "|    total_timesteps      | 1988608      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041618855 |\n",
            "|    clip_fraction        | 0.0326       |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.62        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.29e+04     |\n",
            "|    n_updates            | 9700         |\n",
            "|    policy_gradient_loss | -0.00507     |\n",
            "|    value_loss           | 2.25e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 972         |\n",
            "|    time_elapsed         | 3406        |\n",
            "|    total_timesteps      | 1990656     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004200423 |\n",
            "|    clip_fraction        | 0.0841      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.655      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.66e+03    |\n",
            "|    n_updates            | 9710        |\n",
            "|    policy_gradient_loss | 0.00152     |\n",
            "|    value_loss           | 2.09e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 973         |\n",
            "|    time_elapsed         | 3410        |\n",
            "|    total_timesteps      | 1992704     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.004306098 |\n",
            "|    clip_fraction        | 0.0501      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.586      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 9.56e+03    |\n",
            "|    n_updates            | 9720        |\n",
            "|    policy_gradient_loss | -0.00602    |\n",
            "|    value_loss           | 2.21e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 974         |\n",
            "|    time_elapsed         | 3413        |\n",
            "|    total_timesteps      | 1994752     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.009469417 |\n",
            "|    clip_fraction        | 0.0691      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.623      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.09e+04    |\n",
            "|    n_updates            | 9730        |\n",
            "|    policy_gradient_loss | -0.00136    |\n",
            "|    value_loss           | 2.11e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 975         |\n",
            "|    time_elapsed         | 3417        |\n",
            "|    total_timesteps      | 1996800     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008755143 |\n",
            "|    clip_fraction        | 0.0337      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.668      |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.75e+03    |\n",
            "|    n_updates            | 9740        |\n",
            "|    policy_gradient_loss | -0.00735    |\n",
            "|    value_loss           | 2.11e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| time/                   |             |\n",
            "|    fps                  | 584         |\n",
            "|    iterations           | 976         |\n",
            "|    time_elapsed         | 3420        |\n",
            "|    total_timesteps      | 1998848     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003265436 |\n",
            "|    clip_fraction        | 0.0214      |\n",
            "|    clip_range           | 0.2         |\n",
            "|    entropy_loss         | -0.671      |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 1.44e+04    |\n",
            "|    n_updates            | 9750        |\n",
            "|    policy_gradient_loss | -0.00366    |\n",
            "|    value_loss           | 2.15e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| time/                   |              |\n",
            "|    fps                  | 584          |\n",
            "|    iterations           | 977          |\n",
            "|    time_elapsed         | 3424         |\n",
            "|    total_timesteps      | 2000896      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034227394 |\n",
            "|    clip_fraction        | 0.00991      |\n",
            "|    clip_range           | 0.2          |\n",
            "|    entropy_loss         | -0.694       |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 1.22e+04     |\n",
            "|    n_updates            | 9760         |\n",
            "|    policy_gradient_loss | -0.00435     |\n",
            "|    value_loss           | 2.28e+04     |\n",
            "------------------------------------------\n",
            "Model saved as ppo_2048.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Load model\n",
        "model = PPO.load(\"ppo_2048\")\n",
        "\n",
        "env = Game2048Env(render_mode=\"ansi\")\n",
        "\n",
        "obs, info = env.reset()\n",
        "done = False\n",
        "step = 0\n",
        "\n",
        "print(\"Initial board:\")\n",
        "print(env._board_to_string())\n",
        "\n",
        "while not done:\n",
        "    # stochastic actions so you can see exploration / variability\n",
        "    action, _ = model.predict(obs, deterministic=False)\n",
        "    obs, reward, terminated, truncated, info = env.step(action)\n",
        "    done = terminated or truncated\n",
        "\n",
        "    print(f\"\\nStep {step}, action={action}, reward={reward}, score={info['score']}\")\n",
        "    print(env._board_to_string())\n",
        "    time.sleep(0.1)\n",
        "    step += 1\n",
        "\n",
        "print(\"\\nEpisode finished.\")\n",
        "print(f\"Final score: {info['score']}, max tile: {info['max_tile']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPhPC9Mdsp8z",
        "outputId": "d07d701e-a714-44ae-92b2-17f1beefa3fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial board:\n",
            ".\t.\t.\t.\n",
            "2\t.\t.\t2\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 0, action=0, reward=0, score=0\n",
            "2\t2\t.\t2\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 1, action=3, reward=4, score=4\n",
            ".\t.\t2\t4\n",
            ".\t.\t.\t.\n",
            ".\t.\t2\t.\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 2, action=1, reward=4, score=8\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            ".\t2\t.\t.\n",
            ".\t.\t4\t4\n",
            "\n",
            "Step 3, action=0, reward=0, score=8\n",
            ".\t2\t4\t4\n",
            ".\t.\t.\t.\n",
            ".\t2\t.\t.\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 4, action=3, reward=8, score=16\n",
            ".\t.\t2\t8\n",
            ".\t.\t.\t.\n",
            "2\t.\t.\t2\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 5, action=3, reward=4, score=20\n",
            ".\t.\t2\t8\n",
            "2\t.\t.\t.\n",
            ".\t.\t.\t4\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 6, action=1, reward=0, score=20\n",
            "4\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t8\n",
            "2\t.\t2\t4\n",
            "\n",
            "Step 7, action=0, reward=0, score=20\n",
            "4\t.\t2\t8\n",
            "2\t.\t.\t4\n",
            ".\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "\n",
            "Step 8, action=1, reward=4, score=24\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t2\n",
            "4\t.\t.\t8\n",
            "4\t.\t2\t4\n",
            "\n",
            "Step 9, action=2, reward=0, score=24\n",
            ".\t.\t.\t.\n",
            "2\t2\t.\t.\n",
            "4\t8\t.\t.\n",
            "4\t2\t4\t.\n",
            "\n",
            "Step 10, action=0, reward=8, score=32\n",
            "2\t2\t4\t.\n",
            "8\t8\t.\t.\n",
            ".\t2\t.\t2\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 11, action=3, reward=24, score=56\n",
            ".\t.\t4\t4\n",
            ".\t.\t.\t16\n",
            ".\t.\t.\t4\n",
            "2\t.\t.\t.\n",
            "\n",
            "Step 12, action=1, reward=0, score=56\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t4\n",
            ".\t.\t2\t16\n",
            "2\t.\t4\t4\n",
            "\n",
            "Step 13, action=2, reward=8, score=64\n",
            "2\t.\t.\t.\n",
            "4\t.\t.\t.\n",
            "2\t16\t.\t.\n",
            "2\t8\t.\t.\n",
            "\n",
            "Step 14, action=0, reward=4, score=68\n",
            "2\t16\t.\t.\n",
            "4\t8\t.\t.\n",
            "4\t2\t.\t.\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 15, action=3, reward=0, score=68\n",
            ".\t.\t2\t16\n",
            ".\t.\t4\t8\n",
            ".\t.\t4\t2\n",
            "2\t.\t.\t.\n",
            "\n",
            "Step 16, action=1, reward=8, score=76\n",
            ".\t.\t.\t.\n",
            "4\t.\t.\t16\n",
            ".\t.\t2\t8\n",
            "2\t.\t8\t2\n",
            "\n",
            "Step 17, action=2, reward=0, score=76\n",
            ".\t.\t.\t.\n",
            "4\t16\t.\t.\n",
            "2\t8\t.\t.\n",
            "2\t8\t2\t2\n",
            "\n",
            "Step 18, action=0, reward=20, score=96\n",
            "4\t16\t2\t2\n",
            "4\t16\t.\t.\n",
            ".\t.\t2\t.\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 19, action=3, reward=4, score=100\n",
            ".\t4\t16\t4\n",
            ".\t.\t4\t16\n",
            ".\t.\t.\t2\n",
            "2\t.\t.\t.\n",
            "\n",
            "Step 20, action=1, reward=0, score=100\n",
            ".\t2\t.\t.\n",
            ".\t.\t.\t4\n",
            ".\t.\t16\t16\n",
            "2\t4\t4\t2\n",
            "\n",
            "Step 21, action=2, reward=40, score=140\n",
            "2\t.\t.\t.\n",
            "4\t.\t2\t.\n",
            "32\t.\t.\t.\n",
            "2\t8\t2\t.\n",
            "\n",
            "Step 22, action=2, reward=0, score=140\n",
            "2\t.\t.\t.\n",
            "4\t2\t.\t.\n",
            "32\t2\t.\t.\n",
            "2\t8\t2\t.\n",
            "\n",
            "Step 23, action=3, reward=0, score=140\n",
            ".\t.\t.\t2\n",
            ".\t.\t4\t2\n",
            ".\t.\t32\t2\n",
            "2\t2\t8\t2\n",
            "\n",
            "Step 24, action=2, reward=4, score=144\n",
            "2\t.\t.\t.\n",
            "4\t2\t.\t2\n",
            "32\t2\t.\t.\n",
            "4\t8\t2\t.\n",
            "\n",
            "Step 25, action=0, reward=4, score=148\n",
            "2\t4\t2\t2\n",
            "4\t8\t.\t.\n",
            "32\t.\t2\t.\n",
            "4\t.\t.\t.\n",
            "\n",
            "Step 26, action=3, reward=4, score=152\n",
            ".\t2\t4\t4\n",
            ".\t.\t4\t8\n",
            ".\t.\t32\t2\n",
            ".\t2\t.\t4\n",
            "\n",
            "Step 27, action=1, reward=12, score=164\n",
            ".\t.\t2\t4\n",
            ".\t.\t.\t8\n",
            ".\t.\t8\t2\n",
            ".\t4\t32\t4\n",
            "\n",
            "Step 28, action=2, reward=0, score=164\n",
            "2\t4\t.\t.\n",
            "8\t.\t.\t.\n",
            "8\t2\t.\t2\n",
            "4\t32\t4\t.\n",
            "\n",
            "Step 29, action=0, reward=16, score=180\n",
            "2\t4\t4\t2\n",
            "16\t2\t.\t.\n",
            "4\t32\t.\t.\n",
            ".\t2\t.\t.\n",
            "\n",
            "Step 30, action=3, reward=8, score=188\n",
            "2\t2\t8\t2\n",
            ".\t.\t16\t2\n",
            ".\t.\t4\t32\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 31, action=1, reward=4, score=192\n",
            ".\t.\t.\t.\n",
            ".\t.\t8\t4\n",
            "2\t.\t16\t32\n",
            "2\t2\t4\t2\n",
            "\n",
            "Step 32, action=2, reward=4, score=196\n",
            ".\t.\t2\t.\n",
            "8\t4\t.\t.\n",
            "2\t16\t32\t.\n",
            "4\t4\t2\t.\n",
            "\n",
            "Step 33, action=0, reward=0, score=196\n",
            "8\t4\t2\t.\n",
            "2\t16\t32\t.\n",
            "4\t4\t2\t.\n",
            ".\t2\t.\t.\n",
            "\n",
            "Step 34, action=3, reward=8, score=204\n",
            ".\t8\t4\t2\n",
            ".\t2\t16\t32\n",
            ".\t.\t8\t2\n",
            ".\t.\t2\t2\n",
            "\n",
            "Step 35, action=1, reward=4, score=208\n",
            ".\t.\t4\t2\n",
            ".\t.\t16\t2\n",
            ".\t8\t8\t32\n",
            ".\t2\t2\t4\n",
            "\n",
            "Step 36, action=2, reward=20, score=228\n",
            "4\t2\t.\t.\n",
            "16\t2\t.\t.\n",
            "16\t32\t.\t.\n",
            "4\t4\t4\t.\n",
            "\n",
            "Step 37, action=0, reward=36, score=264\n",
            "4\t4\t4\t.\n",
            "32\t32\t.\t.\n",
            "4\t4\t.\t.\n",
            ".\t.\t2\t.\n",
            "\n",
            "Step 38, action=3, reward=80, score=344\n",
            ".\t.\t4\t8\n",
            ".\t.\t.\t64\n",
            "2\t.\t.\t8\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 39, action=1, reward=0, score=344\n",
            ".\t.\t.\t8\n",
            ".\t.\t2\t64\n",
            ".\t.\t.\t8\n",
            "2\t.\t4\t2\n",
            "\n",
            "Step 40, action=2, reward=0, score=344\n",
            "8\t.\t2\t.\n",
            "2\t64\t.\t.\n",
            "8\t.\t.\t.\n",
            "2\t4\t2\t.\n",
            "\n",
            "Step 41, action=0, reward=4, score=348\n",
            "8\t64\t4\t.\n",
            "2\t4\t.\t.\n",
            "8\t.\t.\t.\n",
            "2\t2\t.\t.\n",
            "\n",
            "Step 42, action=3, reward=4, score=352\n",
            ".\t8\t64\t4\n",
            ".\t2\t2\t4\n",
            ".\t.\t.\t8\n",
            ".\t.\t.\t4\n",
            "\n",
            "Step 43, action=1, reward=8, score=360\n",
            ".\t.\t.\t2\n",
            ".\t.\t.\t8\n",
            ".\t8\t64\t8\n",
            ".\t2\t2\t4\n",
            "\n",
            "Step 44, action=1, reward=16, score=376\n",
            ".\t.\t.\t.\n",
            ".\t.\t2\t2\n",
            ".\t8\t64\t16\n",
            ".\t2\t2\t4\n",
            "\n",
            "Step 45, action=2, reward=8, score=384\n",
            ".\t.\t.\t.\n",
            "4\t.\t.\t.\n",
            "8\t64\t16\t.\n",
            "4\t4\t.\t2\n",
            "\n",
            "Step 46, action=0, reward=0, score=384\n",
            "4\t64\t16\t2\n",
            "8\t4\t.\t.\n",
            "4\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "\n",
            "Step 47, action=3, reward=0, score=384\n",
            "4\t64\t16\t2\n",
            ".\t.\t8\t4\n",
            ".\t2\t.\t4\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 48, action=1, reward=8, score=392\n",
            ".\t2\t.\t.\n",
            ".\t.\t.\t2\n",
            ".\t64\t16\t8\n",
            "4\t2\t8\t2\n",
            "\n",
            "Step 49, action=2, reward=0, score=392\n",
            "2\t.\t.\t4\n",
            "2\t.\t.\t.\n",
            "64\t16\t8\t.\n",
            "4\t2\t8\t2\n",
            "\n",
            "Step 50, action=0, reward=20, score=412\n",
            "4\t16\t16\t4\n",
            "64\t2\t.\t2\n",
            "4\t.\t.\t.\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 51, action=3, reward=36, score=448\n",
            ".\t4\t32\t4\n",
            ".\t.\t64\t4\n",
            ".\t.\t2\t4\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 52, action=1, reward=8, score=456\n",
            ".\t.\t.\t2\n",
            ".\t.\t32\t4\n",
            ".\t.\t64\t8\n",
            ".\t4\t2\t2\n",
            "\n",
            "Step 53, action=1, reward=-1.0, score=455.0\n",
            ".\t.\t.\t2\n",
            ".\t.\t32\t4\n",
            ".\t.\t64\t8\n",
            ".\t4\t2\t2\n",
            "\n",
            "Step 54, action=1, reward=-1.0, score=454.0\n",
            ".\t.\t.\t2\n",
            ".\t.\t32\t4\n",
            ".\t.\t64\t8\n",
            ".\t4\t2\t2\n",
            "\n",
            "Step 55, action=3, reward=4, score=458.0\n",
            ".\t2\t.\t2\n",
            ".\t.\t32\t4\n",
            ".\t.\t64\t8\n",
            ".\t.\t4\t4\n",
            "\n",
            "Step 56, action=1, reward=0, score=458.0\n",
            ".\t.\t.\t2\n",
            "2\t.\t32\t4\n",
            ".\t.\t64\t8\n",
            ".\t2\t4\t4\n",
            "\n",
            "Step 57, action=1, reward=0, score=458.0\n",
            ".\t.\t.\t2\n",
            ".\t.\t32\t4\n",
            ".\t2\t64\t8\n",
            "2\t2\t4\t4\n",
            "\n",
            "Step 58, action=2, reward=12, score=470.0\n",
            "2\t.\t2\t.\n",
            "32\t4\t.\t.\n",
            "2\t64\t8\t.\n",
            "4\t8\t.\t.\n",
            "\n",
            "Step 59, action=0, reward=0, score=470.0\n",
            "2\t4\t2\t.\n",
            "32\t64\t8\t2\n",
            "2\t8\t.\t.\n",
            "4\t.\t.\t.\n",
            "\n",
            "Step 60, action=3, reward=0, score=470.0\n",
            ".\t2\t4\t2\n",
            "32\t64\t8\t2\n",
            "2\t.\t2\t8\n",
            ".\t.\t.\t4\n",
            "\n",
            "Step 61, action=1, reward=4, score=474.0\n",
            ".\t.\t.\t.\n",
            ".\t4\t4\t4\n",
            "32\t2\t8\t8\n",
            "2\t64\t2\t4\n",
            "\n",
            "Step 62, action=2, reward=24, score=498.0\n",
            ".\t.\t.\t.\n",
            "8\t4\t.\t2\n",
            "32\t2\t16\t.\n",
            "2\t64\t2\t4\n",
            "\n",
            "Step 63, action=0, reward=0, score=498.0\n",
            "8\t4\t16\t2\n",
            "32\t2\t2\t4\n",
            "2\t64\t.\t.\n",
            ".\t.\t2\t.\n",
            "\n",
            "Step 64, action=3, reward=4, score=502.0\n",
            "8\t4\t16\t2\n",
            "2\t32\t4\t4\n",
            ".\t.\t2\t64\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 65, action=1, reward=0, score=502.0\n",
            "2\t.\t.\t2\n",
            ".\t.\t16\t4\n",
            "8\t4\t4\t64\n",
            "2\t32\t2\t2\n",
            "\n",
            "Step 66, action=2, reward=16, score=518.0\n",
            "4\t.\t.\t.\n",
            "16\t4\t.\t.\n",
            "8\t8\t64\t.\n",
            "2\t32\t4\t2\n",
            "\n",
            "Step 67, action=0, reward=0, score=518.0\n",
            "4\t4\t64\t2\n",
            "16\t8\t4\t.\n",
            "8\t32\t.\t2\n",
            "2\t.\t.\t.\n",
            "\n",
            "Step 68, action=3, reward=8, score=526.0\n",
            ".\t8\t64\t2\n",
            "4\t16\t8\t4\n",
            ".\t8\t32\t2\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 69, action=1, reward=4, score=530.0\n",
            "2\t.\t.\t.\n",
            ".\t8\t64\t2\n",
            ".\t16\t8\t4\n",
            "4\t8\t32\t4\n",
            "\n",
            "Step 70, action=2, reward=0, score=530.0\n",
            "2\t.\t.\t2\n",
            "8\t64\t2\t.\n",
            "16\t8\t4\t.\n",
            "4\t8\t32\t4\n",
            "\n",
            "Step 71, action=0, reward=16, score=546.0\n",
            "2\t64\t2\t2\n",
            "8\t16\t4\t4\n",
            "16\t.\t32\t.\n",
            "4\t2\t.\t.\n",
            "\n",
            "Step 72, action=3, reward=12, score=558.0\n",
            ".\t2\t64\t4\n",
            ".\t8\t16\t8\n",
            ".\t.\t16\t32\n",
            ".\t2\t4\t2\n",
            "\n",
            "Step 73, action=1, reward=32, score=590.0\n",
            ".\t.\t.\t4\n",
            ".\t2\t64\t8\n",
            "2\t8\t32\t32\n",
            ".\t2\t4\t2\n",
            "\n",
            "Step 74, action=2, reward=64, score=654.0\n",
            "4\t2\t.\t.\n",
            "2\t64\t8\t.\n",
            "2\t8\t64\t.\n",
            "2\t4\t2\t.\n",
            "\n",
            "Step 75, action=0, reward=4, score=658.0\n",
            "4\t2\t8\t.\n",
            "4\t64\t64\t.\n",
            "2\t8\t2\t.\n",
            ".\t4\t.\t2\n",
            "\n",
            "Step 76, action=3, reward=128, score=786.0\n",
            ".\t4\t2\t8\n",
            ".\t.\t4\t128\n",
            ".\t2\t8\t2\n",
            ".\t2\t4\t2\n",
            "\n",
            "Step 77, action=1, reward=8, score=794.0\n",
            ".\t.\t2\t.\n",
            ".\t.\t4\t8\n",
            ".\t4\t8\t128\n",
            "2\t4\t4\t4\n",
            "\n",
            "Step 78, action=2, reward=8, score=802.0\n",
            "2\t.\t.\t.\n",
            "4\t8\t.\t.\n",
            "4\t8\t128\t.\n",
            "2\t8\t4\t2\n",
            "\n",
            "Step 79, action=0, reward=24, score=826.0\n",
            "2\t16\t128\t2\n",
            "8\t8\t4\t.\n",
            "2\t.\t.\t2\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 80, action=1, reward=4, score=830.0\n",
            "2\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "8\t16\t128\t.\n",
            "2\t8\t4\t4\n",
            "\n",
            "Step 81, action=2, reward=8, score=838.0\n",
            "2\t.\t.\t.\n",
            "2\t.\t2\t.\n",
            "8\t16\t128\t.\n",
            "2\t8\t8\t.\n",
            "\n",
            "Step 82, action=0, reward=4, score=842.0\n",
            "4\t16\t2\t.\n",
            "8\t8\t128\t.\n",
            "2\t.\t8\t2\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 83, action=3, reward=16, score=858.0\n",
            ".\t4\t16\t2\n",
            ".\t2\t16\t128\n",
            ".\t2\t8\t2\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 84, action=1, reward=36, score=894.0\n",
            ".\t.\t.\t4\n",
            ".\t.\t.\t2\n",
            ".\t4\t32\t128\n",
            ".\t4\t8\t2\n",
            "\n",
            "Step 85, action=2, reward=0, score=894.0\n",
            "4\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "4\t32\t128\t4\n",
            "4\t8\t2\t.\n",
            "\n",
            "Step 86, action=0, reward=8, score=902.0\n",
            "4\t32\t128\t4\n",
            "2\t8\t2\t.\n",
            "8\t.\t.\t.\n",
            ".\t2\t.\t.\n",
            "\n",
            "Step 87, action=3, reward=0, score=902.0\n",
            "4\t32\t128\t4\n",
            ".\t2\t8\t2\n",
            ".\t.\t.\t8\n",
            "2\t.\t.\t2\n",
            "\n",
            "Step 88, action=1, reward=0, score=902.0\n",
            ".\t.\t2\t4\n",
            ".\t.\t.\t2\n",
            "4\t32\t128\t8\n",
            "2\t2\t8\t2\n",
            "\n",
            "Step 89, action=2, reward=4, score=906.0\n",
            "2\t4\t2\t.\n",
            "2\t.\t.\t.\n",
            "4\t32\t128\t8\n",
            "4\t8\t2\t.\n",
            "\n",
            "Step 90, action=0, reward=12, score=918.0\n",
            "4\t4\t2\t8\n",
            "8\t32\t128\t.\n",
            ".\t8\t2\t2\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 91, action=3, reward=12, score=930.0\n",
            ".\t8\t2\t8\n",
            "2\t8\t32\t128\n",
            ".\t.\t8\t4\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 92, action=1, reward=16, score=946.0\n",
            ".\t.\t.\t.\n",
            ".\t.\t2\t8\n",
            "2\t.\t32\t128\n",
            "2\t16\t8\t4\n",
            "\n",
            "Step 93, action=2, reward=0, score=946.0\n",
            ".\t.\t2\t.\n",
            "2\t8\t.\t.\n",
            "2\t32\t128\t.\n",
            "2\t16\t8\t4\n",
            "\n",
            "Step 94, action=0, reward=4, score=950.0\n",
            "4\t8\t2\t4\n",
            "2\t32\t128\t.\n",
            ".\t16\t8\t.\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 95, action=3, reward=0, score=950.0\n",
            "4\t8\t2\t4\n",
            ".\t2\t32\t128\n",
            ".\t2\t16\t8\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 96, action=1, reward=4, score=954.0\n",
            "2\t.\t.\t4\n",
            ".\t.\t2\t128\n",
            ".\t8\t32\t8\n",
            "4\t4\t16\t2\n",
            "\n",
            "Step 97, action=2, reward=8, score=962.0\n",
            "2\t4\t.\t.\n",
            "2\t128\t2\t.\n",
            "8\t32\t8\t.\n",
            "8\t16\t2\t.\n",
            "\n",
            "Step 98, action=0, reward=20, score=982.0\n",
            "4\t4\t2\t.\n",
            "16\t128\t8\t.\n",
            ".\t32\t2\t.\n",
            ".\t16\t.\t2\n",
            "\n",
            "Step 99, action=0, reward=0, score=982.0\n",
            "4\t4\t2\t2\n",
            "16\t128\t8\t.\n",
            ".\t32\t2\t.\n",
            ".\t16\t.\t2\n",
            "\n",
            "Step 100, action=3, reward=12, score=994.0\n",
            ".\t2\t8\t4\n",
            ".\t16\t128\t8\n",
            ".\t.\t32\t2\n",
            ".\t.\t16\t2\n",
            "\n",
            "Step 101, action=1, reward=4, score=998.0\n",
            "2\t.\t8\t.\n",
            ".\t.\t128\t4\n",
            ".\t2\t32\t8\n",
            ".\t16\t16\t4\n",
            "\n",
            "Step 102, action=2, reward=32, score=1030.0\n",
            "2\t8\t.\t2\n",
            "128\t4\t.\t.\n",
            "2\t32\t8\t.\n",
            "32\t4\t.\t.\n",
            "\n",
            "Step 103, action=0, reward=0, score=1030.0\n",
            "2\t8\t8\t2\n",
            "128\t4\t2\t.\n",
            "2\t32\t.\t.\n",
            "32\t4\t.\t.\n",
            "\n",
            "Step 104, action=3, reward=16, score=1046.0\n",
            ".\t2\t16\t2\n",
            ".\t128\t4\t2\n",
            "2\t.\t2\t32\n",
            ".\t.\t32\t4\n",
            "\n",
            "Step 105, action=1, reward=4, score=1050.0\n",
            ".\t2\t16\t.\n",
            ".\t.\t4\t4\n",
            ".\t2\t2\t32\n",
            "2\t128\t32\t4\n",
            "\n",
            "Step 106, action=2, reward=12, score=1062.0\n",
            "2\t16\t.\t.\n",
            "8\t2\t.\t.\n",
            "4\t32\t.\t.\n",
            "2\t128\t32\t4\n",
            "\n",
            "Step 107, action=0, reward=0, score=1062.0\n",
            "2\t16\t32\t4\n",
            "8\t2\t2\t.\n",
            "4\t32\t.\t.\n",
            "2\t128\t.\t.\n",
            "\n",
            "Step 108, action=3, reward=4, score=1066.0\n",
            "2\t16\t32\t4\n",
            ".\t.\t8\t4\n",
            "2\t.\t4\t32\n",
            ".\t.\t2\t128\n",
            "\n",
            "Step 109, action=1, reward=12, score=1078.0\n",
            ".\t.\t32\t.\n",
            ".\t2\t8\t8\n",
            ".\t.\t4\t32\n",
            "4\t16\t2\t128\n",
            "\n",
            "Step 110, action=2, reward=16, score=1094.0\n",
            "32\t.\t.\t.\n",
            "2\t16\t.\t.\n",
            "4\t32\t2\t.\n",
            "4\t16\t2\t128\n",
            "\n",
            "Step 111, action=0, reward=12, score=1106.0\n",
            "32\t16\t4\t128\n",
            "2\t32\t.\t2\n",
            "8\t16\t.\t.\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 112, action=3, reward=0, score=1106.0\n",
            "32\t16\t4\t128\n",
            ".\t2\t32\t2\n",
            ".\t.\t8\t16\n",
            ".\t.\t4\t.\n",
            "\n",
            "Step 113, action=1, reward=0, score=1106.0\n",
            ".\t2\t4\t.\n",
            ".\t.\t32\t128\n",
            ".\t16\t8\t2\n",
            "32\t2\t4\t16\n",
            "\n",
            "Step 114, action=2, reward=0, score=1106.0\n",
            "2\t4\t2\t.\n",
            "32\t128\t.\t.\n",
            "16\t8\t2\t.\n",
            "32\t2\t4\t16\n",
            "\n",
            "Step 115, action=0, reward=4, score=1110.0\n",
            "2\t4\t4\t16\n",
            "32\t128\t4\t2\n",
            "16\t8\t.\t.\n",
            "32\t2\t.\t.\n",
            "\n",
            "Step 116, action=3, reward=8, score=1118.0\n",
            ".\t2\t8\t16\n",
            "32\t128\t4\t2\n",
            ".\t.\t16\t8\n",
            "2\t.\t32\t2\n",
            "\n",
            "Step 117, action=2, reward=0, score=1118.0\n",
            "2\t8\t16\t2\n",
            "32\t128\t4\t2\n",
            "16\t8\t.\t.\n",
            "2\t32\t2\t.\n",
            "\n",
            "Step 118, action=3, reward=0, score=1118.0\n",
            "2\t8\t16\t2\n",
            "32\t128\t4\t2\n",
            ".\t2\t16\t8\n",
            ".\t2\t32\t2\n",
            "\n",
            "Step 119, action=1, reward=8, score=1126.0\n",
            ".\t2\t16\t.\n",
            ".\t8\t4\t4\n",
            "2\t128\t16\t8\n",
            "32\t4\t32\t2\n",
            "\n",
            "Step 120, action=1, reward=-1.0, score=1125.0\n",
            ".\t2\t16\t.\n",
            ".\t8\t4\t4\n",
            "2\t128\t16\t8\n",
            "32\t4\t32\t2\n",
            "\n",
            "Step 121, action=2, reward=8, score=1133.0\n",
            "2\t16\t2\t.\n",
            "8\t8\t.\t.\n",
            "2\t128\t16\t8\n",
            "32\t4\t32\t2\n",
            "\n",
            "Step 122, action=2, reward=16, score=1149.0\n",
            "2\t16\t2\t.\n",
            "16\t2\t.\t.\n",
            "2\t128\t16\t8\n",
            "32\t4\t32\t2\n",
            "\n",
            "Step 123, action=2, reward=-1.0, score=1148.0\n",
            "2\t16\t2\t.\n",
            "16\t2\t.\t.\n",
            "2\t128\t16\t8\n",
            "32\t4\t32\t2\n",
            "\n",
            "Step 124, action=0, reward=0, score=1148.0\n",
            "2\t16\t2\t8\n",
            "16\t2\t16\t2\n",
            "2\t128\t32\t.\n",
            "32\t4\t.\t4\n",
            "\n",
            "Step 125, action=3, reward=8, score=1156.0\n",
            "2\t16\t2\t8\n",
            "16\t2\t16\t2\n",
            ".\t2\t128\t32\n",
            "2\t.\t32\t8\n",
            "\n",
            "Step 126, action=1, reward=4, score=1160.0\n",
            ".\t2\t2\t8\n",
            "2\t.\t16\t2\n",
            "16\t16\t128\t32\n",
            "2\t4\t32\t8\n",
            "\n",
            "Step 127, action=3, reward=36, score=1196.0\n",
            ".\t2\t4\t8\n",
            ".\t2\t16\t2\n",
            ".\t32\t128\t32\n",
            "2\t4\t32\t8\n",
            "\n",
            "Step 128, action=2, reward=0, score=1196.0\n",
            "2\t4\t8\t.\n",
            "2\t16\t2\t.\n",
            "32\t128\t32\t2\n",
            "2\t4\t32\t8\n",
            "\n",
            "Step 129, action=0, reward=68, score=1264.0\n",
            "4\t4\t8\t2\n",
            "32\t16\t2\t8\n",
            "2\t128\t64\t2\n",
            ".\t4\t.\t.\n",
            "\n",
            "Step 130, action=3, reward=8, score=1272.0\n",
            ".\t8\t8\t2\n",
            "32\t16\t2\t8\n",
            "2\t128\t64\t2\n",
            ".\t2\t.\t4\n",
            "\n",
            "Step 131, action=1, reward=0, score=1272.0\n",
            ".\t8\t2\t2\n",
            ".\t16\t8\t8\n",
            "32\t128\t2\t2\n",
            "2\t2\t64\t4\n",
            "\n",
            "Step 132, action=3, reward=28, score=1300.0\n",
            ".\t.\t8\t4\n",
            ".\t2\t16\t16\n",
            ".\t32\t128\t4\n",
            ".\t4\t64\t4\n",
            "\n",
            "Step 133, action=2, reward=32, score=1332.0\n",
            "8\t4\t.\t.\n",
            "2\t32\t2\t.\n",
            "32\t128\t4\t.\n",
            "4\t64\t4\t.\n",
            "\n",
            "Step 134, action=0, reward=8, score=1340.0\n",
            "8\t4\t2\t.\n",
            "2\t32\t8\t.\n",
            "32\t128\t.\t.\n",
            "4\t64\t.\t2\n",
            "\n",
            "Step 135, action=0, reward=0, score=1340.0\n",
            "8\t4\t2\t2\n",
            "2\t32\t8\t.\n",
            "32\t128\t2\t.\n",
            "4\t64\t.\t.\n",
            "\n",
            "Step 136, action=3, reward=4, score=1344.0\n",
            ".\t8\t4\t4\n",
            ".\t2\t32\t8\n",
            ".\t32\t128\t2\n",
            "4\t.\t4\t64\n",
            "\n",
            "Step 137, action=2, reward=16, score=1360.0\n",
            "8\t8\t2\t.\n",
            "2\t32\t8\t.\n",
            "32\t128\t2\t.\n",
            "8\t64\t.\t.\n",
            "\n",
            "Step 138, action=2, reward=16, score=1376.0\n",
            "16\t2\t.\t.\n",
            "2\t32\t8\t.\n",
            "32\t128\t2\t.\n",
            "8\t64\t2\t.\n",
            "\n",
            "Step 139, action=0, reward=4, score=1380.0\n",
            "16\t2\t8\t.\n",
            "2\t32\t4\t.\n",
            "32\t128\t2\t.\n",
            "8\t64\t.\t.\n",
            "\n",
            "Step 140, action=2, reward=-1.0, score=1379.0\n",
            "16\t2\t8\t.\n",
            "2\t32\t4\t.\n",
            "32\t128\t2\t.\n",
            "8\t64\t.\t.\n",
            "\n",
            "Step 141, action=0, reward=-1.0, score=1378.0\n",
            "16\t2\t8\t.\n",
            "2\t32\t4\t.\n",
            "32\t128\t2\t.\n",
            "8\t64\t.\t.\n",
            "\n",
            "Step 142, action=3, reward=0, score=1378.0\n",
            ".\t16\t2\t8\n",
            ".\t2\t32\t4\n",
            ".\t32\t128\t2\n",
            "2\t.\t8\t64\n",
            "\n",
            "Step 143, action=2, reward=0, score=1378.0\n",
            "16\t2\t8\t.\n",
            "2\t32\t4\t2\n",
            "32\t128\t2\t.\n",
            "2\t8\t64\t.\n",
            "\n",
            "Step 144, action=0, reward=0, score=1378.0\n",
            "16\t2\t8\t2\n",
            "2\t32\t4\t.\n",
            "32\t128\t2\t2\n",
            "2\t8\t64\t.\n",
            "\n",
            "Step 145, action=0, reward=4, score=1382.0\n",
            "16\t2\t8\t4\n",
            "2\t32\t4\t2\n",
            "32\t128\t2\t.\n",
            "2\t8\t64\t.\n",
            "\n",
            "Step 146, action=0, reward=-1.0, score=1381.0\n",
            "16\t2\t8\t4\n",
            "2\t32\t4\t2\n",
            "32\t128\t2\t.\n",
            "2\t8\t64\t.\n",
            "\n",
            "Step 147, action=3, reward=0, score=1381.0\n",
            "16\t2\t8\t4\n",
            "2\t32\t4\t2\n",
            "2\t32\t128\t2\n",
            ".\t2\t8\t64\n",
            "\n",
            "Step 148, action=1, reward=72, score=1453.0\n",
            ".\t2\t8\t.\n",
            ".\t2\t4\t4\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 149, action=2, reward=8, score=1461.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t.\t4\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 150, action=2, reward=0, score=1461.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 151, action=2, reward=-1.0, score=1460.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 152, action=2, reward=-1.0, score=1459.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 153, action=2, reward=-1.0, score=1458.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 154, action=2, reward=-1.0, score=1457.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 155, action=2, reward=-1.0, score=1456.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 156, action=2, reward=-1.0, score=1455.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 157, action=2, reward=-1.0, score=1454.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 158, action=2, reward=-1.0, score=1453.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 159, action=2, reward=-1.0, score=1452.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 160, action=2, reward=-1.0, score=1451.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 161, action=2, reward=-1.0, score=1450.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 162, action=2, reward=-1.0, score=1449.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 163, action=2, reward=-1.0, score=1448.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 164, action=2, reward=-1.0, score=1447.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 165, action=2, reward=-1.0, score=1446.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 166, action=2, reward=-1.0, score=1445.0\n",
            "2\t8\t.\t.\n",
            "2\t8\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            "\n",
            "Step 167, action=0, reward=20, score=1465.0\n",
            "4\t16\t4\t2\n",
            "16\t64\t128\t4\n",
            "4\t2\t8\t64\n",
            ".\t4\t.\t.\n",
            "\n",
            "Step 168, action=1, reward=0, score=1465.0\n",
            ".\t16\t2\t.\n",
            "4\t64\t4\t2\n",
            "16\t2\t128\t4\n",
            "4\t4\t8\t64\n",
            "\n",
            "Step 169, action=3, reward=8, score=1473.0\n",
            "4\t.\t16\t2\n",
            "4\t64\t4\t2\n",
            "16\t2\t128\t4\n",
            ".\t8\t8\t64\n",
            "\n",
            "Step 170, action=0, reward=12, score=1485.0\n",
            "8\t64\t16\t4\n",
            "16\t2\t4\t4\n",
            ".\t8\t128\t64\n",
            ".\t.\t8\t2\n",
            "\n",
            "Step 171, action=1, reward=8, score=1493.0\n",
            ".\t.\t16\t2\n",
            ".\t64\t4\t8\n",
            "8\t2\t128\t64\n",
            "16\t8\t8\t2\n",
            "\n",
            "Step 172, action=2, reward=16, score=1509.0\n",
            "16\t2\t2\t.\n",
            "64\t4\t8\t.\n",
            "8\t2\t128\t64\n",
            "16\t16\t2\t.\n",
            "\n",
            "Step 173, action=0, reward=0, score=1509.0\n",
            "16\t2\t2\t64\n",
            "64\t4\t8\t2\n",
            "8\t2\t128\t.\n",
            "16\t16\t2\t.\n",
            "\n",
            "Step 174, action=3, reward=36, score=1545.0\n",
            ".\t16\t4\t64\n",
            "64\t4\t8\t2\n",
            ".\t8\t2\t128\n",
            ".\t2\t32\t2\n",
            "\n",
            "Step 175, action=1, reward=0, score=1545.0\n",
            "2\t16\t4\t64\n",
            ".\t4\t8\t2\n",
            ".\t8\t2\t128\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 176, action=2, reward=0, score=1545.0\n",
            "2\t16\t4\t64\n",
            "4\t8\t2\t2\n",
            "8\t2\t128\t.\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 177, action=2, reward=4, score=1549.0\n",
            "2\t16\t4\t64\n",
            "4\t8\t4\t.\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 178, action=2, reward=-1.0, score=1548.0\n",
            "2\t16\t4\t64\n",
            "4\t8\t4\t.\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 179, action=2, reward=-1.0, score=1547.0\n",
            "2\t16\t4\t64\n",
            "4\t8\t4\t.\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 180, action=2, reward=-1.0, score=1546.0\n",
            "2\t16\t4\t64\n",
            "4\t8\t4\t.\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 181, action=2, reward=-1.0, score=1545.0\n",
            "2\t16\t4\t64\n",
            "4\t8\t4\t.\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 182, action=2, reward=-1.0, score=1544.0\n",
            "2\t16\t4\t64\n",
            "4\t8\t4\t.\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 183, action=2, reward=-1.0, score=1543.0\n",
            "2\t16\t4\t64\n",
            "4\t8\t4\t.\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 184, action=3, reward=0, score=1543.0\n",
            "2\t16\t4\t64\n",
            "2\t4\t8\t4\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 185, action=2, reward=-1.0, score=1542.0\n",
            "2\t16\t4\t64\n",
            "2\t4\t8\t4\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 186, action=3, reward=-1.0, score=1541.0\n",
            "2\t16\t4\t64\n",
            "2\t4\t8\t4\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 187, action=3, reward=-1.0, score=1540.0\n",
            "2\t16\t4\t64\n",
            "2\t4\t8\t4\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 188, action=2, reward=-1.0, score=1539.0\n",
            "2\t16\t4\t64\n",
            "2\t4\t8\t4\n",
            "8\t2\t128\t2\n",
            "64\t2\t32\t2\n",
            "\n",
            "Step 189, action=1, reward=12, score=1551.0\n",
            ".\t.\t4\t2\n",
            "4\t16\t8\t64\n",
            "8\t4\t128\t4\n",
            "64\t4\t32\t4\n",
            "\n",
            "Step 190, action=2, reward=0, score=1551.0\n",
            "4\t2\t.\t2\n",
            "4\t16\t8\t64\n",
            "8\t4\t128\t4\n",
            "64\t4\t32\t4\n",
            "\n",
            "Step 191, action=0, reward=24, score=1575.0\n",
            "8\t2\t8\t2\n",
            "8\t16\t128\t64\n",
            "64\t8\t32\t8\n",
            "4\t.\t.\t.\n",
            "\n",
            "Step 192, action=3, reward=0, score=1575.0\n",
            "8\t2\t8\t2\n",
            "8\t16\t128\t64\n",
            "64\t8\t32\t8\n",
            "2\t.\t.\t4\n",
            "\n",
            "Step 193, action=1, reward=16, score=1591.0\n",
            ".\t2\t.\t2\n",
            "16\t2\t8\t64\n",
            "64\t16\t128\t8\n",
            "2\t8\t32\t4\n",
            "\n",
            "Step 194, action=1, reward=4, score=1595.0\n",
            "2\t.\t.\t2\n",
            "16\t4\t8\t64\n",
            "64\t16\t128\t8\n",
            "2\t8\t32\t4\n",
            "\n",
            "Step 195, action=1, reward=-1.0, score=1594.0\n",
            "2\t.\t.\t2\n",
            "16\t4\t8\t64\n",
            "64\t16\t128\t8\n",
            "2\t8\t32\t4\n",
            "\n",
            "Step 196, action=3, reward=4, score=1598.0\n",
            "2\t.\t.\t4\n",
            "16\t4\t8\t64\n",
            "64\t16\t128\t8\n",
            "2\t8\t32\t4\n",
            "\n",
            "Step 197, action=2, reward=0, score=1598.0\n",
            "2\t4\t2\t.\n",
            "16\t4\t8\t64\n",
            "64\t16\t128\t8\n",
            "2\t8\t32\t4\n",
            "\n",
            "Step 198, action=2, reward=-1.0, score=1597.0\n",
            "2\t4\t2\t.\n",
            "16\t4\t8\t64\n",
            "64\t16\t128\t8\n",
            "2\t8\t32\t4\n",
            "\n",
            "Step 199, action=0, reward=8, score=1605.0\n",
            "2\t8\t2\t64\n",
            "16\t16\t8\t8\n",
            "64\t8\t128\t4\n",
            "2\t.\t32\t2\n",
            "\n",
            "Step 200, action=1, reward=0, score=1605.0\n",
            "2\t2\t2\t64\n",
            "16\t8\t8\t8\n",
            "64\t16\t128\t4\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 201, action=2, reward=20, score=1625.0\n",
            "4\t2\t64\t.\n",
            "16\t16\t8\t2\n",
            "64\t16\t128\t4\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 202, action=1, reward=32, score=1657.0\n",
            "4\t2\t64\t.\n",
            "16\t2\t8\t2\n",
            "64\t32\t128\t4\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 203, action=2, reward=-1.0, score=1656.0\n",
            "4\t2\t64\t.\n",
            "16\t2\t8\t2\n",
            "64\t32\t128\t4\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 204, action=1, reward=4, score=1660.0\n",
            "4\t4\t64\t.\n",
            "16\t4\t8\t2\n",
            "64\t32\t128\t4\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 205, action=3, reward=8, score=1668.0\n",
            ".\t2\t8\t64\n",
            "16\t4\t8\t2\n",
            "64\t32\t128\t4\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 206, action=2, reward=0, score=1668.0\n",
            "2\t8\t64\t2\n",
            "16\t4\t8\t2\n",
            "64\t32\t128\t4\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 207, action=2, reward=-1.0, score=1667.0\n",
            "2\t8\t64\t2\n",
            "16\t4\t8\t2\n",
            "64\t32\t128\t4\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 208, action=1, reward=4, score=1671.0\n",
            "2\t8\t64\t2\n",
            "16\t4\t8\t4\n",
            "64\t32\t128\t4\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 209, action=3, reward=-1.0, score=1670.0\n",
            "2\t8\t64\t2\n",
            "16\t4\t8\t4\n",
            "64\t32\t128\t4\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 210, action=2, reward=-1.0, score=1669.0\n",
            "2\t8\t64\t2\n",
            "16\t4\t8\t4\n",
            "64\t32\t128\t4\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 211, action=1, reward=8, score=1677.0\n",
            "2\t8\t64\t2\n",
            "16\t4\t8\t2\n",
            "64\t32\t128\t8\n",
            "2\t8\t32\t2\n",
            "\n",
            "Step 212, action=1, reward=4, score=1681.0\n",
            "2\t8\t64\t2\n",
            "16\t4\t8\t4\n",
            "64\t32\t128\t8\n",
            "2\t8\t32\t2\n",
            "\n",
            "Episode finished.\n",
            "Final score: 1681.0, max tile: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git config --global user.email \"sarah04@mit.edu\"\n",
        "!git config --global user.name \"sarah-mokhtar\"\n"
      ],
      "metadata": {
        "id": "KeXlm3vb86nX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/sarah-mokhtar/RL-Project-2048.git\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1RWV9bXcNaGZ",
        "outputId": "ec09ae98-8514-44bb-f6a6-2e4280726ac5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'RL-Project-2048' already exists and is not an empty directory.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "!cp 2048\\ RL.ipynb /content/RL-Project-2048/\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "whfNYGcfPrK6",
        "outputId": "eb41e37f-5197-415e-ce7c-7e93be8f0af2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "cp: cannot stat '2048 RL.ipynb': No such file or directory\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%cd RL-Project-2048\n",
        "\n",
        "!git add .\n",
        "!git commit -m \"PPO\"\n",
        "!git push\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o5432T8BPM9d",
        "outputId": "aa2e925f-b3fe-42bd-bae9-35cacaf73c8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Errno 2] No such file or directory: 'RL-Project-2048'\n",
            "/content/RL-Project-2048\n",
            "On branch main\n",
            "Your branch is up to date with 'origin/main'.\n",
            "\n",
            "nothing to commit, working tree clean\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Clone a team/organization repo\n",
        "\n",
        "from getpass import getpass\n",
        "\n",
        "# Get your Personal Access Token\n",
        "print(\"Get token from: https://github.com/settings/tokens\")\n",
        "print(\"Make sure 'repo' scope is checked!\")\n",
        "token = getpass('Paste your GitHub Personal Access Token: ')\n",
        "\n",
        "# Team/Organization repo details\n",
        "org_or_username = \"team-name-or-org\"  # The organization/team name\n",
        "repo_name = \"repo-name\"  # The repository name\n",
        "\n",
        "# Clone with authentication\n",
        "!git clone https://{token}@github.com/{org_or_username}/{repo_name}.git\n",
        "\n",
        "# Navigate into repo\n",
        "%cd {repo_name}\n",
        "\n",
        "# Configure your git identity (important for team repos)\n",
        "!git config user.email \"your-email@example.com\"\n",
        "!git config user.name \"Your Name\"\n",
        "\n",
        "print(\"‚úÖ Team repo cloned successfully!\")"
      ],
      "metadata": {
        "id": "o6JjYhRe80ka"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/Ali-Backour/2048_RL.git"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZxxAXmtb9EzT",
        "outputId": "7cda8cc5-e39c-4de9-b191-4b712d3960ce"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into '2048_RL'...\n",
            "fatal: could not read Username for 'https://github.com': No such device or address\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "\n",
        "class Game2048EnvV2(gym.Env):\n",
        "    \"\"\"\n",
        "    Version 2 of the 2048 environment.\n",
        "    Same logic as V1, but you can modify reward shaping etc. later.\n",
        "\n",
        "    Normalized observation: exponents / 15.0 ‚Üí float32 in [0,1]\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"ansi\"], \"render_fps\": 60}\n",
        "\n",
        "    def __init__(self, render_mode=None, target_tile=2048):\n",
        "        super().__init__()\n",
        "        self.board_size = 4\n",
        "        self.target_tile = target_tile\n",
        "\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0.0,\n",
        "            high=1.0,\n",
        "            shape=(4, 4),\n",
        "            dtype=np.float32\n",
        "        )\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.board = np.zeros((4, 4), dtype=np.int32)\n",
        "        self.score = 0.0\n",
        "        self.prev_max_tile = 0\n",
        "        self.rng = np.random.default_rng()\n",
        "\n",
        "    # ---------- Helpers ----------\n",
        "    def _get_obs(self):\n",
        "        return self.board.astype(np.float32) / 15.0\n",
        "\n",
        "    def _slide_and_merge_line(self, line):\n",
        "        non_zero = line[line != 0].tolist()\n",
        "        new = []\n",
        "        reward = 0\n",
        "        i = 0\n",
        "        while i < len(non_zero):\n",
        "            if i+1 < len(non_zero) and non_zero[i] == non_zero[i+1]:\n",
        "                exp = non_zero[i] + 1\n",
        "                new.append(exp)\n",
        "                reward += 2**exp\n",
        "                i += 2\n",
        "            else:\n",
        "                new.append(non_zero[i])\n",
        "                i += 1\n",
        "        new += [0]*(len(line)-len(new))\n",
        "        return np.array(new, dtype=np.int32), reward\n",
        "\n",
        "    def _add_random_tile(self):\n",
        "        empty = list(zip(*np.where(self.board == 0)))\n",
        "        if not empty:\n",
        "            return\n",
        "        r, c = empty[self.rng.integers(len(empty))]\n",
        "        self.board[r,c] = 1 if self.rng.random()<0.9 else 2\n",
        "\n",
        "    def _can_move(self):\n",
        "        if np.any(self.board==0): return True\n",
        "        for r in range(4):\n",
        "            for c in range(3):\n",
        "                if self.board[r,c]==self.board[r,c+1]:\n",
        "                    return True\n",
        "        for c in range(4):\n",
        "            for r in range(3):\n",
        "                if self.board[r,c]==self.board[r+1,c]:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def _get_max_tile(self):\n",
        "        return 0 if self.board.max()==0 else 2**int(self.board.max())\n",
        "\n",
        "    # ---------- Gym API ----------\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.board[:] = 0\n",
        "        self.score = 0.0\n",
        "        self.prev_max_tile = 0\n",
        "\n",
        "        self._add_random_tile()\n",
        "        self._add_random_tile()\n",
        "\n",
        "        return self._get_obs(), {\"score\":0, \"max_tile\":self._get_max_tile()}\n",
        "\n",
        "    def step(self, action):\n",
        "        old_board = self.board.copy()\n",
        "        reward = 0\n",
        "\n",
        "        if action == 0:\n",
        "            for c in range(4):\n",
        "                new_line, r = self._slide_and_merge_line(self.board[:,c])\n",
        "                self.board[:,c] = new_line\n",
        "                reward += r\n",
        "        elif action == 1:\n",
        "            for c in range(4):\n",
        "                new_line, r = self._slide_and_merge_line(self.board[:,c][::-1])\n",
        "                self.board[:,c] = new_line[::-1]\n",
        "                reward += r\n",
        "        elif action == 2:\n",
        "            for r in range(4):\n",
        "                new_line, rwd = self._slide_and_merge_line(self.board[r])\n",
        "                self.board[r] = new_line\n",
        "                reward += rwd\n",
        "        elif action == 3:\n",
        "            for r in range(4):\n",
        "                new_line, rwd = self._slide_and_merge_line(self.board[r][::-1])\n",
        "                self.board[r] = new_line[::-1]\n",
        "                reward += rwd\n",
        "\n",
        "        moved = not np.array_equal(old_board, self.board)\n",
        "\n",
        "        if not moved:\n",
        "            reward -= 2.0\n",
        "        else:\n",
        "            self._add_random_tile()\n",
        "\n",
        "        max_tile = self._get_max_tile()\n",
        "\n",
        "        # reward shaping\n",
        "        if max_tile > self.prev_max_tile:\n",
        "            reward += 0.5\n",
        "        self.prev_max_tile = max_tile\n",
        "\n",
        "        reward += 0.01 * np.sum(self.board==0)\n",
        "        self.score += reward\n",
        "\n",
        "        terminated = (not self._can_move()) or (max_tile >= self.target_tile)\n",
        "        truncated = False\n",
        "\n",
        "        return self._get_obs(), reward, terminated, truncated, {\n",
        "            \"score\": self.score,\n",
        "            \"max_tile\": max_tile\n",
        "        }\n",
        "\n",
        "    def _board_to_string(self):\n",
        "        rows = []\n",
        "        for row in self.board:\n",
        "            r = []\n",
        "            for exp in row:\n",
        "                r.append(\".\" if exp==0 else str(2**exp))\n",
        "            rows.append(\"\\t\".join(r))\n",
        "        return \"\\n\".join(rows)\n"
      ],
      "metadata": {
        "id": "PSQhMFcB9osr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "env_v2 = Game2048EnvV2()\n",
        "\n",
        "policy_kwargs_v2 = dict(\n",
        "    net_arch=[dict(pi=[256, 256, 256],\n",
        "                   vf=[256, 256, 256])]\n",
        ")\n",
        "\n",
        "model_v2 = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    env_v2,\n",
        "    policy_kwargs=policy_kwargs_v2,\n",
        "    learning_rate=3e-4,\n",
        "    n_steps=2048,\n",
        "    batch_size=512,\n",
        "    n_epochs=10,\n",
        "    gamma=0.99,\n",
        "    clip_range=0.15,\n",
        "    ent_coef=0.05,      # üîº higher entropy ‚Üí more stochastic policy\n",
        "    target_kl=0.02,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "model_v2.learn(total_timesteps=300_000)\n",
        "\n",
        "model_v2.save(\"ppo_2048_v2_simple\")\n",
        "print(\"Saved model ppo_2048_v2_simple.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v4IkIb7P9oap",
        "outputId": "9d62130c-1f51-4747-e8f3-3792e08c14fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 149      |\n",
            "|    ep_rew_mean     | 1.16e+03 |\n",
            "| time/              |          |\n",
            "|    fps             | 608      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 3        |\n",
            "|    total_timesteps | 2048     |\n",
            "---------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 147           |\n",
            "|    ep_rew_mean          | 1.13e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 592           |\n",
            "|    iterations           | 2             |\n",
            "|    time_elapsed         | 6             |\n",
            "|    total_timesteps      | 4096          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00044112414 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | -5.89e-05     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.36e+03      |\n",
            "|    n_updates            | 10            |\n",
            "|    policy_gradient_loss | -0.0014       |\n",
            "|    value_loss           | 1.84e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 147           |\n",
            "|    ep_rew_mean          | 1.14e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 585           |\n",
            "|    iterations           | 3             |\n",
            "|    time_elapsed         | 10            |\n",
            "|    total_timesteps      | 6144          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.2115495e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.00256       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.82e+03      |\n",
            "|    n_updates            | 20            |\n",
            "|    policy_gradient_loss | -3.96e-05     |\n",
            "|    value_loss           | 1.58e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 144           |\n",
            "|    ep_rew_mean          | 1.11e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 581           |\n",
            "|    iterations           | 4             |\n",
            "|    time_elapsed         | 14            |\n",
            "|    total_timesteps      | 8192          |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.9768918e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.00561       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 9.32e+03      |\n",
            "|    n_updates            | 30            |\n",
            "|    policy_gradient_loss | -0.000338     |\n",
            "|    value_loss           | 1.95e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 145           |\n",
            "|    ep_rew_mean          | 1.12e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 579           |\n",
            "|    iterations           | 5             |\n",
            "|    time_elapsed         | 17            |\n",
            "|    total_timesteps      | 10240         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.9867352e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.00413       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.49e+03      |\n",
            "|    n_updates            | 40            |\n",
            "|    policy_gradient_loss | -0.000162     |\n",
            "|    value_loss           | 1.67e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 144           |\n",
            "|    ep_rew_mean          | 1.08e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 578           |\n",
            "|    iterations           | 6             |\n",
            "|    time_elapsed         | 21            |\n",
            "|    total_timesteps      | 12288         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.1527376e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.00204       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.06e+03      |\n",
            "|    n_updates            | 50            |\n",
            "|    policy_gradient_loss | -0.00019      |\n",
            "|    value_loss           | 1.61e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 147           |\n",
            "|    ep_rew_mean          | 1.12e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 577           |\n",
            "|    iterations           | 7             |\n",
            "|    time_elapsed         | 24            |\n",
            "|    total_timesteps      | 14336         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.5848778e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.00193       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.77e+03      |\n",
            "|    n_updates            | 60            |\n",
            "|    policy_gradient_loss | -0.000263     |\n",
            "|    value_loss           | 1.28e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 146          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 576          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 28           |\n",
            "|    total_timesteps      | 16384        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.356829e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.000923     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.48e+03     |\n",
            "|    n_updates            | 70           |\n",
            "|    policy_gradient_loss | -0.000264    |\n",
            "|    value_loss           | 1.98e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 143          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 575          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 32           |\n",
            "|    total_timesteps      | 18432        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.819749e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.000839     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.75e+03     |\n",
            "|    n_updates            | 80           |\n",
            "|    policy_gradient_loss | -0.000117    |\n",
            "|    value_loss           | 1.69e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 143           |\n",
            "|    ep_rew_mean          | 1.07e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 575           |\n",
            "|    iterations           | 10            |\n",
            "|    time_elapsed         | 35            |\n",
            "|    total_timesteps      | 20480         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.5931603e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.000915      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.21e+03      |\n",
            "|    n_updates            | 90            |\n",
            "|    policy_gradient_loss | -9.93e-05     |\n",
            "|    value_loss           | 1.56e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 140          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 575          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 39           |\n",
            "|    total_timesteps      | 22528        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.521809e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.000606     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.4e+03      |\n",
            "|    n_updates            | 100          |\n",
            "|    policy_gradient_loss | -0.000157    |\n",
            "|    value_loss           | 1.55e+04     |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| rollout/                |                |\n",
            "|    ep_len_mean          | 138            |\n",
            "|    ep_rew_mean          | 1.02e+03       |\n",
            "| time/                   |                |\n",
            "|    fps                  | 573            |\n",
            "|    iterations           | 12             |\n",
            "|    time_elapsed         | 42             |\n",
            "|    total_timesteps      | 24576          |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 1.11446425e-05 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.15           |\n",
            "|    entropy_loss         | -1.39          |\n",
            "|    explained_variance   | 0.000319       |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 6.62e+03       |\n",
            "|    n_updates            | 110            |\n",
            "|    policy_gradient_loss | -9.85e-05      |\n",
            "|    value_loss           | 1.3e+04        |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 139          |\n",
            "|    ep_rew_mean          | 1.04e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 573          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 46           |\n",
            "|    total_timesteps      | 26624        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.209325e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.000327     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.21e+03     |\n",
            "|    n_updates            | 120          |\n",
            "|    policy_gradient_loss | -7.77e-05    |\n",
            "|    value_loss           | 1.28e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 984          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 573          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 50           |\n",
            "|    total_timesteps      | 28672        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.687318e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.000289     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.98e+03     |\n",
            "|    n_updates            | 130          |\n",
            "|    policy_gradient_loss | -0.000566    |\n",
            "|    value_loss           | 1.71e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 134           |\n",
            "|    ep_rew_mean          | 976           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 15            |\n",
            "|    time_elapsed         | 53            |\n",
            "|    total_timesteps      | 30720         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.5635912e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.000194      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.91e+03      |\n",
            "|    n_updates            | 140           |\n",
            "|    policy_gradient_loss | -0.000198     |\n",
            "|    value_loss           | 1.4e+04       |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 985          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 57           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.338881e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 0.000171     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.48e+03     |\n",
            "|    n_updates            | 150          |\n",
            "|    policy_gradient_loss | -0.00011     |\n",
            "|    value_loss           | 1.38e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 135           |\n",
            "|    ep_rew_mean          | 1e+03         |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 17            |\n",
            "|    time_elapsed         | 60            |\n",
            "|    total_timesteps      | 34816         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.6000064e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.00017       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.93e+03      |\n",
            "|    n_updates            | 160           |\n",
            "|    policy_gradient_loss | -0.000128     |\n",
            "|    value_loss           | 1.72e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 134           |\n",
            "|    ep_rew_mean          | 995           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 18            |\n",
            "|    time_elapsed         | 64            |\n",
            "|    total_timesteps      | 36864         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.5501103e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.000157      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.38e+03      |\n",
            "|    n_updates            | 170           |\n",
            "|    policy_gradient_loss | -0.000292     |\n",
            "|    value_loss           | 1.57e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 131           |\n",
            "|    ep_rew_mean          | 962           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 19            |\n",
            "|    time_elapsed         | 67            |\n",
            "|    total_timesteps      | 38912         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.0287949e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 0.000124      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.26e+03      |\n",
            "|    n_updates            | 180           |\n",
            "|    policy_gradient_loss | -0.000168     |\n",
            "|    value_loss           | 1.38e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 131           |\n",
            "|    ep_rew_mean          | 968           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 20            |\n",
            "|    time_elapsed         | 71            |\n",
            "|    total_timesteps      | 40960         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8174236e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 9.14e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.1e+03       |\n",
            "|    n_updates            | 190           |\n",
            "|    policy_gradient_loss | -0.000134     |\n",
            "|    value_loss           | 1.12e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 132           |\n",
            "|    ep_rew_mean          | 996           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 21            |\n",
            "|    time_elapsed         | 75            |\n",
            "|    total_timesteps      | 43008         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.9021077e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 9.02e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.07e+03      |\n",
            "|    n_updates            | 200           |\n",
            "|    policy_gradient_loss | -0.000224     |\n",
            "|    value_loss           | 1.6e+04       |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 135         |\n",
            "|    ep_rew_mean          | 1.03e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 572         |\n",
            "|    iterations           | 22          |\n",
            "|    time_elapsed         | 78          |\n",
            "|    total_timesteps      | 45056       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 6.42488e-05 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | 0.000101    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.03e+03    |\n",
            "|    n_updates            | 210         |\n",
            "|    policy_gradient_loss | -0.000364   |\n",
            "|    value_loss           | 1.72e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 135           |\n",
            "|    ep_rew_mean          | 1.01e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 23            |\n",
            "|    time_elapsed         | 82            |\n",
            "|    total_timesteps      | 47104         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.7040765e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 8.06e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.59e+03      |\n",
            "|    n_updates            | 220           |\n",
            "|    policy_gradient_loss | -0.00021      |\n",
            "|    value_loss           | 1.6e+04       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 133           |\n",
            "|    ep_rew_mean          | 988           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 24            |\n",
            "|    time_elapsed         | 85            |\n",
            "|    total_timesteps      | 49152         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.4638645e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 8.49e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.1e+03       |\n",
            "|    n_updates            | 230           |\n",
            "|    policy_gradient_loss | -0.000109     |\n",
            "|    value_loss           | 1.36e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 138          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 25           |\n",
            "|    time_elapsed         | 89           |\n",
            "|    total_timesteps      | 51200        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 1.540652e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 7.9e-05      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.01e+03     |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.000123    |\n",
            "|    value_loss           | 1.48e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 141           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 26            |\n",
            "|    time_elapsed         | 93            |\n",
            "|    total_timesteps      | 53248         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.8145452e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 4.94e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.26e+03      |\n",
            "|    n_updates            | 250           |\n",
            "|    policy_gradient_loss | -0.000134     |\n",
            "|    value_loss           | 1.36e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 142           |\n",
            "|    ep_rew_mean          | 1.08e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 27            |\n",
            "|    time_elapsed         | 96            |\n",
            "|    total_timesteps      | 55296         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.8019556e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 4.88e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.4e+03       |\n",
            "|    n_updates            | 260           |\n",
            "|    policy_gradient_loss | -0.000221     |\n",
            "|    value_loss           | 1.25e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 141           |\n",
            "|    ep_rew_mean          | 1.07e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 28            |\n",
            "|    time_elapsed         | 100           |\n",
            "|    total_timesteps      | 57344         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5230675e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 7.96e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.61e+03      |\n",
            "|    n_updates            | 270           |\n",
            "|    policy_gradient_loss | -0.000108     |\n",
            "|    value_loss           | 1.48e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 137           |\n",
            "|    ep_rew_mean          | 1.03e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 29            |\n",
            "|    time_elapsed         | 103           |\n",
            "|    total_timesteps      | 59392         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.3969136e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 5.47e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.18e+03      |\n",
            "|    n_updates            | 280           |\n",
            "|    policy_gradient_loss | -0.000243     |\n",
            "|    value_loss           | 1.36e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 138           |\n",
            "|    ep_rew_mean          | 1.04e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 30            |\n",
            "|    time_elapsed         | 107           |\n",
            "|    total_timesteps      | 61440         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.5437272e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 6.15e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.12e+03      |\n",
            "|    n_updates            | 290           |\n",
            "|    policy_gradient_loss | -0.000147     |\n",
            "|    value_loss           | 1.15e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 139           |\n",
            "|    ep_rew_mean          | 1.04e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 31            |\n",
            "|    time_elapsed         | 111           |\n",
            "|    total_timesteps      | 63488         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010038057 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 3e-05         |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.92e+03      |\n",
            "|    n_updates            | 300           |\n",
            "|    policy_gradient_loss | -0.000478     |\n",
            "|    value_loss           | 1.29e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 1.02e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 114          |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.966802e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 3.43e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.37e+03     |\n",
            "|    n_updates            | 310          |\n",
            "|    policy_gradient_loss | -0.000343    |\n",
            "|    value_loss           | 1.38e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 137          |\n",
            "|    ep_rew_mean          | 1.04e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 118          |\n",
            "|    total_timesteps      | 67584        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.824372e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 4.41e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.32e+03     |\n",
            "|    n_updates            | 320          |\n",
            "|    policy_gradient_loss | -0.000293    |\n",
            "|    value_loss           | 1.62e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 137          |\n",
            "|    ep_rew_mean          | 1.04e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 121          |\n",
            "|    total_timesteps      | 69632        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.790841e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 3.74e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7e+03        |\n",
            "|    n_updates            | 330          |\n",
            "|    policy_gradient_loss | -0.000207    |\n",
            "|    value_loss           | 1.38e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 139           |\n",
            "|    ep_rew_mean          | 1.07e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 35            |\n",
            "|    time_elapsed         | 125           |\n",
            "|    total_timesteps      | 71680         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.0959556e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 2.73e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.77e+03      |\n",
            "|    n_updates            | 340           |\n",
            "|    policy_gradient_loss | -0.000236     |\n",
            "|    value_loss           | 1.57e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 143          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 129          |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.246532e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 3.15e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.37e+03     |\n",
            "|    n_updates            | 350          |\n",
            "|    policy_gradient_loss | -0.000316    |\n",
            "|    value_loss           | 1.56e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 143           |\n",
            "|    ep_rew_mean          | 1.1e+03       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 37            |\n",
            "|    time_elapsed         | 132           |\n",
            "|    total_timesteps      | 75776         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.1424297e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 3.9e-05       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.37e+03      |\n",
            "|    n_updates            | 360           |\n",
            "|    policy_gradient_loss | -0.000241     |\n",
            "|    value_loss           | 1.22e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 143          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 136          |\n",
            "|    total_timesteps      | 77824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.712067e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 8.64e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.29e+03     |\n",
            "|    n_updates            | 370          |\n",
            "|    policy_gradient_loss | -0.000342    |\n",
            "|    value_loss           | 1.02e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 144           |\n",
            "|    ep_rew_mean          | 1.1e+03       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 39            |\n",
            "|    time_elapsed         | 139           |\n",
            "|    total_timesteps      | 79872         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.5120302e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 1.79e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.39e+03      |\n",
            "|    n_updates            | 380           |\n",
            "|    policy_gradient_loss | -0.000105     |\n",
            "|    value_loss           | 1.22e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 143           |\n",
            "|    ep_rew_mean          | 1.09e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 40            |\n",
            "|    time_elapsed         | 143           |\n",
            "|    total_timesteps      | 81920         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.8877199e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 3.22e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.61e+03      |\n",
            "|    n_updates            | 390           |\n",
            "|    policy_gradient_loss | -0.000187     |\n",
            "|    value_loss           | 1.56e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 143          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 146          |\n",
            "|    total_timesteps      | 83968        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.071067e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 2.25e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.37e+03     |\n",
            "|    n_updates            | 400          |\n",
            "|    policy_gradient_loss | -0.00023     |\n",
            "|    value_loss           | 1.28e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 143           |\n",
            "|    ep_rew_mean          | 1.09e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 42            |\n",
            "|    time_elapsed         | 150           |\n",
            "|    total_timesteps      | 86016         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00010959321 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 2.55e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.87e+03      |\n",
            "|    n_updates            | 410           |\n",
            "|    policy_gradient_loss | -0.000549     |\n",
            "|    value_loss           | 1.68e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 140           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 43            |\n",
            "|    time_elapsed         | 154           |\n",
            "|    total_timesteps      | 88064         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013863557 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 1.44e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.86e+03      |\n",
            "|    n_updates            | 420           |\n",
            "|    policy_gradient_loss | -0.000567     |\n",
            "|    value_loss           | 1.24e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 139          |\n",
            "|    ep_rew_mean          | 1.06e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 44           |\n",
            "|    time_elapsed         | 157          |\n",
            "|    total_timesteps      | 90112        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.424142e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 1.14e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.63e+03     |\n",
            "|    n_updates            | 430          |\n",
            "|    policy_gradient_loss | -0.000241    |\n",
            "|    value_loss           | 1.23e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 1.01e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 45           |\n",
            "|    time_elapsed         | 161          |\n",
            "|    total_timesteps      | 92160        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.812106e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 5.54e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.03e+03     |\n",
            "|    n_updates            | 440          |\n",
            "|    policy_gradient_loss | -0.000259    |\n",
            "|    value_loss           | 1.01e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 136           |\n",
            "|    ep_rew_mean          | 1.02e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 46            |\n",
            "|    time_elapsed         | 164           |\n",
            "|    total_timesteps      | 94208         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.4306548e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 8.4e-06       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.71e+03      |\n",
            "|    n_updates            | 450           |\n",
            "|    policy_gradient_loss | -0.000161     |\n",
            "|    value_loss           | 9.76e+03      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 135           |\n",
            "|    ep_rew_mean          | 1.01e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 47            |\n",
            "|    time_elapsed         | 168           |\n",
            "|    total_timesteps      | 96256         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.4429075e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 1.29e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.65e+03      |\n",
            "|    n_updates            | 460           |\n",
            "|    policy_gradient_loss | -0.000197     |\n",
            "|    value_loss           | 1.36e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 1.01e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 48           |\n",
            "|    time_elapsed         | 172          |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.989429e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 1.55e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.28e+03     |\n",
            "|    n_updates            | 470          |\n",
            "|    policy_gradient_loss | -0.000177    |\n",
            "|    value_loss           | 1.26e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 136          |\n",
            "|    ep_rew_mean          | 1e+03        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 49           |\n",
            "|    time_elapsed         | 175          |\n",
            "|    total_timesteps      | 100352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.729948e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 1.12e-05     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6e+03        |\n",
            "|    n_updates            | 480          |\n",
            "|    policy_gradient_loss | -7.96e-05    |\n",
            "|    value_loss           | 1.31e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 136           |\n",
            "|    ep_rew_mean          | 1.01e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 50            |\n",
            "|    time_elapsed         | 179           |\n",
            "|    total_timesteps      | 102400        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016349673 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 1.04e-05      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.34e+03      |\n",
            "|    n_updates            | 490           |\n",
            "|    policy_gradient_loss | -0.000645     |\n",
            "|    value_loss           | 1.18e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 137           |\n",
            "|    ep_rew_mean          | 1.03e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 51            |\n",
            "|    time_elapsed         | 182           |\n",
            "|    total_timesteps      | 104448        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.9565574e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 9.3e-06       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.12e+03      |\n",
            "|    n_updates            | 500           |\n",
            "|    policy_gradient_loss | -8.82e-05     |\n",
            "|    value_loss           | 1.17e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 139           |\n",
            "|    ep_rew_mean          | 1.04e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 52            |\n",
            "|    time_elapsed         | 186           |\n",
            "|    total_timesteps      | 106496        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.5834714e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 8.23e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.26e+03      |\n",
            "|    n_updates            | 510           |\n",
            "|    policy_gradient_loss | -0.000136     |\n",
            "|    value_loss           | 1.05e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 138           |\n",
            "|    ep_rew_mean          | 1.03e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 53            |\n",
            "|    time_elapsed         | 189           |\n",
            "|    total_timesteps      | 108544        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00020912514 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 5.48e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.43e+03      |\n",
            "|    n_updates            | 520           |\n",
            "|    policy_gradient_loss | -0.000756     |\n",
            "|    value_loss           | 1.04e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 137          |\n",
            "|    ep_rew_mean          | 1.01e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 54           |\n",
            "|    time_elapsed         | 193          |\n",
            "|    total_timesteps      | 110592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.784658e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 7.75e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.69e+03     |\n",
            "|    n_updates            | 530          |\n",
            "|    policy_gradient_loss | -0.000373    |\n",
            "|    value_loss           | 1.35e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 136           |\n",
            "|    ep_rew_mean          | 997           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 55            |\n",
            "|    time_elapsed         | 197           |\n",
            "|    total_timesteps      | 112640        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015890066 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 8.88e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6e+03         |\n",
            "|    n_updates            | 540           |\n",
            "|    policy_gradient_loss | -0.000588     |\n",
            "|    value_loss           | 1.2e+04       |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 981          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 56           |\n",
            "|    time_elapsed         | 200          |\n",
            "|    total_timesteps      | 114688       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001807391 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 4.05e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.36e+03     |\n",
            "|    n_updates            | 550          |\n",
            "|    policy_gradient_loss | -0.000562    |\n",
            "|    value_loss           | 1.07e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 138           |\n",
            "|    ep_rew_mean          | 1.02e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 57            |\n",
            "|    time_elapsed         | 204           |\n",
            "|    total_timesteps      | 116736        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015381968 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 4.23e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.45e+03      |\n",
            "|    n_updates            | 560           |\n",
            "|    policy_gradient_loss | -0.000532     |\n",
            "|    value_loss           | 1.08e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 139          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 58           |\n",
            "|    time_elapsed         | 207          |\n",
            "|    total_timesteps      | 118784       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.450124e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 7.45e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.5e+03      |\n",
            "|    n_updates            | 570          |\n",
            "|    policy_gradient_loss | -0.000258    |\n",
            "|    value_loss           | 1.35e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 139           |\n",
            "|    ep_rew_mean          | 1.04e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 59            |\n",
            "|    time_elapsed         | 211           |\n",
            "|    total_timesteps      | 120832        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.3437194e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 6.26e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.82e+03      |\n",
            "|    n_updates            | 580           |\n",
            "|    policy_gradient_loss | -0.000237     |\n",
            "|    value_loss           | 1.15e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 134           |\n",
            "|    ep_rew_mean          | 983           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 60            |\n",
            "|    time_elapsed         | 215           |\n",
            "|    total_timesteps      | 122880        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016943036 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 4.89e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.24e+03      |\n",
            "|    n_updates            | 590           |\n",
            "|    policy_gradient_loss | -0.000612     |\n",
            "|    value_loss           | 1.16e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 132           |\n",
            "|    ep_rew_mean          | 967           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 61            |\n",
            "|    time_elapsed         | 218           |\n",
            "|    total_timesteps      | 124928        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015558756 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | -5.96e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 4.52e+03      |\n",
            "|    n_updates            | 600           |\n",
            "|    policy_gradient_loss | -0.000465     |\n",
            "|    value_loss           | 9.72e+03      |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| rollout/                |                |\n",
            "|    ep_len_mean          | 136            |\n",
            "|    ep_rew_mean          | 1.01e+03       |\n",
            "| time/                   |                |\n",
            "|    fps                  | 571            |\n",
            "|    iterations           | 62             |\n",
            "|    time_elapsed         | 222            |\n",
            "|    total_timesteps      | 126976         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000110119436 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.15           |\n",
            "|    entropy_loss         | -1.39          |\n",
            "|    explained_variance   | 5.72e-06       |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 5.71e+03       |\n",
            "|    n_updates            | 610            |\n",
            "|    policy_gradient_loss | -0.000352      |\n",
            "|    value_loss           | 1.06e+04       |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 139           |\n",
            "|    ep_rew_mean          | 1.05e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 63            |\n",
            "|    time_elapsed         | 225           |\n",
            "|    total_timesteps      | 129024        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012997576 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 5.54e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.35e+03      |\n",
            "|    n_updates            | 620           |\n",
            "|    policy_gradient_loss | -0.000465     |\n",
            "|    value_loss           | 1.48e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 142          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 64           |\n",
            "|    time_elapsed         | 229          |\n",
            "|    total_timesteps      | 131072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0001223839 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 5.36e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.97e+03     |\n",
            "|    n_updates            | 630          |\n",
            "|    policy_gradient_loss | -0.000468    |\n",
            "|    value_loss           | 1.35e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 145           |\n",
            "|    ep_rew_mean          | 1.13e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 65            |\n",
            "|    time_elapsed         | 232           |\n",
            "|    total_timesteps      | 133120        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 8.1433565e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 7.45e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.29e+03      |\n",
            "|    n_updates            | 640           |\n",
            "|    policy_gradient_loss | -0.000346     |\n",
            "|    value_loss           | 1.7e+04       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 149           |\n",
            "|    ep_rew_mean          | 1.17e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 66            |\n",
            "|    time_elapsed         | 236           |\n",
            "|    total_timesteps      | 135168        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.3070956e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 7.15e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.64e+03      |\n",
            "|    n_updates            | 650           |\n",
            "|    policy_gradient_loss | -0.000138     |\n",
            "|    value_loss           | 1.43e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 148           |\n",
            "|    ep_rew_mean          | 1.16e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 67            |\n",
            "|    time_elapsed         | 240           |\n",
            "|    total_timesteps      | 137216        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00013266032 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 5.13e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.95e+03      |\n",
            "|    n_updates            | 660           |\n",
            "|    policy_gradient_loss | -0.000421     |\n",
            "|    value_loss           | 1.27e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 145           |\n",
            "|    ep_rew_mean          | 1.14e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 68            |\n",
            "|    time_elapsed         | 243           |\n",
            "|    total_timesteps      | 139264        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00014120634 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 2.92e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.9e+03       |\n",
            "|    n_updates            | 670           |\n",
            "|    policy_gradient_loss | -0.00051      |\n",
            "|    value_loss           | 1.19e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 147          |\n",
            "|    ep_rew_mean          | 1.16e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 69           |\n",
            "|    time_elapsed         | 247          |\n",
            "|    total_timesteps      | 141312       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.199999e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.65e+03     |\n",
            "|    n_updates            | 680          |\n",
            "|    policy_gradient_loss | -0.000244    |\n",
            "|    value_loss           | 9.09e+03     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 147           |\n",
            "|    ep_rew_mean          | 1.16e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 70            |\n",
            "|    time_elapsed         | 250           |\n",
            "|    total_timesteps      | 143360        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.3379103e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 6.02e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.6e+03       |\n",
            "|    n_updates            | 690           |\n",
            "|    policy_gradient_loss | -0.00023      |\n",
            "|    value_loss           | 1.63e+04      |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| rollout/                |                |\n",
            "|    ep_len_mean          | 139            |\n",
            "|    ep_rew_mean          | 1.06e+03       |\n",
            "| time/                   |                |\n",
            "|    fps                  | 571            |\n",
            "|    iterations           | 71             |\n",
            "|    time_elapsed         | 254            |\n",
            "|    total_timesteps      | 145408         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000117861404 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.15           |\n",
            "|    entropy_loss         | -1.39          |\n",
            "|    explained_variance   | 4.59e-06       |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 7.36e+03       |\n",
            "|    n_updates            | 700            |\n",
            "|    policy_gradient_loss | -0.000423      |\n",
            "|    value_loss           | 1.39e+04       |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 138          |\n",
            "|    ep_rew_mean          | 1.06e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 72           |\n",
            "|    time_elapsed         | 258          |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.076098e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 2.8e-06      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.16e+03     |\n",
            "|    n_updates            | 710          |\n",
            "|    policy_gradient_loss | -0.000239    |\n",
            "|    value_loss           | 1.17e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 137           |\n",
            "|    ep_rew_mean          | 1.05e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 73            |\n",
            "|    time_elapsed         | 261           |\n",
            "|    total_timesteps      | 149504        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.1550276e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 3.7e-06       |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.7e+03       |\n",
            "|    n_updates            | 720           |\n",
            "|    policy_gradient_loss | -0.000262     |\n",
            "|    value_loss           | 1.38e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 138           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 74            |\n",
            "|    time_elapsed         | 265           |\n",
            "|    total_timesteps      | 151552        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00023746118 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 1.37e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.43e+03      |\n",
            "|    n_updates            | 730           |\n",
            "|    policy_gradient_loss | -0.000756     |\n",
            "|    value_loss           | 1.08e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 138           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 75            |\n",
            "|    time_elapsed         | 268           |\n",
            "|    total_timesteps      | 153600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00020389276 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 1.97e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.58e+03      |\n",
            "|    n_updates            | 740           |\n",
            "|    policy_gradient_loss | -0.000559     |\n",
            "|    value_loss           | 1.4e+04       |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 137         |\n",
            "|    ep_rew_mean          | 1.04e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 571         |\n",
            "|    iterations           | 76          |\n",
            "|    time_elapsed         | 272         |\n",
            "|    total_timesteps      | 155648      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000336607 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | 8.94e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.6e+03     |\n",
            "|    n_updates            | 750         |\n",
            "|    policy_gradient_loss | -0.000954   |\n",
            "|    value_loss           | 1.11e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 138          |\n",
            "|    ep_rew_mean          | 1.05e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 77           |\n",
            "|    time_elapsed         | 276          |\n",
            "|    total_timesteps      | 157696       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.210595e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 1.13e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 4.74e+03     |\n",
            "|    n_updates            | 760          |\n",
            "|    policy_gradient_loss | -0.000226    |\n",
            "|    value_loss           | 1.07e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 140          |\n",
            "|    ep_rew_mean          | 1.06e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 78           |\n",
            "|    time_elapsed         | 279          |\n",
            "|    total_timesteps      | 159744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003113499 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 2.5e-06      |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.4e+03      |\n",
            "|    n_updates            | 770          |\n",
            "|    policy_gradient_loss | -0.000922    |\n",
            "|    value_loss           | 1.13e+04     |\n",
            "------------------------------------------\n",
            "--------------------------------------------\n",
            "| rollout/                |                |\n",
            "|    ep_len_mean          | 139            |\n",
            "|    ep_rew_mean          | 1.05e+03       |\n",
            "| time/                   |                |\n",
            "|    fps                  | 571            |\n",
            "|    iterations           | 79             |\n",
            "|    time_elapsed         | 283            |\n",
            "|    total_timesteps      | 161792         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000111210364 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.15           |\n",
            "|    entropy_loss         | -1.38          |\n",
            "|    explained_variance   | 2.92e-06       |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 7.53e+03       |\n",
            "|    n_updates            | 780            |\n",
            "|    policy_gradient_loss | -0.000355      |\n",
            "|    value_loss           | 1.48e+04       |\n",
            "--------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 139          |\n",
            "|    ep_rew_mean          | 1.05e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 80           |\n",
            "|    time_elapsed         | 286          |\n",
            "|    total_timesteps      | 163840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.154656e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 3.58e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.07e+03     |\n",
            "|    n_updates            | 790          |\n",
            "|    policy_gradient_loss | -0.000325    |\n",
            "|    value_loss           | 1.01e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 141          |\n",
            "|    ep_rew_mean          | 1.06e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 81           |\n",
            "|    time_elapsed         | 290          |\n",
            "|    total_timesteps      | 165888       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.930231e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.39        |\n",
            "|    explained_variance   | 2.44e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.69e+03     |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | -0.000285    |\n",
            "|    value_loss           | 1.28e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 141           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 82            |\n",
            "|    time_elapsed         | 294           |\n",
            "|    total_timesteps      | 167936        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00039177155 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 7.75e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.18e+03      |\n",
            "|    n_updates            | 810           |\n",
            "|    policy_gradient_loss | -0.00117      |\n",
            "|    value_loss           | 1.23e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 141           |\n",
            "|    ep_rew_mean          | 1.07e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 83            |\n",
            "|    time_elapsed         | 297           |\n",
            "|    total_timesteps      | 169984        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00029415215 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 8.34e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.22e+03      |\n",
            "|    n_updates            | 820           |\n",
            "|    policy_gradient_loss | -0.000791     |\n",
            "|    value_loss           | 1.22e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 138          |\n",
            "|    ep_rew_mean          | 1.04e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 84           |\n",
            "|    time_elapsed         | 301          |\n",
            "|    total_timesteps      | 172032       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.053107e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 1.67e-06     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.49e+03     |\n",
            "|    n_updates            | 830          |\n",
            "|    policy_gradient_loss | -0.000152    |\n",
            "|    value_loss           | 1.15e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 136           |\n",
            "|    ep_rew_mean          | 1.01e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 85            |\n",
            "|    time_elapsed         | 304           |\n",
            "|    total_timesteps      | 174080        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.8963393e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.24e+03      |\n",
            "|    n_updates            | 840           |\n",
            "|    policy_gradient_loss | -0.000181     |\n",
            "|    value_loss           | 1.06e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 139           |\n",
            "|    ep_rew_mean          | 1.05e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 86            |\n",
            "|    time_elapsed         | 308           |\n",
            "|    total_timesteps      | 176128        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.8475468e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 9.54e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.98e+03      |\n",
            "|    n_updates            | 850           |\n",
            "|    policy_gradient_loss | -0.000231     |\n",
            "|    value_loss           | 1.2e+04       |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 137         |\n",
            "|    ep_rew_mean          | 1.02e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 571         |\n",
            "|    iterations           | 87          |\n",
            "|    time_elapsed         | 311         |\n",
            "|    total_timesteps      | 178176      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 9.89089e-05 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -1.39       |\n",
            "|    explained_variance   | 4.17e-07    |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 5.57e+03    |\n",
            "|    n_updates            | 860         |\n",
            "|    policy_gradient_loss | -0.000332   |\n",
            "|    value_loss           | 1.14e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 137           |\n",
            "|    ep_rew_mean          | 1.03e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 88            |\n",
            "|    time_elapsed         | 315           |\n",
            "|    total_timesteps      | 180224        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00029705293 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 8.34e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.87e+03      |\n",
            "|    n_updates            | 870           |\n",
            "|    policy_gradient_loss | -0.000735     |\n",
            "|    value_loss           | 1.23e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 140           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 89            |\n",
            "|    time_elapsed         | 319           |\n",
            "|    total_timesteps      | 182272        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016473897 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 1.01e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.32e+03      |\n",
            "|    n_updates            | 880           |\n",
            "|    policy_gradient_loss | -0.000455     |\n",
            "|    value_loss           | 1.22e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 140           |\n",
            "|    ep_rew_mean          | 1.05e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 90            |\n",
            "|    time_elapsed         | 322           |\n",
            "|    total_timesteps      | 184320        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015737841 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 1.25e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.76e+03      |\n",
            "|    n_updates            | 890           |\n",
            "|    policy_gradient_loss | -0.00039      |\n",
            "|    value_loss           | 1.27e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 142           |\n",
            "|    ep_rew_mean          | 1.07e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 91            |\n",
            "|    time_elapsed         | 326           |\n",
            "|    total_timesteps      | 186368        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00018781915 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.39         |\n",
            "|    explained_variance   | 5.36e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.02e+03      |\n",
            "|    n_updates            | 900           |\n",
            "|    policy_gradient_loss | -0.000506     |\n",
            "|    value_loss           | 1.23e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 143          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 92           |\n",
            "|    time_elapsed         | 329          |\n",
            "|    total_timesteps      | 188416       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005898593 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 5.43e+03     |\n",
            "|    n_updates            | 910          |\n",
            "|    policy_gradient_loss | -0.00125     |\n",
            "|    value_loss           | 1.08e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 141           |\n",
            "|    ep_rew_mean          | 1.07e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 93            |\n",
            "|    time_elapsed         | 333           |\n",
            "|    total_timesteps      | 190464        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015712166 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 5.36e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.85e+03      |\n",
            "|    n_updates            | 920           |\n",
            "|    policy_gradient_loss | -0.000328     |\n",
            "|    value_loss           | 1.22e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 140           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 94            |\n",
            "|    time_elapsed         | 336           |\n",
            "|    total_timesteps      | 192512        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012614051 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 6.56e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.36e+03      |\n",
            "|    n_updates            | 930           |\n",
            "|    policy_gradient_loss | -0.000358     |\n",
            "|    value_loss           | 1.18e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 138          |\n",
            "|    ep_rew_mean          | 1.04e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 95           |\n",
            "|    time_elapsed         | 340          |\n",
            "|    total_timesteps      | 194560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.961945e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.27e+03     |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | -0.000221    |\n",
            "|    value_loss           | 1.25e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 139           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 96            |\n",
            "|    time_elapsed         | 344           |\n",
            "|    total_timesteps      | 196608        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021155269 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 2.98e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.57e+03      |\n",
            "|    n_updates            | 950           |\n",
            "|    policy_gradient_loss | -0.000555     |\n",
            "|    value_loss           | 1.4e+04       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 136           |\n",
            "|    ep_rew_mean          | 1.02e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 97            |\n",
            "|    time_elapsed         | 347           |\n",
            "|    total_timesteps      | 198656        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00022060581 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 7.75e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7e+03         |\n",
            "|    n_updates            | 960           |\n",
            "|    policy_gradient_loss | -0.000498     |\n",
            "|    value_loss           | 1.31e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 1e+03        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 98           |\n",
            "|    time_elapsed         | 351          |\n",
            "|    total_timesteps      | 200704       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008901984 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -3.58e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.16e+03     |\n",
            "|    n_updates            | 970          |\n",
            "|    policy_gradient_loss | -0.00109     |\n",
            "|    value_loss           | 1.21e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 137          |\n",
            "|    ep_rew_mean          | 1.02e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 99           |\n",
            "|    time_elapsed         | 354          |\n",
            "|    total_timesteps      | 202752       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005604662 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.77e+03     |\n",
            "|    n_updates            | 980          |\n",
            "|    policy_gradient_loss | -0.00105     |\n",
            "|    value_loss           | 1.33e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 137          |\n",
            "|    ep_rew_mean          | 1.02e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 100          |\n",
            "|    time_elapsed         | 358          |\n",
            "|    total_timesteps      | 204800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.932614e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 5.36e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.33e+03     |\n",
            "|    n_updates            | 990          |\n",
            "|    policy_gradient_loss | -0.000215    |\n",
            "|    value_loss           | 1.36e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 143           |\n",
            "|    ep_rew_mean          | 1.07e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 101           |\n",
            "|    time_elapsed         | 361           |\n",
            "|    total_timesteps      | 206848        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00012315315 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.72e+03      |\n",
            "|    n_updates            | 1000          |\n",
            "|    policy_gradient_loss | -0.00032      |\n",
            "|    value_loss           | 1.26e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 145           |\n",
            "|    ep_rew_mean          | 1.09e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 102           |\n",
            "|    time_elapsed         | 365           |\n",
            "|    total_timesteps      | 208896        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021055908 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 1.19e-06      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.87e+03      |\n",
            "|    n_updates            | 1010          |\n",
            "|    policy_gradient_loss | -0.000633     |\n",
            "|    value_loss           | 1.45e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 144          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 103          |\n",
            "|    time_elapsed         | 369          |\n",
            "|    total_timesteps      | 210944       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003731753 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.64e+03     |\n",
            "|    n_updates            | 1020         |\n",
            "|    policy_gradient_loss | -0.000624    |\n",
            "|    value_loss           | 1.22e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 147          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 104          |\n",
            "|    time_elapsed         | 372          |\n",
            "|    total_timesteps      | 212992       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005931078 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.7e+03      |\n",
            "|    n_updates            | 1030         |\n",
            "|    policy_gradient_loss | -0.0012      |\n",
            "|    value_loss           | 1.32e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 146           |\n",
            "|    ep_rew_mean          | 1.11e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 105           |\n",
            "|    time_elapsed         | 376           |\n",
            "|    total_timesteps      | 215040        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.7450303e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 3.58e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.73e+03      |\n",
            "|    n_updates            | 1040          |\n",
            "|    policy_gradient_loss | -0.00018      |\n",
            "|    value_loss           | 1.39e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 144          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 106          |\n",
            "|    time_elapsed         | 379          |\n",
            "|    total_timesteps      | 217088       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0041915434 |\n",
            "|    clip_fraction        | 0.041        |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -1.31e-06    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.15e+03     |\n",
            "|    n_updates            | 1050         |\n",
            "|    policy_gradient_loss | -0.00222     |\n",
            "|    value_loss           | 1.17e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 144           |\n",
            "|    ep_rew_mean          | 1.09e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 107           |\n",
            "|    time_elapsed         | 383           |\n",
            "|    total_timesteps      | 219136        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00048199925 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 1.79e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 5.91e+03      |\n",
            "|    n_updates            | 1060          |\n",
            "|    policy_gradient_loss | -3.16e-06     |\n",
            "|    value_loss           | 1.39e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 140          |\n",
            "|    ep_rew_mean          | 1.06e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 108          |\n",
            "|    time_elapsed         | 387          |\n",
            "|    total_timesteps      | 221184       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.324033e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.37e+03     |\n",
            "|    n_updates            | 1070         |\n",
            "|    policy_gradient_loss | -0.000237    |\n",
            "|    value_loss           | 1.46e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 137           |\n",
            "|    ep_rew_mean          | 1.03e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 109           |\n",
            "|    time_elapsed         | 390           |\n",
            "|    total_timesteps      | 223232        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.6963913e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 2.98e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.93e+03      |\n",
            "|    n_updates            | 1080          |\n",
            "|    policy_gradient_loss | -0.000178     |\n",
            "|    value_loss           | 1.33e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 1e+03        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 110          |\n",
            "|    time_elapsed         | 394          |\n",
            "|    total_timesteps      | 225280       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.467403e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.23e+03     |\n",
            "|    n_updates            | 1090         |\n",
            "|    policy_gradient_loss | -0.00044     |\n",
            "|    value_loss           | 1.32e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 137           |\n",
            "|    ep_rew_mean          | 1.03e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 111           |\n",
            "|    time_elapsed         | 397           |\n",
            "|    total_timesteps      | 227328        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.3774948e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.15e+03      |\n",
            "|    n_updates            | 1100          |\n",
            "|    policy_gradient_loss | -9.06e-05     |\n",
            "|    value_loss           | 1.23e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 137           |\n",
            "|    ep_rew_mean          | 1.04e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 112           |\n",
            "|    time_elapsed         | 401           |\n",
            "|    total_timesteps      | 229376        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.8705784e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.71e+03      |\n",
            "|    n_updates            | 1110          |\n",
            "|    policy_gradient_loss | -0.000231     |\n",
            "|    value_loss           | 1.42e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 138           |\n",
            "|    ep_rew_mean          | 1.05e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 113           |\n",
            "|    time_elapsed         | 404           |\n",
            "|    total_timesteps      | 231424        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.9934261e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.54e+03      |\n",
            "|    n_updates            | 1120          |\n",
            "|    policy_gradient_loss | -0.000192     |\n",
            "|    value_loss           | 1.31e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 1.01e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 114          |\n",
            "|    time_elapsed         | 408          |\n",
            "|    total_timesteps      | 233472       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.196578e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.37e+03     |\n",
            "|    n_updates            | 1130         |\n",
            "|    policy_gradient_loss | -0.000155    |\n",
            "|    value_loss           | 1.45e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 988          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 115          |\n",
            "|    time_elapsed         | 411          |\n",
            "|    total_timesteps      | 235520       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0035296506 |\n",
            "|    clip_fraction        | 0.024        |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -7.15e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.75e+03     |\n",
            "|    n_updates            | 1140         |\n",
            "|    policy_gradient_loss | -0.000434    |\n",
            "|    value_loss           | 1.26e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 131           |\n",
            "|    ep_rew_mean          | 982           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 116           |\n",
            "|    time_elapsed         | 415           |\n",
            "|    total_timesteps      | 237568        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038467225 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | -3.58e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.22e+03      |\n",
            "|    n_updates            | 1150          |\n",
            "|    policy_gradient_loss | 0.000213      |\n",
            "|    value_loss           | 1.3e+04       |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 981          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 117          |\n",
            "|    time_elapsed         | 419          |\n",
            "|    total_timesteps      | 239616       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014763912 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -4.77e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.21e+03     |\n",
            "|    n_updates            | 1160         |\n",
            "|    policy_gradient_loss | -0.00105     |\n",
            "|    value_loss           | 1.31e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 129           |\n",
            "|    ep_rew_mean          | 961           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 118           |\n",
            "|    time_elapsed         | 422           |\n",
            "|    total_timesteps      | 241664        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.1834074e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.06e+03      |\n",
            "|    n_updates            | 1170          |\n",
            "|    policy_gradient_loss | -0.000242     |\n",
            "|    value_loss           | 1.42e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 962          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 119          |\n",
            "|    time_elapsed         | 426          |\n",
            "|    total_timesteps      | 243712       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.018073e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.95e+03     |\n",
            "|    n_updates            | 1180         |\n",
            "|    policy_gradient_loss | -0.000194    |\n",
            "|    value_loss           | 1.32e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 131           |\n",
            "|    ep_rew_mean          | 983           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 120           |\n",
            "|    time_elapsed         | 429           |\n",
            "|    total_timesteps      | 245760        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7926825e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.63e+03      |\n",
            "|    n_updates            | 1190          |\n",
            "|    policy_gradient_loss | -0.000159     |\n",
            "|    value_loss           | 1.39e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 996          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 571          |\n",
            "|    iterations           | 121          |\n",
            "|    time_elapsed         | 433          |\n",
            "|    total_timesteps      | 247808       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013811891 |\n",
            "|    clip_fraction        | 4.88e-05     |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.48e+03     |\n",
            "|    n_updates            | 1200         |\n",
            "|    policy_gradient_loss | -0.000803    |\n",
            "|    value_loss           | 1.38e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 137           |\n",
            "|    ep_rew_mean          | 1.03e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 122           |\n",
            "|    time_elapsed         | 436           |\n",
            "|    total_timesteps      | 249856        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.5811513e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.2e+03       |\n",
            "|    n_updates            | 1210          |\n",
            "|    policy_gradient_loss | 2.03e-05      |\n",
            "|    value_loss           | 1.47e+04      |\n",
            "-------------------------------------------\n",
            "--------------------------------------------\n",
            "| rollout/                |                |\n",
            "|    ep_len_mean          | 138            |\n",
            "|    ep_rew_mean          | 1.04e+03       |\n",
            "| time/                   |                |\n",
            "|    fps                  | 572            |\n",
            "|    iterations           | 123            |\n",
            "|    time_elapsed         | 440            |\n",
            "|    total_timesteps      | 251904         |\n",
            "| train/                  |                |\n",
            "|    approx_kl            | 0.000109488145 |\n",
            "|    clip_fraction        | 0              |\n",
            "|    clip_range           | 0.15           |\n",
            "|    entropy_loss         | -1.38          |\n",
            "|    explained_variance   | -1.19e-07      |\n",
            "|    learning_rate        | 0.0003         |\n",
            "|    loss                 | 6.8e+03        |\n",
            "|    n_updates            | 1220           |\n",
            "|    policy_gradient_loss | -0.000402      |\n",
            "|    value_loss           | 1.28e+04       |\n",
            "--------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 141           |\n",
            "|    ep_rew_mean          | 1.07e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 571           |\n",
            "|    iterations           | 124           |\n",
            "|    time_elapsed         | 443           |\n",
            "|    total_timesteps      | 253952        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.6619964e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.01e+03      |\n",
            "|    n_updates            | 1230          |\n",
            "|    policy_gradient_loss | -0.000189     |\n",
            "|    value_loss           | 1.45e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 139           |\n",
            "|    ep_rew_mean          | 1.05e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 125           |\n",
            "|    time_elapsed         | 447           |\n",
            "|    total_timesteps      | 256000        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1378055e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.28e+03      |\n",
            "|    n_updates            | 1240          |\n",
            "|    policy_gradient_loss | -0.000104     |\n",
            "|    value_loss           | 1.37e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 139          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 126          |\n",
            "|    time_elapsed         | 451          |\n",
            "|    total_timesteps      | 258048       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0026607034 |\n",
            "|    clip_fraction        | 0.0219       |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -2.38e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 8.02e+03     |\n",
            "|    n_updates            | 1250         |\n",
            "|    policy_gradient_loss | -0.00282     |\n",
            "|    value_loss           | 1.42e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 138          |\n",
            "|    ep_rew_mean          | 1.01e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 127          |\n",
            "|    time_elapsed         | 454          |\n",
            "|    total_timesteps      | 260096       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012350171 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.78e+03     |\n",
            "|    n_updates            | 1260         |\n",
            "|    policy_gradient_loss | 0.000242     |\n",
            "|    value_loss           | 1.42e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 137          |\n",
            "|    ep_rew_mean          | 1.01e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 128          |\n",
            "|    time_elapsed         | 458          |\n",
            "|    total_timesteps      | 262144       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0006536473 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.25e+03     |\n",
            "|    n_updates            | 1270         |\n",
            "|    policy_gradient_loss | -0.00111     |\n",
            "|    value_loss           | 1.36e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 137          |\n",
            "|    ep_rew_mean          | 1e+03        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 129          |\n",
            "|    time_elapsed         | 461          |\n",
            "|    total_timesteps      | 264192       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0002875535 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.98e+03     |\n",
            "|    n_updates            | 1280         |\n",
            "|    policy_gradient_loss | -0.000617    |\n",
            "|    value_loss           | 1.39e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 140          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 130          |\n",
            "|    time_elapsed         | 465          |\n",
            "|    total_timesteps      | 266240       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.922709e-06 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.5e+03      |\n",
            "|    n_updates            | 1290         |\n",
            "|    policy_gradient_loss | -9.15e-06    |\n",
            "|    value_loss           | 1.42e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 137           |\n",
            "|    ep_rew_mean          | 1.01e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 131           |\n",
            "|    time_elapsed         | 468           |\n",
            "|    total_timesteps      | 268288        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016341516 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.45e+03      |\n",
            "|    n_updates            | 1300          |\n",
            "|    policy_gradient_loss | -0.000421     |\n",
            "|    value_loss           | 1.49e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 139           |\n",
            "|    ep_rew_mean          | 1.03e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 132           |\n",
            "|    time_elapsed         | 472           |\n",
            "|    total_timesteps      | 270336        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00055983517 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 6.97e+03      |\n",
            "|    n_updates            | 1310          |\n",
            "|    policy_gradient_loss | -0.00134      |\n",
            "|    value_loss           | 1.42e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 139          |\n",
            "|    ep_rew_mean          | 1.04e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 133          |\n",
            "|    time_elapsed         | 475          |\n",
            "|    total_timesteps      | 272384       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012203441 |\n",
            "|    clip_fraction        | 0.00781      |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 6.64e+03     |\n",
            "|    n_updates            | 1320         |\n",
            "|    policy_gradient_loss | -0.00102     |\n",
            "|    value_loss           | 1.39e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 141           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 134           |\n",
            "|    time_elapsed         | 479           |\n",
            "|    total_timesteps      | 274432        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00038184988 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.34e+03      |\n",
            "|    n_updates            | 1330          |\n",
            "|    policy_gradient_loss | 0.000564      |\n",
            "|    value_loss           | 1.49e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 144          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 135          |\n",
            "|    time_elapsed         | 482          |\n",
            "|    total_timesteps      | 276480       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014762256 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.46e+03     |\n",
            "|    n_updates            | 1340         |\n",
            "|    policy_gradient_loss | -0.00113     |\n",
            "|    value_loss           | 1.51e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 142           |\n",
            "|    ep_rew_mean          | 1.08e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 136           |\n",
            "|    time_elapsed         | 486           |\n",
            "|    total_timesteps      | 278528        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015108651 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.82e+03      |\n",
            "|    n_updates            | 1350          |\n",
            "|    policy_gradient_loss | -0.000195     |\n",
            "|    value_loss           | 1.66e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 140           |\n",
            "|    ep_rew_mean          | 1.05e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 137           |\n",
            "|    time_elapsed         | 490           |\n",
            "|    total_timesteps      | 280576        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00029934806 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 7.46e+03      |\n",
            "|    n_updates            | 1360          |\n",
            "|    policy_gradient_loss | -0.000767     |\n",
            "|    value_loss           | 1.42e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 143           |\n",
            "|    ep_rew_mean          | 1.09e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 138           |\n",
            "|    time_elapsed         | 493           |\n",
            "|    total_timesteps      | 282624        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00016134785 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.02e+03      |\n",
            "|    n_updates            | 1370          |\n",
            "|    policy_gradient_loss | -0.000467     |\n",
            "|    value_loss           | 1.64e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 145          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 139          |\n",
            "|    time_elapsed         | 497          |\n",
            "|    total_timesteps      | 284672       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023832812 |\n",
            "|    clip_fraction        | 0.000244     |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.38        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 9.33e+03     |\n",
            "|    n_updates            | 1380         |\n",
            "|    policy_gradient_loss | -0.00232     |\n",
            "|    value_loss           | 1.58e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 145           |\n",
            "|    ep_rew_mean          | 1.12e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 140           |\n",
            "|    time_elapsed         | 500           |\n",
            "|    total_timesteps      | 286720        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015718231 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.38         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.97e+03      |\n",
            "|    n_updates            | 1390          |\n",
            "|    policy_gradient_loss | -0.000348     |\n",
            "|    value_loss           | 1.57e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 144          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 141          |\n",
            "|    time_elapsed         | 504          |\n",
            "|    total_timesteps      | 288768       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0032121784 |\n",
            "|    clip_fraction        | 0.0758       |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.37        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.78e+03     |\n",
            "|    n_updates            | 1400         |\n",
            "|    policy_gradient_loss | -0.004       |\n",
            "|    value_loss           | 1.5e+04      |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 143        |\n",
            "|    ep_rew_mean          | 1.1e+03    |\n",
            "| time/                   |            |\n",
            "|    fps                  | 572        |\n",
            "|    iterations           | 142        |\n",
            "|    time_elapsed         | 507        |\n",
            "|    total_timesteps      | 290816     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00333516 |\n",
            "|    clip_fraction        | 0.0452     |\n",
            "|    clip_range           | 0.15       |\n",
            "|    entropy_loss         | -1.37      |\n",
            "|    explained_variance   | -1.19e-07  |\n",
            "|    learning_rate        | 0.0003     |\n",
            "|    loss                 | 7.38e+03   |\n",
            "|    n_updates            | 1410       |\n",
            "|    policy_gradient_loss | -0.0035    |\n",
            "|    value_loss           | 1.56e+04   |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 142         |\n",
            "|    ep_rew_mean          | 1.07e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 572         |\n",
            "|    iterations           | 143         |\n",
            "|    time_elapsed         | 511         |\n",
            "|    total_timesteps      | 292864      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002996153 |\n",
            "|    clip_fraction        | 0.00332     |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -1.36       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 6.41e+03    |\n",
            "|    n_updates            | 1420        |\n",
            "|    policy_gradient_loss | -0.00248    |\n",
            "|    value_loss           | 1.46e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 140         |\n",
            "|    ep_rew_mean          | 1.05e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 572         |\n",
            "|    iterations           | 144         |\n",
            "|    time_elapsed         | 515         |\n",
            "|    total_timesteps      | 294912      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.008744746 |\n",
            "|    clip_fraction        | 0.0361      |\n",
            "|    clip_range           | 0.15        |\n",
            "|    entropy_loss         | -1.35       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 0.0003      |\n",
            "|    loss                 | 8.84e+03    |\n",
            "|    n_updates            | 1430        |\n",
            "|    policy_gradient_loss | -0.000814   |\n",
            "|    value_loss           | 1.62e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 142          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 145          |\n",
            "|    time_elapsed         | 518          |\n",
            "|    total_timesteps      | 296960       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0033835701 |\n",
            "|    clip_fraction        | 0.0699       |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.35        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.61e+03     |\n",
            "|    n_updates            | 1440         |\n",
            "|    policy_gradient_loss | -0.00155     |\n",
            "|    value_loss           | 1.64e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 144           |\n",
            "|    ep_rew_mean          | 1.09e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 572           |\n",
            "|    iterations           | 146           |\n",
            "|    time_elapsed         | 522           |\n",
            "|    total_timesteps      | 299008        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00044724985 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.15          |\n",
            "|    entropy_loss         | -1.35         |\n",
            "|    explained_variance   | 5.96e-08      |\n",
            "|    learning_rate        | 0.0003        |\n",
            "|    loss                 | 8.17e+03      |\n",
            "|    n_updates            | 1450          |\n",
            "|    policy_gradient_loss | -2.93e-05     |\n",
            "|    value_loss           | 1.57e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 143          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 572          |\n",
            "|    iterations           | 147          |\n",
            "|    time_elapsed         | 525          |\n",
            "|    total_timesteps      | 301056       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.050469e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.15         |\n",
            "|    entropy_loss         | -1.35        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 0.0003       |\n",
            "|    loss                 | 7.55e+03     |\n",
            "|    n_updates            | 1460         |\n",
            "|    policy_gradient_loss | -0.000359    |\n",
            "|    value_loss           | 1.64e+04     |\n",
            "------------------------------------------\n",
            "Saved model ppo_2048_v2_simple.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "model_v2 = PPO.load(\"ppo_2048_v2_simple\")\n",
        "\n",
        "env_eval = Game2048EnvV2(render_mode=\"ansi\")\n",
        "obs, info = env_eval.reset()\n",
        "done = False\n",
        "step = 0\n",
        "\n",
        "print(\"Initial board:\")\n",
        "print(env_eval._board_to_string())\n",
        "\n",
        "while not done:\n",
        "    # üîΩ IMPORTANT: try deterministic=False to see variety\n",
        "    action, _ = model_v2.predict(obs, deterministic=False)\n",
        "    obs, reward, terminated, truncated, info = env_eval.step(int(action))\n",
        "    done = terminated or truncated\n",
        "\n",
        "    print(f\"\\nStep {step}, action={int(action)}, reward={reward:.2f}, score={info['score']:.2f}\")\n",
        "    print(env_eval._board_to_string())\n",
        "    time.sleep(0.1)\n",
        "    step += 1\n",
        "\n",
        "print(\"\\nEpisode finished.\")\n",
        "print(f\"Final score: {info['score']}, max tile: {info['max_tile']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "h_jOji2Y9oBx",
        "outputId": "976e9fa3-5885-4853-a77e-3bb338e3c065"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial board:\n",
            ".\t.\t.\t4\n",
            ".\t.\t.\t.\n",
            ".\t.\t2\t.\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 0, action=1, reward=0.63, score=0.63\n",
            ".\t.\t.\t2\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t2\t4\n",
            "\n",
            "Step 1, action=2, reward=0.12, score=0.75\n",
            "2\t.\t2\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "2\t4\t.\t.\n",
            "\n",
            "Step 2, action=0, reward=4.12, score=4.87\n",
            "4\t4\t2\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t4\t.\n",
            "\n",
            "Step 3, action=0, reward=0.11, score=4.98\n",
            "4\t4\t2\t.\n",
            ".\t.\t4\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 4, action=1, reward=0.10, score=5.08\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t2\n",
            ".\t.\t2\t.\n",
            "4\t4\t4\t2\n",
            "\n",
            "Step 5, action=0, reward=4.10, score=9.18\n",
            "4\t4\t2\t4\n",
            ".\t.\t4\t.\n",
            "2\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 6, action=3, reward=8.60, score=17.78\n",
            ".\t8\t2\t4\n",
            ".\t.\t2\t4\n",
            ".\t.\t.\t2\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 7, action=2, reward=0.09, score=17.87\n",
            "8\t2\t4\t.\n",
            "2\t4\t.\t.\n",
            "2\t.\t.\t2\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 8, action=0, reward=4.09, score=21.96\n",
            "8\t2\t4\t2\n",
            "4\t4\t.\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t2\t.\n",
            "\n",
            "Step 9, action=1, reward=0.08, score=22.04\n",
            ".\t.\t2\t.\n",
            ".\t.\t.\t.\n",
            "8\t2\t4\t.\n",
            "4\t4\t2\t2\n",
            "\n",
            "Step 10, action=3, reward=12.09, score=34.13\n",
            ".\t.\t.\t2\n",
            "2\t.\t.\t.\n",
            ".\t8\t2\t4\n",
            ".\t.\t8\t4\n",
            "\n",
            "Step 11, action=0, reward=8.09, score=42.22\n",
            "2\t8\t2\t2\n",
            ".\t2\t8\t8\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 12, action=1, reward=0.08, score=42.30\n",
            "2\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            ".\t8\t2\t2\n",
            "2\t2\t8\t8\n",
            "\n",
            "Step 13, action=2, reward=24.60, score=66.90\n",
            "2\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "8\t4\t2\t.\n",
            "4\t16\t.\t.\n",
            "\n",
            "Step 14, action=1, reward=0.09, score=66.99\n",
            ".\t2\t.\t.\n",
            "2\t.\t.\t.\n",
            "8\t4\t.\t.\n",
            "4\t16\t2\t.\n",
            "\n",
            "Step 15, action=3, reward=0.08, score=67.07\n",
            ".\t2\t.\t2\n",
            ".\t.\t.\t2\n",
            ".\t.\t8\t4\n",
            ".\t4\t16\t2\n",
            "\n",
            "Step 16, action=1, reward=4.08, score=71.15\n",
            ".\t.\t.\t2\n",
            ".\t.\t.\t4\n",
            ".\t2\t8\t4\n",
            ".\t4\t16\t2\n",
            "\n",
            "Step 17, action=1, reward=8.08, score=79.23\n",
            ".\t.\t.\t.\n",
            ".\t4\t.\t2\n",
            ".\t2\t8\t8\n",
            ".\t4\t16\t2\n",
            "\n",
            "Step 18, action=3, reward=16.08, score=95.31\n",
            ".\t.\t2\t.\n",
            ".\t.\t4\t2\n",
            ".\t.\t2\t16\n",
            ".\t4\t16\t2\n",
            "\n",
            "Step 19, action=1, reward=-1.92, score=93.39\n",
            ".\t.\t2\t.\n",
            ".\t.\t4\t2\n",
            ".\t.\t2\t16\n",
            ".\t4\t16\t2\n",
            "\n",
            "Step 20, action=3, reward=0.07, score=93.46\n",
            ".\t2\t.\t2\n",
            ".\t.\t4\t2\n",
            ".\t.\t2\t16\n",
            ".\t4\t16\t2\n",
            "\n",
            "Step 21, action=3, reward=4.07, score=97.53\n",
            ".\t.\t.\t4\n",
            ".\t.\t4\t2\n",
            ".\t.\t2\t16\n",
            "2\t4\t16\t2\n",
            "\n",
            "Step 22, action=0, reward=0.06, score=97.59\n",
            "2\t4\t4\t4\n",
            ".\t.\t2\t2\n",
            ".\t.\t16\t16\n",
            ".\t2\t.\t2\n",
            "\n",
            "Step 23, action=0, reward=0.05, score=97.64\n",
            "2\t4\t4\t4\n",
            ".\t2\t2\t2\n",
            ".\t.\t16\t16\n",
            ".\t.\t4\t2\n",
            "\n",
            "Step 24, action=2, reward=44.57, score=142.21\n",
            "2\t8\t4\t2\n",
            "4\t2\t.\t.\n",
            "32\t.\t.\t.\n",
            "4\t2\t.\t.\n",
            "\n",
            "Step 25, action=1, reward=4.07, score=146.28\n",
            "2\t.\t.\t.\n",
            "4\t2\t.\t.\n",
            "32\t8\t.\t.\n",
            "4\t4\t4\t2\n",
            "\n",
            "Step 26, action=2, reward=8.07, score=154.35\n",
            "2\t.\t.\t.\n",
            "4\t2\t4\t.\n",
            "32\t8\t.\t.\n",
            "8\t4\t2\t.\n",
            "\n",
            "Step 27, action=3, reward=0.06, score=154.41\n",
            ".\t.\t2\t2\n",
            ".\t4\t2\t4\n",
            ".\t.\t32\t8\n",
            ".\t8\t4\t2\n",
            "\n",
            "Step 28, action=0, reward=4.06, score=158.47\n",
            "2\t4\t4\t2\n",
            ".\t8\t32\t4\n",
            ".\t.\t4\t8\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 29, action=0, reward=-1.94, score=156.53\n",
            "2\t4\t4\t2\n",
            ".\t8\t32\t4\n",
            ".\t.\t4\t8\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 30, action=0, reward=-1.94, score=154.59\n",
            "2\t4\t4\t2\n",
            ".\t8\t32\t4\n",
            ".\t.\t4\t8\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 31, action=3, reward=8.06, score=162.65\n",
            ".\t2\t8\t2\n",
            ".\t8\t32\t4\n",
            ".\t.\t4\t8\n",
            ".\t.\t2\t2\n",
            "\n",
            "Step 32, action=3, reward=4.06, score=166.71\n",
            ".\t2\t8\t2\n",
            ".\t8\t32\t4\n",
            ".\t2\t4\t8\n",
            ".\t.\t.\t4\n",
            "\n",
            "Step 33, action=3, reward=-1.94, score=164.77\n",
            ".\t2\t8\t2\n",
            ".\t8\t32\t4\n",
            ".\t2\t4\t8\n",
            ".\t.\t.\t4\n",
            "\n",
            "Step 34, action=0, reward=-1.94, score=162.83\n",
            ".\t2\t8\t2\n",
            ".\t8\t32\t4\n",
            ".\t2\t4\t8\n",
            ".\t.\t.\t4\n",
            "\n",
            "Step 35, action=1, reward=0.05, score=162.88\n",
            ".\t2\t.\t2\n",
            ".\t2\t8\t4\n",
            ".\t8\t32\t8\n",
            ".\t2\t4\t4\n",
            "\n",
            "Step 36, action=3, reward=12.06, score=174.94\n",
            ".\t.\t.\t4\n",
            ".\t2\t8\t4\n",
            ".\t8\t32\t8\n",
            "2\t.\t2\t8\n",
            "\n",
            "Step 37, action=0, reward=24.07, score=199.01\n",
            "2\t2\t8\t8\n",
            ".\t8\t32\t16\n",
            ".\t.\t2\t2\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 38, action=2, reward=24.09, score=223.10\n",
            "4\t16\t.\t.\n",
            "8\t32\t16\t.\n",
            "4\t.\t.\t2\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 39, action=0, reward=0.08, score=223.18\n",
            "4\t16\t16\t2\n",
            "8\t32\t.\t.\n",
            "4\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "\n",
            "Step 40, action=2, reward=32.08, score=255.26\n",
            "4\t32\t2\t.\n",
            "8\t32\t4\t.\n",
            "4\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "\n",
            "Step 41, action=1, reward=64.58, score=319.84\n",
            "4\t.\t.\t.\n",
            "8\t.\t.\t2\n",
            "4\t.\t2\t.\n",
            "2\t64\t4\t.\n",
            "\n",
            "Step 42, action=1, reward=0.07, score=319.91\n",
            "4\t.\t.\t.\n",
            "8\t.\t.\t.\n",
            "4\t2\t2\t.\n",
            "2\t64\t4\t2\n",
            "\n",
            "Step 43, action=0, reward=0.06, score=319.97\n",
            "4\t2\t2\t2\n",
            "8\t64\t4\t.\n",
            "4\t.\t.\t.\n",
            "2\t.\t2\t.\n",
            "\n",
            "Step 44, action=3, reward=8.07, score=328.04\n",
            ".\t4\t2\t4\n",
            ".\t8\t64\t4\n",
            ".\t.\t.\t4\n",
            ".\t2\t.\t4\n",
            "\n",
            "Step 45, action=2, reward=0.06, score=328.10\n",
            "4\t2\t4\t.\n",
            "8\t64\t4\t.\n",
            "4\t2\t.\t.\n",
            "2\t4\t.\t.\n",
            "\n",
            "Step 46, action=3, reward=0.05, score=328.15\n",
            ".\t4\t2\t4\n",
            ".\t8\t64\t4\n",
            "2\t.\t4\t2\n",
            ".\t.\t2\t4\n",
            "\n",
            "Step 47, action=1, reward=8.05, score=336.20\n",
            ".\t.\t2\t.\n",
            ".\t2\t64\t8\n",
            ".\t4\t4\t2\n",
            "2\t8\t2\t4\n",
            "\n",
            "Step 48, action=3, reward=8.05, score=344.25\n",
            ".\t.\t.\t2\n",
            ".\t2\t64\t8\n",
            "2\t.\t8\t2\n",
            "2\t8\t2\t4\n",
            "\n",
            "Step 49, action=1, reward=4.05, score=348.30\n",
            ".\t.\t2\t2\n",
            ".\t.\t64\t8\n",
            ".\t2\t8\t2\n",
            "4\t8\t2\t4\n",
            "\n",
            "Step 50, action=1, reward=-1.95, score=346.35\n",
            ".\t.\t2\t2\n",
            ".\t.\t64\t8\n",
            ".\t2\t8\t2\n",
            "4\t8\t2\t4\n",
            "\n",
            "Step 51, action=2, reward=4.05, score=350.40\n",
            "4\t.\t.\t.\n",
            "64\t8\t.\t2\n",
            "2\t8\t2\t.\n",
            "4\t8\t2\t4\n",
            "\n",
            "Step 52, action=3, reward=0.04, score=350.44\n",
            ".\t.\t2\t4\n",
            ".\t64\t8\t2\n",
            ".\t2\t8\t2\n",
            "4\t8\t2\t4\n",
            "\n",
            "Step 53, action=3, reward=-1.96, score=348.48\n",
            ".\t.\t2\t4\n",
            ".\t64\t8\t2\n",
            ".\t2\t8\t2\n",
            "4\t8\t2\t4\n",
            "\n",
            "Step 54, action=0, reward=20.05, score=368.53\n",
            "4\t64\t2\t4\n",
            "2\t2\t16\t4\n",
            ".\t8\t2\t4\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 55, action=1, reward=8.05, score=376.58\n",
            "2\t.\t.\t.\n",
            ".\t64\t2\t.\n",
            "4\t2\t16\t4\n",
            "2\t8\t2\t8\n",
            "\n",
            "Step 56, action=3, reward=0.04, score=376.62\n",
            ".\t.\t.\t2\n",
            "2\t.\t64\t2\n",
            "4\t2\t16\t4\n",
            "2\t8\t2\t8\n",
            "\n",
            "Step 57, action=1, reward=4.04, score=380.66\n",
            "2\t.\t.\t.\n",
            "2\t.\t64\t4\n",
            "4\t2\t16\t4\n",
            "2\t8\t2\t8\n",
            "\n",
            "Step 58, action=0, reward=12.05, score=392.71\n",
            "4\t2\t64\t8\n",
            "4\t8\t16\t8\n",
            "2\t.\t2\t.\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 59, action=1, reward=24.06, score=416.77\n",
            ".\t.\t.\t.\n",
            "4\t.\t64\t.\n",
            "8\t2\t16\t16\n",
            "2\t8\t2\t2\n",
            "\n",
            "Step 60, action=3, reward=36.07, score=452.84\n",
            ".\t.\t.\t.\n",
            "2\t.\t4\t64\n",
            ".\t8\t2\t32\n",
            ".\t2\t8\t4\n",
            "\n",
            "Step 61, action=3, reward=0.06, score=452.90\n",
            ".\t.\t.\t.\n",
            "2\t2\t4\t64\n",
            ".\t8\t2\t32\n",
            ".\t2\t8\t4\n",
            "\n",
            "Step 62, action=2, reward=4.06, score=456.96\n",
            ".\t.\t.\t4\n",
            "4\t4\t64\t.\n",
            "8\t2\t32\t.\n",
            "2\t8\t4\t.\n",
            "\n",
            "Step 63, action=0, reward=0.05, score=457.01\n",
            "4\t4\t64\t4\n",
            "8\t2\t32\t.\n",
            "2\t8\t4\t.\n",
            ".\t.\t2\t.\n",
            "\n",
            "Step 64, action=0, reward=-1.95, score=455.06\n",
            "4\t4\t64\t4\n",
            "8\t2\t32\t.\n",
            "2\t8\t4\t.\n",
            ".\t.\t2\t.\n",
            "\n",
            "Step 65, action=3, reward=8.05, score=463.11\n",
            ".\t8\t64\t4\n",
            "2\t8\t2\t32\n",
            ".\t2\t8\t4\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 66, action=1, reward=16.05, score=479.16\n",
            "2\t.\t.\t4\n",
            ".\t.\t64\t32\n",
            ".\t16\t2\t4\n",
            "2\t2\t8\t2\n",
            "\n",
            "Step 67, action=2, reward=4.05, score=483.21\n",
            "2\t4\t.\t.\n",
            "64\t32\t.\t.\n",
            "16\t2\t4\t4\n",
            "4\t8\t2\t.\n",
            "\n",
            "Step 68, action=1, reward=0.04, score=483.25\n",
            "2\t4\t.\t.\n",
            "64\t32\t.\t.\n",
            "16\t2\t4\t2\n",
            "4\t8\t2\t4\n",
            "\n",
            "Step 69, action=1, reward=-1.96, score=481.29\n",
            "2\t4\t.\t.\n",
            "64\t32\t.\t.\n",
            "16\t2\t4\t2\n",
            "4\t8\t2\t4\n",
            "\n",
            "Step 70, action=3, reward=0.03, score=481.32\n",
            ".\t2\t2\t4\n",
            ".\t.\t64\t32\n",
            "16\t2\t4\t2\n",
            "4\t8\t2\t4\n",
            "\n",
            "Step 71, action=1, reward=4.03, score=485.35\n",
            ".\t.\t2\t4\n",
            "2\t.\t64\t32\n",
            "16\t4\t4\t2\n",
            "4\t8\t2\t4\n",
            "\n",
            "Step 72, action=2, reward=8.03, score=493.38\n",
            "2\t4\t.\t2\n",
            "2\t64\t32\t.\n",
            "16\t8\t2\t.\n",
            "4\t8\t2\t4\n",
            "\n",
            "Step 73, action=0, reward=24.05, score=517.43\n",
            "4\t4\t32\t2\n",
            "16\t64\t4\t4\n",
            "4\t16\t.\t.\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 74, action=0, reward=0.04, score=517.47\n",
            "4\t4\t32\t2\n",
            "16\t64\t4\t4\n",
            "4\t16\t.\t2\n",
            ".\t2\t.\t.\n",
            "\n",
            "Step 75, action=0, reward=-1.96, score=515.51\n",
            "4\t4\t32\t2\n",
            "16\t64\t4\t4\n",
            "4\t16\t.\t2\n",
            ".\t2\t.\t.\n",
            "\n",
            "Step 76, action=3, reward=16.05, score=531.56\n",
            "2\t8\t32\t2\n",
            ".\t16\t64\t8\n",
            ".\t4\t16\t2\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 77, action=0, reward=4.05, score=535.61\n",
            "2\t8\t32\t2\n",
            ".\t16\t64\t8\n",
            ".\t4\t16\t4\n",
            ".\t2\t.\t.\n",
            "\n",
            "Step 78, action=1, reward=0.04, score=535.65\n",
            "2\t8\t.\t.\n",
            ".\t16\t32\t2\n",
            ".\t4\t64\t8\n",
            "2\t2\t16\t4\n",
            "\n",
            "Step 79, action=3, reward=4.04, score=539.69\n",
            "2\t.\t2\t8\n",
            ".\t16\t32\t2\n",
            ".\t4\t64\t8\n",
            ".\t4\t16\t4\n",
            "\n",
            "Step 80, action=3, reward=4.04, score=543.73\n",
            "2\t.\t4\t8\n",
            ".\t16\t32\t2\n",
            ".\t4\t64\t8\n",
            ".\t4\t16\t4\n",
            "\n",
            "Step 81, action=0, reward=8.04, score=551.77\n",
            "2\t16\t4\t8\n",
            ".\t8\t32\t2\n",
            ".\t.\t64\t8\n",
            "2\t.\t16\t4\n",
            "\n",
            "Step 82, action=3, reward=0.03, score=551.80\n",
            "2\t16\t4\t8\n",
            ".\t8\t32\t2\n",
            ".\t2\t64\t8\n",
            ".\t2\t16\t4\n",
            "\n",
            "Step 83, action=3, reward=-1.97, score=549.83\n",
            "2\t16\t4\t8\n",
            ".\t8\t32\t2\n",
            ".\t2\t64\t8\n",
            ".\t2\t16\t4\n",
            "\n",
            "Step 84, action=0, reward=4.03, score=553.86\n",
            "2\t16\t4\t8\n",
            "2\t8\t32\t2\n",
            ".\t4\t64\t8\n",
            ".\t.\t16\t4\n",
            "\n",
            "Step 85, action=0, reward=4.03, score=557.89\n",
            "4\t16\t4\t8\n",
            ".\t8\t32\t2\n",
            ".\t4\t64\t8\n",
            "2\t.\t16\t4\n",
            "\n",
            "Step 86, action=0, reward=0.02, score=557.91\n",
            "4\t16\t4\t8\n",
            "2\t8\t32\t2\n",
            ".\t4\t64\t8\n",
            ".\t2\t16\t4\n",
            "\n",
            "Step 87, action=1, reward=0.01, score=557.92\n",
            ".\t16\t4\t8\n",
            "2\t8\t32\t2\n",
            "4\t4\t64\t8\n",
            "2\t2\t16\t4\n",
            "\n",
            "Step 88, action=3, reward=12.02, score=569.94\n",
            ".\t16\t4\t8\n",
            "2\t8\t32\t2\n",
            ".\t8\t64\t8\n",
            "2\t4\t16\t4\n",
            "\n",
            "Step 89, action=3, reward=-1.98, score=567.96\n",
            ".\t16\t4\t8\n",
            "2\t8\t32\t2\n",
            ".\t8\t64\t8\n",
            "2\t4\t16\t4\n",
            "\n",
            "Step 90, action=0, reward=20.03, score=587.99\n",
            "4\t16\t4\t8\n",
            "2\t16\t32\t2\n",
            ".\t4\t64\t8\n",
            ".\t.\t16\t4\n",
            "\n",
            "Step 91, action=3, reward=-1.97, score=586.02\n",
            "4\t16\t4\t8\n",
            "2\t16\t32\t2\n",
            ".\t4\t64\t8\n",
            ".\t.\t16\t4\n",
            "\n",
            "Step 92, action=3, reward=-1.97, score=584.05\n",
            "4\t16\t4\t8\n",
            "2\t16\t32\t2\n",
            ".\t4\t64\t8\n",
            ".\t.\t16\t4\n",
            "\n",
            "Step 93, action=0, reward=32.03, score=616.08\n",
            "4\t32\t4\t8\n",
            "2\t4\t32\t2\n",
            "2\t.\t64\t8\n",
            ".\t.\t16\t4\n",
            "\n",
            "Step 94, action=3, reward=0.02, score=616.10\n",
            "4\t32\t4\t8\n",
            "2\t4\t32\t2\n",
            ".\t2\t64\t8\n",
            ".\t2\t16\t4\n",
            "\n",
            "Step 95, action=3, reward=-1.98, score=614.12\n",
            "4\t32\t4\t8\n",
            "2\t4\t32\t2\n",
            ".\t2\t64\t8\n",
            ".\t2\t16\t4\n",
            "\n",
            "Step 96, action=3, reward=-1.98, score=612.14\n",
            "4\t32\t4\t8\n",
            "2\t4\t32\t2\n",
            ".\t2\t64\t8\n",
            ".\t2\t16\t4\n",
            "\n",
            "Step 97, action=3, reward=-1.98, score=610.16\n",
            "4\t32\t4\t8\n",
            "2\t4\t32\t2\n",
            ".\t2\t64\t8\n",
            ".\t2\t16\t4\n",
            "\n",
            "Step 98, action=1, reward=4.02, score=614.18\n",
            "2\t.\t4\t8\n",
            ".\t32\t32\t2\n",
            "4\t4\t64\t8\n",
            "2\t4\t16\t4\n",
            "\n",
            "Step 99, action=0, reward=8.02, score=622.20\n",
            "2\t32\t4\t8\n",
            "4\t8\t32\t2\n",
            "2\t.\t64\t8\n",
            ".\t2\t16\t4\n",
            "\n",
            "Step 100, action=3, reward=0.01, score=622.21\n",
            "2\t32\t4\t8\n",
            "4\t8\t32\t2\n",
            ".\t2\t64\t8\n",
            "2\t2\t16\t4\n",
            "\n",
            "Step 101, action=1, reward=4.01, score=626.22\n",
            ".\t2\t4\t8\n",
            "2\t32\t32\t2\n",
            "4\t8\t64\t8\n",
            "2\t4\t16\t4\n",
            "\n",
            "Step 102, action=1, reward=-1.99, score=624.23\n",
            ".\t2\t4\t8\n",
            "2\t32\t32\t2\n",
            "4\t8\t64\t8\n",
            "2\t4\t16\t4\n",
            "\n",
            "Step 103, action=3, reward=64.01, score=688.24\n",
            "2\t2\t4\t8\n",
            ".\t2\t64\t2\n",
            "4\t8\t64\t8\n",
            "2\t4\t16\t4\n",
            "\n",
            "Step 104, action=3, reward=4.01, score=692.25\n",
            "2\t4\t4\t8\n",
            ".\t2\t64\t2\n",
            "4\t8\t64\t8\n",
            "2\t4\t16\t4\n",
            "\n",
            "Step 105, action=1, reward=128.51, score=820.76\n",
            ".\t4\t2\t8\n",
            "2\t2\t4\t2\n",
            "4\t8\t128\t8\n",
            "2\t4\t16\t4\n",
            "\n",
            "Step 106, action=1, reward=-1.99, score=818.77\n",
            ".\t4\t2\t8\n",
            "2\t2\t4\t2\n",
            "4\t8\t128\t8\n",
            "2\t4\t16\t4\n",
            "\n",
            "Step 107, action=3, reward=4.01, score=822.78\n",
            ".\t4\t2\t8\n",
            "2\t4\t4\t2\n",
            "4\t8\t128\t8\n",
            "2\t4\t16\t4\n",
            "\n",
            "Step 108, action=0, reward=8.01, score=830.79\n",
            "2\t8\t2\t8\n",
            "4\t8\t4\t2\n",
            "2\t4\t128\t8\n",
            ".\t2\t16\t4\n",
            "\n",
            "Step 109, action=0, reward=16.01, score=846.80\n",
            "2\t16\t2\t8\n",
            "4\t4\t4\t2\n",
            "2\t2\t128\t8\n",
            "2\t.\t16\t4\n",
            "\n",
            "Step 110, action=0, reward=4.01, score=850.81\n",
            "2\t16\t2\t8\n",
            "4\t4\t4\t2\n",
            "4\t2\t128\t8\n",
            ".\t2\t16\t4\n",
            "\n",
            "Step 111, action=0, reward=12.02, score=862.83\n",
            "2\t16\t2\t8\n",
            "8\t4\t4\t2\n",
            ".\t4\t128\t8\n",
            ".\t2\t16\t4\n",
            "\n",
            "Step 112, action=1, reward=8.02, score=870.85\n",
            ".\t.\t2\t8\n",
            "2\t16\t4\t2\n",
            "2\t8\t128\t8\n",
            "8\t2\t16\t4\n",
            "\n",
            "Step 113, action=1, reward=4.02, score=874.87\n",
            "2\t.\t2\t8\n",
            ".\t16\t4\t2\n",
            "4\t8\t128\t8\n",
            "8\t2\t16\t4\n",
            "\n",
            "Step 114, action=1, reward=0.01, score=874.88\n",
            "2\t.\t2\t8\n",
            "2\t16\t4\t2\n",
            "4\t8\t128\t8\n",
            "8\t2\t16\t4\n",
            "\n",
            "Step 115, action=1, reward=4.01, score=878.89\n",
            ".\t2\t2\t8\n",
            "4\t16\t4\t2\n",
            "4\t8\t128\t8\n",
            "8\t2\t16\t4\n",
            "\n",
            "Step 116, action=1, reward=8.01, score=886.90\n",
            ".\t2\t2\t8\n",
            "2\t16\t4\t2\n",
            "8\t8\t128\t8\n",
            "8\t2\t16\t4\n",
            "\n",
            "Step 117, action=3, reward=20.02, score=906.92\n",
            "4\t.\t4\t8\n",
            "2\t16\t4\t2\n",
            ".\t16\t128\t8\n",
            "8\t2\t16\t4\n",
            "\n",
            "Step 118, action=0, reward=40.03, score=946.95\n",
            "4\t32\t8\t8\n",
            "2\t2\t128\t2\n",
            "8\t2\t16\t8\n",
            ".\t.\t.\t4\n",
            "\n",
            "Step 119, action=0, reward=4.03, score=950.98\n",
            "4\t32\t8\t8\n",
            "2\t4\t128\t2\n",
            "8\t.\t16\t8\n",
            ".\t4\t.\t4\n",
            "\n",
            "Step 120, action=3, reward=24.04, score=975.02\n",
            "2\t4\t32\t16\n",
            "2\t4\t128\t2\n",
            ".\t8\t16\t8\n",
            ".\t.\t.\t8\n",
            "\n",
            "Step 121, action=0, reward=28.06, score=1003.08\n",
            "4\t8\t32\t16\n",
            ".\t8\t128\t2\n",
            ".\t.\t16\t16\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 122, action=3, reward=32.06, score=1035.14\n",
            "4\t8\t32\t16\n",
            ".\t8\t128\t2\n",
            ".\t.\t.\t32\n",
            ".\t2\t.\t2\n",
            "\n",
            "Step 123, action=0, reward=16.06, score=1051.20\n",
            "4\t16\t32\t16\n",
            ".\t2\t128\t2\n",
            ".\t.\t2\t32\n",
            ".\t.\t.\t2\n",
            "\n",
            "Step 124, action=2, reward=0.05, score=1051.25\n",
            "4\t16\t32\t16\n",
            "2\t128\t2\t.\n",
            "2\t32\t2\t.\n",
            "2\t.\t.\t.\n",
            "\n",
            "Step 125, action=1, reward=8.06, score=1059.31\n",
            ".\t2\t.\t.\n",
            "4\t16\t.\t.\n",
            "2\t128\t32\t.\n",
            "4\t32\t4\t16\n",
            "\n",
            "Step 126, action=2, reward=0.05, score=1059.36\n",
            "2\t.\t.\t.\n",
            "4\t16\t.\t2\n",
            "2\t128\t32\t.\n",
            "4\t32\t4\t16\n",
            "\n",
            "Step 127, action=3, reward=0.04, score=1059.40\n",
            ".\t.\t2\t2\n",
            ".\t4\t16\t2\n",
            ".\t2\t128\t32\n",
            "4\t32\t4\t16\n",
            "\n",
            "Step 128, action=2, reward=4.04, score=1063.44\n",
            "4\t.\t.\t.\n",
            "4\t16\t2\t2\n",
            "2\t128\t32\t.\n",
            "4\t32\t4\t16\n",
            "\n",
            "Step 129, action=3, reward=4.04, score=1067.48\n",
            ".\t.\t.\t4\n",
            "2\t4\t16\t4\n",
            ".\t2\t128\t32\n",
            "4\t32\t4\t16\n",
            "\n",
            "Step 130, action=1, reward=8.04, score=1075.52\n",
            ".\t.\t.\t.\n",
            "2\t4\t16\t8\n",
            "2\t2\t128\t32\n",
            "4\t32\t4\t16\n",
            "\n",
            "Step 131, action=2, reward=4.04, score=1079.56\n",
            ".\t.\t2\t.\n",
            "2\t4\t16\t8\n",
            "4\t128\t32\t.\n",
            "4\t32\t4\t16\n",
            "\n",
            "Step 132, action=2, reward=0.03, score=1079.59\n",
            "2\t4\t.\t.\n",
            "2\t4\t16\t8\n",
            "4\t128\t32\t.\n",
            "4\t32\t4\t16\n",
            "\n",
            "Step 133, action=3, reward=0.02, score=1079.61\n",
            ".\t.\t2\t4\n",
            "2\t4\t16\t8\n",
            "2\t4\t128\t32\n",
            "4\t32\t4\t16\n",
            "\n",
            "Step 134, action=3, reward=-1.98, score=1077.63\n",
            ".\t.\t2\t4\n",
            "2\t4\t16\t8\n",
            "2\t4\t128\t32\n",
            "4\t32\t4\t16\n",
            "\n",
            "Step 135, action=0, reward=12.03, score=1089.66\n",
            "4\t8\t2\t4\n",
            "4\t32\t16\t8\n",
            "4\t.\t128\t32\n",
            ".\t.\t4\t16\n",
            "\n",
            "Step 136, action=3, reward=0.02, score=1089.68\n",
            "4\t8\t2\t4\n",
            "4\t32\t16\t8\n",
            ".\t4\t128\t32\n",
            "2\t.\t4\t16\n",
            "\n",
            "Step 137, action=3, reward=0.01, score=1089.69\n",
            "4\t8\t2\t4\n",
            "4\t32\t16\t8\n",
            ".\t4\t128\t32\n",
            "2\t2\t4\t16\n",
            "\n",
            "Step 138, action=3, reward=4.01, score=1093.70\n",
            "4\t8\t2\t4\n",
            "4\t32\t16\t8\n",
            "2\t4\t128\t32\n",
            ".\t4\t4\t16\n",
            "\n",
            "Step 139, action=0, reward=16.02, score=1109.72\n",
            "8\t8\t2\t4\n",
            "2\t32\t16\t8\n",
            ".\t8\t128\t32\n",
            "2\t.\t4\t16\n",
            "\n",
            "Step 140, action=3, reward=16.02, score=1125.74\n",
            ".\t16\t2\t4\n",
            "2\t32\t16\t8\n",
            "2\t8\t128\t32\n",
            ".\t2\t4\t16\n",
            "\n",
            "Step 141, action=1, reward=4.02, score=1129.76\n",
            "2\t16\t2\t4\n",
            ".\t32\t16\t8\n",
            ".\t8\t128\t32\n",
            "4\t2\t4\t16\n",
            "\n",
            "Step 142, action=1, reward=0.01, score=1129.77\n",
            ".\t16\t2\t4\n",
            "4\t32\t16\t8\n",
            "2\t8\t128\t32\n",
            "4\t2\t4\t16\n",
            "\n",
            "Step 143, action=1, reward=-1.99, score=1127.78\n",
            ".\t16\t2\t4\n",
            "4\t32\t16\t8\n",
            "2\t8\t128\t32\n",
            "4\t2\t4\t16\n",
            "\n",
            "Step 144, action=2, reward=0.00, score=1127.78\n",
            "16\t2\t4\t2\n",
            "4\t32\t16\t8\n",
            "2\t8\t128\t32\n",
            "4\t2\t4\t16\n",
            "\n",
            "Episode finished.\n",
            "Final score: 1127.7799999999986, max tile: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# ---- Settings ----\n",
        "NUM_EPISODES = 30\n",
        "MODEL_PATH = \"ppo_2048_v2_simple\"  # change if your model name is different\n",
        "\n",
        "# ---- Helper: run one episode with a given policy ----\n",
        "def run_episode_with_policy(env, policy_fn, render=False):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "    final_score = 0.0\n",
        "    max_tile = 0\n",
        "\n",
        "    while not done:\n",
        "        action = policy_fn(obs, env)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        total_reward += reward\n",
        "        final_score = info[\"score\"]\n",
        "        max_tile = info[\"max_tile\"]\n",
        "\n",
        "        if render:\n",
        "            print(env._board_to_string())\n",
        "            print(f\"Action: {action}, Reward: {reward:.2f}, Score: {final_score:.2f}\")\n",
        "            print(\"-\" * 40)\n",
        "\n",
        "    return total_reward, final_score, max_tile\n",
        "\n",
        "# ---- Random policy ----\n",
        "def random_policy(obs, env):\n",
        "    return env.action_space.sample()\n",
        "\n",
        "# ---- PPO policy ----\n",
        "print(\"Loading PPO model...\")\n",
        "ppo_model = PPO.load(MODEL_PATH)\n",
        "\n",
        "def ppo_policy(obs, env):\n",
        "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
        "    return int(action)\n",
        "\n",
        "# ---- Evaluate both ----\n",
        "def evaluate_policy(name, policy_fn, num_episodes=NUM_EPISODES, render=False):\n",
        "    rewards = []\n",
        "    scores = []\n",
        "    max_tiles = []\n",
        "\n",
        "    for ep in range(num_episodes):\n",
        "        env = Game2048EnvV2()  # new fresh env each episode\n",
        "        total_r, score, max_tile = run_episode_with_policy(env, policy_fn, render=False)\n",
        "        rewards.append(total_r)\n",
        "        scores.append(score)\n",
        "        max_tiles.append(max_tile)\n",
        "\n",
        "    print(f\"\\n=== {name} over {num_episodes} episodes ===\")\n",
        "    print(f\"Avg total reward: {np.mean(rewards):.2f} ¬± {np.std(rewards):.2f}\")\n",
        "    print(f\"Avg final score:  {np.mean(scores):.2f} ¬± {np.std(scores):.2f}\")\n",
        "    print(f\"Avg max tile:     {np.mean(max_tiles):.1f}\")\n",
        "    print(f\"Max of max tiles: {np.max(max_tiles)}\")\n",
        "\n",
        "    return rewards, scores, max_tiles\n",
        "\n",
        "# Run comparison\n",
        "rand_rewards, rand_scores, rand_tiles = evaluate_policy(\"Random policy\", random_policy)\n",
        "ppo_rewards,  ppo_scores,  ppo_tiles  = evaluate_policy(\"PPO policy\",    ppo_policy)\n",
        "\n",
        "print(\"\\nDone.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 530
        },
        "id": "rSsC-Sy7PiI6",
        "outputId": "92a89d28-6184-4513-a15e-d6a6a51152ed"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading PPO model...\n",
            "\n",
            "=== Random policy over 30 episodes ===\n",
            "Avg total reward: 991.10 ¬± 475.12\n",
            "Avg final score:  991.10 ¬± 475.12\n",
            "Avg max tile:     104.5\n",
            "Max of max tiles: 256\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-191529638.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0;31m# Run comparison\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[0mrand_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_scores\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_tiles\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Random policy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m \u001b[0mppo_rewards\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mppo_scores\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mppo_tiles\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mevaluate_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PPO policy\"\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0mppo_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDone.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-191529638.py\u001b[0m in \u001b[0;36mevaluate_policy\u001b[0;34m(name, policy_fn, num_episodes, render)\u001b[0m\n\u001b[1;32m     50\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame2048EnvV2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# new fresh env each episode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mtotal_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_tile\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode_with_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrender\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_r\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-191529638.py\u001b[0m in \u001b[0;36mrun_episode_with_policy\u001b[0;34m(env, policy_fn, render)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-191529638.py\u001b[0m in \u001b[0;36mppo_policy\u001b[0;34m(obs, env)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mppo_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mppo_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/base_class.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    555\u001b[0m             \u001b[0;34m(\u001b[0m\u001b[0mused\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrecurrent\u001b[0m \u001b[0mpolicies\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m         \"\"\"\n\u001b[0;32m--> 557\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpolicy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobservation\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_start\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    558\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mset_random_seed\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mseed\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mOptional\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mint\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/policies.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, observation, state, episode_start, deterministic)\u001b[0m\n\u001b[1;32m    368\u001b[0m             \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs_tensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdeterministic\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdeterministic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;31m# Convert to numpy, and reshape to the original action shape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 370\u001b[0;31m         \u001b[0mactions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mactions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc, assignment]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    371\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    372\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mspaces\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBox\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class Game2048EnvV3(gym.Env):\n",
        "    \"\"\"\n",
        "    Simple, stable 2048 env for PPO.\n",
        "\n",
        "    - Obs: 4x4 grid, exponents / 15.0 in [0, 1]\n",
        "    - Action: 0=up,1=down,2=left,3=right\n",
        "    - Reward:\n",
        "        * (sum of merged tile values) / 32.0\n",
        "        * -1 for invalid move (no board change)\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"ansi\"], \"render_fps\": 60}\n",
        "\n",
        "    def __init__(self, render_mode=None, target_tile=2048):\n",
        "        super().__init__()\n",
        "        self.board_size = 4\n",
        "        self.target_tile = target_tile\n",
        "\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0.0,\n",
        "            high=1.0,\n",
        "            shape=(self.board_size, self.board_size),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.board = np.zeros((self.board_size, self.board_size), dtype=np.int32)\n",
        "        self.score = 0.0\n",
        "        self.rng = np.random.default_rng()\n",
        "\n",
        "    # ---------- Helpers ----------\n",
        "    def _get_obs(self):\n",
        "        return self.board.astype(np.float32) / 15.0\n",
        "\n",
        "    def _slide_and_merge_line(self, line):\n",
        "        non_zero = line[line != 0].tolist()\n",
        "        new = []\n",
        "        merged_value = 0\n",
        "        i = 0\n",
        "        while i < len(non_zero):\n",
        "            if i + 1 < len(non_zero) and non_zero[i] == non_zero[i+1]:\n",
        "                exp = non_zero[i] + 1\n",
        "                new.append(exp)\n",
        "                merged_value += 2 ** exp\n",
        "                i += 2\n",
        "            else:\n",
        "                new.append(non_zero[i])\n",
        "                i += 1\n",
        "        new += [0] * (len(line) - len(new))\n",
        "        return np.array(new, dtype=np.int32), merged_value\n",
        "\n",
        "    def _add_random_tile(self):\n",
        "        empties = list(zip(*np.where(self.board == 0)))\n",
        "        if not empties:\n",
        "            return\n",
        "        r, c = empties[self.rng.integers(len(empties))]\n",
        "        self.board[r, c] = 1 if self.rng.random() < 0.9 else 2  # 2 or 4\n",
        "\n",
        "    def _can_move(self):\n",
        "        if np.any(self.board == 0):\n",
        "            return True\n",
        "        for r in range(self.board_size):\n",
        "            for c in range(self.board_size - 1):\n",
        "                if self.board[r, c] == self.board[r, c+1]:\n",
        "                    return True\n",
        "        for c in range(self.board_size):\n",
        "            for r in range(self.board_size - 1):\n",
        "                if self.board[r, c] == self.board[r+1, c]:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def _get_max_tile(self):\n",
        "        exp = int(self.board.max())\n",
        "        return 0 if exp == 0 else 2 ** exp\n",
        "\n",
        "    # ---------- Gym API ----------\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.board[:] = 0\n",
        "        self.score = 0.0\n",
        "\n",
        "        self._add_random_tile()\n",
        "        self._add_random_tile()\n",
        "\n",
        "        return self._get_obs(), {\"score\": self.score, \"max_tile\": self._get_max_tile()}\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action), \"Invalid action\"\n",
        "\n",
        "        old_board = self.board.copy()\n",
        "        merged_value = 0\n",
        "\n",
        "        if action == 0:  # up\n",
        "            for c in range(self.board_size):\n",
        "                new_line, mv = self._slide_and_merge_line(self.board[:, c])\n",
        "                self.board[:, c] = new_line\n",
        "                merged_value += mv\n",
        "        elif action == 1:  # down\n",
        "            for c in range(self.board_size):\n",
        "                new_line, mv = self._slide_and_merge_line(self.board[:, c][::-1])\n",
        "                self.board[:, c] = new_line[::-1]\n",
        "                merged_value += mv\n",
        "        elif action == 2:  # left\n",
        "            for r in range(self.board_size):\n",
        "                new_line, mv = self._slide_and_merge_line(self.board[r])\n",
        "                self.board[r] = new_line\n",
        "                merged_value += mv\n",
        "        elif action == 3:  # right\n",
        "            for r in range(self.board_size):\n",
        "                new_line, mv = self._slide_and_merge_line(self.board[r][::-1])\n",
        "                self.board[r] = new_line[::-1]\n",
        "                merged_value += mv\n",
        "\n",
        "        moved = not np.array_equal(old_board, self.board)\n",
        "\n",
        "        reward = 0.0\n",
        "        if moved:\n",
        "            self._add_random_tile()\n",
        "            # scale down raw 2048 reward to keep PPO stable\n",
        "            reward += merged_value / 32.0\n",
        "        else:\n",
        "            # strong penalty for useless move\n",
        "            reward -= 1.0\n",
        "\n",
        "        self.score += merged_value  # human-style score for reporting\n",
        "\n",
        "        max_tile = self._get_max_tile()\n",
        "        terminated = (not self._can_move()) or (max_tile >= self.target_tile)\n",
        "        truncated = False\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        info = {\"score\": self.score, \"max_tile\": max_tile}\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def _board_to_string(self):\n",
        "        rows = []\n",
        "        for row in self.board:\n",
        "            r = []\n",
        "            for exp in row:\n",
        "                r.append(\".\" if exp == 0 else str(2 ** exp))\n",
        "            rows.append(\"\\t\".join(r))\n",
        "        return \"\\n\".join(rows)\n"
      ],
      "metadata": {
        "id": "H7B5PM1NVbPv"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from stable_baselines3 import PPO\n",
        "\n",
        "env = Game2048EnvV3()\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    net_arch=[dict(pi=[256, 256, 256],\n",
        "                   vf=[256, 256, 256])]\n",
        ")\n",
        "\n",
        "model = PPO(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    learning_rate=1e-4,   # smaller LR\n",
        "    n_steps=4096,         # more rollout per update\n",
        "    batch_size=512,\n",
        "    n_epochs=20,          # reuse data more\n",
        "    gamma=0.99,\n",
        "    clip_range=0.1,\n",
        "    ent_coef=0.1,         # stronger exploration\n",
        "    target_kl=0.02,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "model.learn(total_timesteps=2_000_000)  # 2M instead of 500k\n",
        "model.save(\"ppo_2048_v3_big\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O0p7Gt_JVepb",
        "outputId": "6553513b-af84-4ce1-bc1e-4488a6331139"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/policies.py:486: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "|    value_loss           | 8.07         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 18.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 252          |\n",
            "|    time_elapsed         | 1865         |\n",
            "|    total_timesteps      | 1032192      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016753317 |\n",
            "|    clip_fraction        | 0.0377       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.461        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.19         |\n",
            "|    n_updates            | 5020         |\n",
            "|    policy_gradient_loss | -0.00353     |\n",
            "|    value_loss           | 7.36         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 19.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 253          |\n",
            "|    time_elapsed         | 1873         |\n",
            "|    total_timesteps      | 1036288      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016772728 |\n",
            "|    clip_fraction        | 0.0421       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.429        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.37         |\n",
            "|    n_updates            | 5040         |\n",
            "|    policy_gradient_loss | -0.00357     |\n",
            "|    value_loss           | 7.58         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 17.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 254          |\n",
            "|    time_elapsed         | 1880         |\n",
            "|    total_timesteps      | 1040384      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016890832 |\n",
            "|    clip_fraction        | 0.0463       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.552        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.58         |\n",
            "|    n_updates            | 5060         |\n",
            "|    policy_gradient_loss | -0.00444     |\n",
            "|    value_loss           | 6.65         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 126         |\n",
            "|    ep_rew_mean          | 18.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 255         |\n",
            "|    time_elapsed         | 1888        |\n",
            "|    total_timesteps      | 1044480     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001037695 |\n",
            "|    clip_fraction        | 0.0132      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.552       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.52        |\n",
            "|    n_updates            | 5080        |\n",
            "|    policy_gradient_loss | -0.00287    |\n",
            "|    value_loss           | 7.62        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 18.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 256          |\n",
            "|    time_elapsed         | 1895         |\n",
            "|    total_timesteps      | 1048576      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018641459 |\n",
            "|    clip_fraction        | 0.0685       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.595        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.87         |\n",
            "|    n_updates            | 5100         |\n",
            "|    policy_gradient_loss | -0.00471     |\n",
            "|    value_loss           | 7.08         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 19.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 257          |\n",
            "|    time_elapsed         | 1903         |\n",
            "|    total_timesteps      | 1052672      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014167677 |\n",
            "|    clip_fraction        | 0.0313       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.484        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.86         |\n",
            "|    n_updates            | 5120         |\n",
            "|    policy_gradient_loss | -0.00366     |\n",
            "|    value_loss           | 8.42         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 258          |\n",
            "|    time_elapsed         | 1910         |\n",
            "|    total_timesteps      | 1056768      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015226618 |\n",
            "|    clip_fraction        | 0.0434       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0.535        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.12         |\n",
            "|    n_updates            | 5140         |\n",
            "|    policy_gradient_loss | -0.00387     |\n",
            "|    value_loss           | 6.63         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 20.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 259          |\n",
            "|    time_elapsed         | 1918         |\n",
            "|    total_timesteps      | 1060864      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014990314 |\n",
            "|    clip_fraction        | 0.0359       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0.428        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.6          |\n",
            "|    n_updates            | 5160         |\n",
            "|    policy_gradient_loss | -0.00333     |\n",
            "|    value_loss           | 9.87         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 21.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 260          |\n",
            "|    time_elapsed         | 1925         |\n",
            "|    total_timesteps      | 1064960      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012088905 |\n",
            "|    clip_fraction        | 0.0314       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.473        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.53         |\n",
            "|    n_updates            | 5180         |\n",
            "|    policy_gradient_loss | -0.00274     |\n",
            "|    value_loss           | 7.08         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 20.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 261          |\n",
            "|    time_elapsed         | 1932         |\n",
            "|    total_timesteps      | 1069056      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014044784 |\n",
            "|    clip_fraction        | 0.0436       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.482        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.24         |\n",
            "|    n_updates            | 5200         |\n",
            "|    policy_gradient_loss | -0.00354     |\n",
            "|    value_loss           | 7.99         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 19.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 262          |\n",
            "|    time_elapsed         | 1940         |\n",
            "|    total_timesteps      | 1073152      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015764779 |\n",
            "|    clip_fraction        | 0.0583       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0.519        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.18         |\n",
            "|    n_updates            | 5220         |\n",
            "|    policy_gradient_loss | -0.00522     |\n",
            "|    value_loss           | 7            |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 17.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 263          |\n",
            "|    time_elapsed         | 1947         |\n",
            "|    total_timesteps      | 1077248      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013361733 |\n",
            "|    clip_fraction        | 0.0375       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.534        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.13         |\n",
            "|    n_updates            | 5240         |\n",
            "|    policy_gradient_loss | -0.00243     |\n",
            "|    value_loss           | 8.47         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 17.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 264          |\n",
            "|    time_elapsed         | 1955         |\n",
            "|    total_timesteps      | 1081344      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014113047 |\n",
            "|    clip_fraction        | 0.0342       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0.532        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.73         |\n",
            "|    n_updates            | 5260         |\n",
            "|    policy_gradient_loss | -0.00356     |\n",
            "|    value_loss           | 6.83         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 17.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 265          |\n",
            "|    time_elapsed         | 1962         |\n",
            "|    total_timesteps      | 1085440      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016966861 |\n",
            "|    clip_fraction        | 0.05         |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0.516        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.69         |\n",
            "|    n_updates            | 5280         |\n",
            "|    policy_gradient_loss | -0.00464     |\n",
            "|    value_loss           | 6.5          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 17.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 266          |\n",
            "|    time_elapsed         | 1969         |\n",
            "|    total_timesteps      | 1089536      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016253302 |\n",
            "|    clip_fraction        | 0.0521       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0.541        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.83         |\n",
            "|    n_updates            | 5300         |\n",
            "|    policy_gradient_loss | -0.00398     |\n",
            "|    value_loss           | 6.75         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 123         |\n",
            "|    ep_rew_mean          | 18          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 267         |\n",
            "|    time_elapsed         | 1977        |\n",
            "|    total_timesteps      | 1093632     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001638494 |\n",
            "|    clip_fraction        | 0.0432      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.489       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.04        |\n",
            "|    n_updates            | 5320        |\n",
            "|    policy_gradient_loss | -0.00309    |\n",
            "|    value_loss           | 7.3         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 18.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 268          |\n",
            "|    time_elapsed         | 1984         |\n",
            "|    total_timesteps      | 1097728      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019003884 |\n",
            "|    clip_fraction        | 0.062        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.285        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.55         |\n",
            "|    n_updates            | 5340         |\n",
            "|    policy_gradient_loss | -0.00485     |\n",
            "|    value_loss           | 9            |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 20.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 269          |\n",
            "|    time_elapsed         | 1991         |\n",
            "|    total_timesteps      | 1101824      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015079257 |\n",
            "|    clip_fraction        | 0.0398       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.299        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.69         |\n",
            "|    n_updates            | 5360         |\n",
            "|    policy_gradient_loss | -0.00358     |\n",
            "|    value_loss           | 9.02         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 19.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 270          |\n",
            "|    time_elapsed         | 1999         |\n",
            "|    total_timesteps      | 1105920      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014211175 |\n",
            "|    clip_fraction        | 0.0247       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.301        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.12         |\n",
            "|    n_updates            | 5380         |\n",
            "|    policy_gradient_loss | -0.00263     |\n",
            "|    value_loss           | 8.09         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 271          |\n",
            "|    time_elapsed         | 2006         |\n",
            "|    total_timesteps      | 1110016      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019622091 |\n",
            "|    clip_fraction        | 0.0616       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.383        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.56         |\n",
            "|    n_updates            | 5400         |\n",
            "|    policy_gradient_loss | -0.00477     |\n",
            "|    value_loss           | 7.68         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 19           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 272          |\n",
            "|    time_elapsed         | 2014         |\n",
            "|    total_timesteps      | 1114112      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015681253 |\n",
            "|    clip_fraction        | 0.0418       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.463        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.59         |\n",
            "|    n_updates            | 5420         |\n",
            "|    policy_gradient_loss | -0.00438     |\n",
            "|    value_loss           | 7.74         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 20.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 273          |\n",
            "|    time_elapsed         | 2021         |\n",
            "|    total_timesteps      | 1118208      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016731188 |\n",
            "|    clip_fraction        | 0.0505       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.472        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.84         |\n",
            "|    n_updates            | 5440         |\n",
            "|    policy_gradient_loss | -0.00412     |\n",
            "|    value_loss           | 6.95         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 18.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 274          |\n",
            "|    time_elapsed         | 2029         |\n",
            "|    total_timesteps      | 1122304      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015804536 |\n",
            "|    clip_fraction        | 0.0365       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.5          |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.06         |\n",
            "|    n_updates            | 5460         |\n",
            "|    policy_gradient_loss | -0.00299     |\n",
            "|    value_loss           | 7.24         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 20.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 275          |\n",
            "|    time_elapsed         | 2036         |\n",
            "|    total_timesteps      | 1126400      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013945708 |\n",
            "|    clip_fraction        | 0.032        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.332        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.4          |\n",
            "|    n_updates            | 5480         |\n",
            "|    policy_gradient_loss | -0.00303     |\n",
            "|    value_loss           | 10.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 21.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 276          |\n",
            "|    time_elapsed         | 2044         |\n",
            "|    total_timesteps      | 1130496      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017294514 |\n",
            "|    clip_fraction        | 0.0389       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.378        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.72         |\n",
            "|    n_updates            | 5500         |\n",
            "|    policy_gradient_loss | -0.00288     |\n",
            "|    value_loss           | 8.41         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 22.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 277          |\n",
            "|    time_elapsed         | 2051         |\n",
            "|    total_timesteps      | 1134592      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016682633 |\n",
            "|    clip_fraction        | 0.0536       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.404        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.16         |\n",
            "|    n_updates            | 5520         |\n",
            "|    policy_gradient_loss | -0.00515     |\n",
            "|    value_loss           | 9.16         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 278          |\n",
            "|    time_elapsed         | 2058         |\n",
            "|    total_timesteps      | 1138688      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014315229 |\n",
            "|    clip_fraction        | 0.0465       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.408        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4            |\n",
            "|    n_updates            | 5540         |\n",
            "|    policy_gradient_loss | -0.00265     |\n",
            "|    value_loss           | 9.52         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 20.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 279          |\n",
            "|    time_elapsed         | 2066         |\n",
            "|    total_timesteps      | 1142784      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021819042 |\n",
            "|    clip_fraction        | 0.0731       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.447        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.31         |\n",
            "|    n_updates            | 5560         |\n",
            "|    policy_gradient_loss | -0.00504     |\n",
            "|    value_loss           | 8.4          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 280          |\n",
            "|    time_elapsed         | 2073         |\n",
            "|    total_timesteps      | 1146880      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016731138 |\n",
            "|    clip_fraction        | 0.0488       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.509        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.35         |\n",
            "|    n_updates            | 5580         |\n",
            "|    policy_gradient_loss | -0.00387     |\n",
            "|    value_loss           | 7.97         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 22.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 281          |\n",
            "|    time_elapsed         | 2081         |\n",
            "|    total_timesteps      | 1150976      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015671103 |\n",
            "|    clip_fraction        | 0.0404       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.398        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.5          |\n",
            "|    n_updates            | 5600         |\n",
            "|    policy_gradient_loss | -0.00362     |\n",
            "|    value_loss           | 8.99         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 22.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 282          |\n",
            "|    time_elapsed         | 2088         |\n",
            "|    total_timesteps      | 1155072      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016541381 |\n",
            "|    clip_fraction        | 0.0427       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.394        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.64         |\n",
            "|    n_updates            | 5620         |\n",
            "|    policy_gradient_loss | -0.00308     |\n",
            "|    value_loss           | 8.19         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 20.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 283          |\n",
            "|    time_elapsed         | 2095         |\n",
            "|    total_timesteps      | 1159168      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014763733 |\n",
            "|    clip_fraction        | 0.052        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.442        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.46         |\n",
            "|    n_updates            | 5640         |\n",
            "|    policy_gradient_loss | -0.00369     |\n",
            "|    value_loss           | 8.55         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 18.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 284          |\n",
            "|    time_elapsed         | 2103         |\n",
            "|    total_timesteps      | 1163264      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015301114 |\n",
            "|    clip_fraction        | 0.0343       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.439        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.97         |\n",
            "|    n_updates            | 5660         |\n",
            "|    policy_gradient_loss | -0.00283     |\n",
            "|    value_loss           | 9.33         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 19.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 285          |\n",
            "|    time_elapsed         | 2110         |\n",
            "|    total_timesteps      | 1167360      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015830994 |\n",
            "|    clip_fraction        | 0.0414       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.553        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.91         |\n",
            "|    n_updates            | 5680         |\n",
            "|    policy_gradient_loss | -0.00395     |\n",
            "|    value_loss           | 7.38         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 125         |\n",
            "|    ep_rew_mean          | 18.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 286         |\n",
            "|    time_elapsed         | 2118        |\n",
            "|    total_timesteps      | 1171456     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001646631 |\n",
            "|    clip_fraction        | 0.0546      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.583       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.23        |\n",
            "|    n_updates            | 5700        |\n",
            "|    policy_gradient_loss | -0.00468    |\n",
            "|    value_loss           | 7.23        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 20           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 287          |\n",
            "|    time_elapsed         | 2125         |\n",
            "|    total_timesteps      | 1175552      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015267043 |\n",
            "|    clip_fraction        | 0.0362       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.497        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.28         |\n",
            "|    n_updates            | 5720         |\n",
            "|    policy_gradient_loss | -0.00263     |\n",
            "|    value_loss           | 8.72         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 288          |\n",
            "|    time_elapsed         | 2132         |\n",
            "|    total_timesteps      | 1179648      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016401571 |\n",
            "|    clip_fraction        | 0.044        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.584        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.67         |\n",
            "|    n_updates            | 5740         |\n",
            "|    policy_gradient_loss | -0.00342     |\n",
            "|    value_loss           | 6.47         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 19.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 289          |\n",
            "|    time_elapsed         | 2140         |\n",
            "|    total_timesteps      | 1183744      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016405864 |\n",
            "|    clip_fraction        | 0.0372       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.494        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.35         |\n",
            "|    n_updates            | 5760         |\n",
            "|    policy_gradient_loss | -0.00321     |\n",
            "|    value_loss           | 8.02         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 20           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 290          |\n",
            "|    time_elapsed         | 2147         |\n",
            "|    total_timesteps      | 1187840      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015980473 |\n",
            "|    clip_fraction        | 0.0525       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.5          |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.1          |\n",
            "|    n_updates            | 5780         |\n",
            "|    policy_gradient_loss | -0.00485     |\n",
            "|    value_loss           | 7.32         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 20.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 291          |\n",
            "|    time_elapsed         | 2155         |\n",
            "|    total_timesteps      | 1191936      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012403452 |\n",
            "|    clip_fraction        | 0.0229       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0.525        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.39         |\n",
            "|    n_updates            | 5800         |\n",
            "|    policy_gradient_loss | -0.00342     |\n",
            "|    value_loss           | 6.81         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 21.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 292          |\n",
            "|    time_elapsed         | 2162         |\n",
            "|    total_timesteps      | 1196032      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014283591 |\n",
            "|    clip_fraction        | 0.0341       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.476        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.78         |\n",
            "|    n_updates            | 5820         |\n",
            "|    policy_gradient_loss | -0.00322     |\n",
            "|    value_loss           | 6.95         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 21           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 293          |\n",
            "|    time_elapsed         | 2169         |\n",
            "|    total_timesteps      | 1200128      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016865171 |\n",
            "|    clip_fraction        | 0.0419       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.408        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.82         |\n",
            "|    n_updates            | 5840         |\n",
            "|    policy_gradient_loss | -0.00371     |\n",
            "|    value_loss           | 9.16         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 21.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 294          |\n",
            "|    time_elapsed         | 2177         |\n",
            "|    total_timesteps      | 1204224      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014819708 |\n",
            "|    clip_fraction        | 0.0365       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.514        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.38         |\n",
            "|    n_updates            | 5860         |\n",
            "|    policy_gradient_loss | -0.00342     |\n",
            "|    value_loss           | 8.02         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 134         |\n",
            "|    ep_rew_mean          | 21.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 295         |\n",
            "|    time_elapsed         | 2184        |\n",
            "|    total_timesteps      | 1208320     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001529149 |\n",
            "|    clip_fraction        | 0.048       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.483       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 4.94        |\n",
            "|    n_updates            | 5880        |\n",
            "|    policy_gradient_loss | -0.00308    |\n",
            "|    value_loss           | 10.7        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 19.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 296          |\n",
            "|    time_elapsed         | 2191         |\n",
            "|    total_timesteps      | 1212416      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015966373 |\n",
            "|    clip_fraction        | 0.0503       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.487        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.02         |\n",
            "|    n_updates            | 5900         |\n",
            "|    policy_gradient_loss | -0.0038      |\n",
            "|    value_loss           | 8.34         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 18.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 297          |\n",
            "|    time_elapsed         | 2199         |\n",
            "|    total_timesteps      | 1216512      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015706884 |\n",
            "|    clip_fraction        | 0.0337       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.473        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.09         |\n",
            "|    n_updates            | 5920         |\n",
            "|    policy_gradient_loss | -0.00303     |\n",
            "|    value_loss           | 7.98         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 16.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 298          |\n",
            "|    time_elapsed         | 2206         |\n",
            "|    total_timesteps      | 1220608      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014015859 |\n",
            "|    clip_fraction        | 0.0306       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.551        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.83         |\n",
            "|    n_updates            | 5940         |\n",
            "|    policy_gradient_loss | -0.00246     |\n",
            "|    value_loss           | 6.82         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 16.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 299          |\n",
            "|    time_elapsed         | 2214         |\n",
            "|    total_timesteps      | 1224704      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013193056 |\n",
            "|    clip_fraction        | 0.0278       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.592        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.07         |\n",
            "|    n_updates            | 5960         |\n",
            "|    policy_gradient_loss | -0.00323     |\n",
            "|    value_loss           | 6.89         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 18.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 300          |\n",
            "|    time_elapsed         | 2221         |\n",
            "|    total_timesteps      | 1228800      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018147668 |\n",
            "|    clip_fraction        | 0.0585       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.515        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.05         |\n",
            "|    n_updates            | 5980         |\n",
            "|    policy_gradient_loss | -0.00509     |\n",
            "|    value_loss           | 7.26         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 19.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 301          |\n",
            "|    time_elapsed         | 2228         |\n",
            "|    total_timesteps      | 1232896      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012710532 |\n",
            "|    clip_fraction        | 0.0305       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.498        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.7          |\n",
            "|    n_updates            | 6000         |\n",
            "|    policy_gradient_loss | -0.00406     |\n",
            "|    value_loss           | 6.79         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 21           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 302          |\n",
            "|    time_elapsed         | 2236         |\n",
            "|    total_timesteps      | 1236992      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013616245 |\n",
            "|    clip_fraction        | 0.0331       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.436        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.14         |\n",
            "|    n_updates            | 6020         |\n",
            "|    policy_gradient_loss | -0.00352     |\n",
            "|    value_loss           | 7.29         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 20           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 303          |\n",
            "|    time_elapsed         | 2243         |\n",
            "|    total_timesteps      | 1241088      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015896753 |\n",
            "|    clip_fraction        | 0.0563       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.43         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.77         |\n",
            "|    n_updates            | 6040         |\n",
            "|    policy_gradient_loss | -0.00515     |\n",
            "|    value_loss           | 6.79         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 19.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 304          |\n",
            "|    time_elapsed         | 2251         |\n",
            "|    total_timesteps      | 1245184      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015062233 |\n",
            "|    clip_fraction        | 0.0463       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.459        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.04         |\n",
            "|    n_updates            | 6060         |\n",
            "|    policy_gradient_loss | -0.00514     |\n",
            "|    value_loss           | 7.56         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 20.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 305          |\n",
            "|    time_elapsed         | 2258         |\n",
            "|    total_timesteps      | 1249280      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0022435985 |\n",
            "|    clip_fraction        | 0.0763       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.34         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.21         |\n",
            "|    n_updates            | 6080         |\n",
            "|    policy_gradient_loss | -0.00454     |\n",
            "|    value_loss           | 8.09         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 21.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 306          |\n",
            "|    time_elapsed         | 2265         |\n",
            "|    total_timesteps      | 1253376      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018122622 |\n",
            "|    clip_fraction        | 0.0517       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.297        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.43         |\n",
            "|    n_updates            | 6100         |\n",
            "|    policy_gradient_loss | -0.00493     |\n",
            "|    value_loss           | 8.58         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 21.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 307          |\n",
            "|    time_elapsed         | 2273         |\n",
            "|    total_timesteps      | 1257472      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018598187 |\n",
            "|    clip_fraction        | 0.0576       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.42         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.46         |\n",
            "|    n_updates            | 6120         |\n",
            "|    policy_gradient_loss | -0.00356     |\n",
            "|    value_loss           | 7.94         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 20.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 308          |\n",
            "|    time_elapsed         | 2280         |\n",
            "|    total_timesteps      | 1261568      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013978714 |\n",
            "|    clip_fraction        | 0.0325       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.457        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.7          |\n",
            "|    n_updates            | 6140         |\n",
            "|    policy_gradient_loss | -0.00365     |\n",
            "|    value_loss           | 8.2          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 19.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 309          |\n",
            "|    time_elapsed         | 2288         |\n",
            "|    total_timesteps      | 1265664      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012787166 |\n",
            "|    clip_fraction        | 0.0299       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.419        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.55         |\n",
            "|    n_updates            | 6160         |\n",
            "|    policy_gradient_loss | -0.00263     |\n",
            "|    value_loss           | 9.46         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 122         |\n",
            "|    ep_rew_mean          | 18.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 310         |\n",
            "|    time_elapsed         | 2295        |\n",
            "|    total_timesteps      | 1269760     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001239591 |\n",
            "|    clip_fraction        | 0.02        |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.28       |\n",
            "|    explained_variance   | 0.413       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 4.43        |\n",
            "|    n_updates            | 6180        |\n",
            "|    policy_gradient_loss | -0.00318    |\n",
            "|    value_loss           | 9.49        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 311          |\n",
            "|    time_elapsed         | 2302         |\n",
            "|    total_timesteps      | 1273856      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017466587 |\n",
            "|    clip_fraction        | 0.0477       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.529        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.47         |\n",
            "|    n_updates            | 6200         |\n",
            "|    policy_gradient_loss | -0.00385     |\n",
            "|    value_loss           | 7.56         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 125         |\n",
            "|    ep_rew_mean          | 19.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 312         |\n",
            "|    time_elapsed         | 2310        |\n",
            "|    total_timesteps      | 1277952     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001527387 |\n",
            "|    clip_fraction        | 0.0397      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.51        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.39        |\n",
            "|    n_updates            | 6220        |\n",
            "|    policy_gradient_loss | -0.00358    |\n",
            "|    value_loss           | 7.19        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 313          |\n",
            "|    time_elapsed         | 2317         |\n",
            "|    total_timesteps      | 1282048      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018704848 |\n",
            "|    clip_fraction        | 0.0586       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0.427        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.34         |\n",
            "|    n_updates            | 6240         |\n",
            "|    policy_gradient_loss | -0.00437     |\n",
            "|    value_loss           | 8.98         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 20.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 314          |\n",
            "|    time_elapsed         | 2325         |\n",
            "|    total_timesteps      | 1286144      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016784237 |\n",
            "|    clip_fraction        | 0.0367       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.394        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.88         |\n",
            "|    n_updates            | 6260         |\n",
            "|    policy_gradient_loss | -0.0028      |\n",
            "|    value_loss           | 8.88         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 315          |\n",
            "|    time_elapsed         | 2332         |\n",
            "|    total_timesteps      | 1290240      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015355244 |\n",
            "|    clip_fraction        | 0.0301       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0.377        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.97         |\n",
            "|    n_updates            | 6280         |\n",
            "|    policy_gradient_loss | -0.00352     |\n",
            "|    value_loss           | 9.28         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 21.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 316          |\n",
            "|    time_elapsed         | 2339         |\n",
            "|    total_timesteps      | 1294336      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015445695 |\n",
            "|    clip_fraction        | 0.04         |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.429        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.98         |\n",
            "|    n_updates            | 6300         |\n",
            "|    policy_gradient_loss | -0.00371     |\n",
            "|    value_loss           | 9.16         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 20.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 317          |\n",
            "|    time_elapsed         | 2347         |\n",
            "|    total_timesteps      | 1298432      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018919513 |\n",
            "|    clip_fraction        | 0.0474       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.432        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.15         |\n",
            "|    n_updates            | 6320         |\n",
            "|    policy_gradient_loss | -0.00396     |\n",
            "|    value_loss           | 9.16         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 20.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 318          |\n",
            "|    time_elapsed         | 2354         |\n",
            "|    total_timesteps      | 1302528      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016406926 |\n",
            "|    clip_fraction        | 0.0569       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.543        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.31         |\n",
            "|    n_updates            | 6340         |\n",
            "|    policy_gradient_loss | -0.00484     |\n",
            "|    value_loss           | 7.73         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 21.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 319          |\n",
            "|    time_elapsed         | 2361         |\n",
            "|    total_timesteps      | 1306624      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013034793 |\n",
            "|    clip_fraction        | 0.0286       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.505        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.23         |\n",
            "|    n_updates            | 6360         |\n",
            "|    policy_gradient_loss | -0.00272     |\n",
            "|    value_loss           | 9.65         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 21.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 320          |\n",
            "|    time_elapsed         | 2369         |\n",
            "|    total_timesteps      | 1310720      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014592437 |\n",
            "|    clip_fraction        | 0.0378       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.493        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.51         |\n",
            "|    n_updates            | 6380         |\n",
            "|    policy_gradient_loss | -0.00399     |\n",
            "|    value_loss           | 8.58         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 321          |\n",
            "|    time_elapsed         | 2376         |\n",
            "|    total_timesteps      | 1314816      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017393476 |\n",
            "|    clip_fraction        | 0.0462       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0.416        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.22         |\n",
            "|    n_updates            | 6400         |\n",
            "|    policy_gradient_loss | -0.00425     |\n",
            "|    value_loss           | 9.24         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 20.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 322          |\n",
            "|    time_elapsed         | 2384         |\n",
            "|    total_timesteps      | 1318912      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018728389 |\n",
            "|    clip_fraction        | 0.0519       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.514        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.23         |\n",
            "|    n_updates            | 6420         |\n",
            "|    policy_gradient_loss | -0.00409     |\n",
            "|    value_loss           | 7.9          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 20.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 323          |\n",
            "|    time_elapsed         | 2391         |\n",
            "|    total_timesteps      | 1323008      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015445559 |\n",
            "|    clip_fraction        | 0.0412       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.419        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.65         |\n",
            "|    n_updates            | 6440         |\n",
            "|    policy_gradient_loss | -0.00481     |\n",
            "|    value_loss           | 8.6          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 324          |\n",
            "|    time_elapsed         | 2399         |\n",
            "|    total_timesteps      | 1327104      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014303462 |\n",
            "|    clip_fraction        | 0.036        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.501        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.92         |\n",
            "|    n_updates            | 6460         |\n",
            "|    policy_gradient_loss | -0.00262     |\n",
            "|    value_loss           | 7.55         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 21.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 325          |\n",
            "|    time_elapsed         | 2406         |\n",
            "|    total_timesteps      | 1331200      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017303908 |\n",
            "|    clip_fraction        | 0.0635       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.535        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.46         |\n",
            "|    n_updates            | 6480         |\n",
            "|    policy_gradient_loss | -0.0036      |\n",
            "|    value_loss           | 7.67         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 20.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 326          |\n",
            "|    time_elapsed         | 2414         |\n",
            "|    total_timesteps      | 1335296      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017440654 |\n",
            "|    clip_fraction        | 0.05         |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.477        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.45         |\n",
            "|    n_updates            | 6500         |\n",
            "|    policy_gradient_loss | -0.00456     |\n",
            "|    value_loss           | 8.28         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 137          |\n",
            "|    ep_rew_mean          | 19.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 327          |\n",
            "|    time_elapsed         | 2421         |\n",
            "|    total_timesteps      | 1339392      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018355739 |\n",
            "|    clip_fraction        | 0.0503       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.54         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.45         |\n",
            "|    n_updates            | 6520         |\n",
            "|    policy_gradient_loss | -0.00545     |\n",
            "|    value_loss           | 6.34         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 142          |\n",
            "|    ep_rew_mean          | 21.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 328          |\n",
            "|    time_elapsed         | 2428         |\n",
            "|    total_timesteps      | 1343488      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016330048 |\n",
            "|    clip_fraction        | 0.0297       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.515        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.13         |\n",
            "|    n_updates            | 6540         |\n",
            "|    policy_gradient_loss | -0.00378     |\n",
            "|    value_loss           | 9.09         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 138          |\n",
            "|    ep_rew_mean          | 21.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 329          |\n",
            "|    time_elapsed         | 2436         |\n",
            "|    total_timesteps      | 1347584      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017812492 |\n",
            "|    clip_fraction        | 0.0572       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.507        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.36         |\n",
            "|    n_updates            | 6560         |\n",
            "|    policy_gradient_loss | -0.00372     |\n",
            "|    value_loss           | 7.89         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 136          |\n",
            "|    ep_rew_mean          | 23.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 330          |\n",
            "|    time_elapsed         | 2443         |\n",
            "|    total_timesteps      | 1351680      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015932978 |\n",
            "|    clip_fraction        | 0.0612       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.408        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.93         |\n",
            "|    n_updates            | 6580         |\n",
            "|    policy_gradient_loss | -0.00539     |\n",
            "|    value_loss           | 9.48         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 22.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 331          |\n",
            "|    time_elapsed         | 2450         |\n",
            "|    total_timesteps      | 1355776      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015747346 |\n",
            "|    clip_fraction        | 0.0403       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.292        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.83         |\n",
            "|    n_updates            | 6600         |\n",
            "|    policy_gradient_loss | -0.00402     |\n",
            "|    value_loss           | 9.84         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 136          |\n",
            "|    ep_rew_mean          | 23.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 332          |\n",
            "|    time_elapsed         | 2458         |\n",
            "|    total_timesteps      | 1359872      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017070856 |\n",
            "|    clip_fraction        | 0.0575       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.357        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.67         |\n",
            "|    n_updates            | 6620         |\n",
            "|    policy_gradient_loss | -0.00501     |\n",
            "|    value_loss           | 9.35         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 21.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 333          |\n",
            "|    time_elapsed         | 2465         |\n",
            "|    total_timesteps      | 1363968      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017167402 |\n",
            "|    clip_fraction        | 0.0641       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.32         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.84         |\n",
            "|    n_updates            | 6640         |\n",
            "|    policy_gradient_loss | -0.00444     |\n",
            "|    value_loss           | 9.12         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 21.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 334          |\n",
            "|    time_elapsed         | 2473         |\n",
            "|    total_timesteps      | 1368064      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015434248 |\n",
            "|    clip_fraction        | 0.0471       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.4          |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.63         |\n",
            "|    n_updates            | 6660         |\n",
            "|    policy_gradient_loss | -0.0046      |\n",
            "|    value_loss           | 9.03         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 20.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 335          |\n",
            "|    time_elapsed         | 2480         |\n",
            "|    total_timesteps      | 1372160      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013944653 |\n",
            "|    clip_fraction        | 0.0364       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.462        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.28         |\n",
            "|    n_updates            | 6680         |\n",
            "|    policy_gradient_loss | -0.00412     |\n",
            "|    value_loss           | 8.98         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 133         |\n",
            "|    ep_rew_mean          | 20.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 336         |\n",
            "|    time_elapsed         | 2488        |\n",
            "|    total_timesteps      | 1376256     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002258771 |\n",
            "|    clip_fraction        | 0.0876      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.517       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.78        |\n",
            "|    n_updates            | 6700        |\n",
            "|    policy_gradient_loss | -0.00532    |\n",
            "|    value_loss           | 8.54        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 21           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 337          |\n",
            "|    time_elapsed         | 2495         |\n",
            "|    total_timesteps      | 1380352      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015686385 |\n",
            "|    clip_fraction        | 0.0566       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.542        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.92         |\n",
            "|    n_updates            | 6720         |\n",
            "|    policy_gradient_loss | -0.0042      |\n",
            "|    value_loss           | 7.84         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 19.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 338          |\n",
            "|    time_elapsed         | 2502         |\n",
            "|    total_timesteps      | 1384448      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016530959 |\n",
            "|    clip_fraction        | 0.058        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.501        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.02         |\n",
            "|    n_updates            | 6740         |\n",
            "|    policy_gradient_loss | -0.00468     |\n",
            "|    value_loss           | 9.28         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 20.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 339          |\n",
            "|    time_elapsed         | 2510         |\n",
            "|    total_timesteps      | 1388544      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019527421 |\n",
            "|    clip_fraction        | 0.0675       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.51         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.17         |\n",
            "|    n_updates            | 6760         |\n",
            "|    policy_gradient_loss | -0.0039      |\n",
            "|    value_loss           | 7.81         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 123         |\n",
            "|    ep_rew_mean          | 18.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 340         |\n",
            "|    time_elapsed         | 2517        |\n",
            "|    total_timesteps      | 1392640     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001577346 |\n",
            "|    clip_fraction        | 0.0425      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.477       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.72        |\n",
            "|    n_updates            | 6780        |\n",
            "|    policy_gradient_loss | -0.00411    |\n",
            "|    value_loss           | 7.73        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 19.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 341          |\n",
            "|    time_elapsed         | 2525         |\n",
            "|    total_timesteps      | 1396736      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015022769 |\n",
            "|    clip_fraction        | 0.035        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.48         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.25         |\n",
            "|    n_updates            | 6800         |\n",
            "|    policy_gradient_loss | -0.00487     |\n",
            "|    value_loss           | 7.56         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 123         |\n",
            "|    ep_rew_mean          | 18.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 342         |\n",
            "|    time_elapsed         | 2532        |\n",
            "|    total_timesteps      | 1400832     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001673836 |\n",
            "|    clip_fraction        | 0.0642      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.447       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.18        |\n",
            "|    n_updates            | 6820        |\n",
            "|    policy_gradient_loss | -0.00376    |\n",
            "|    value_loss           | 7.95        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 19.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 343          |\n",
            "|    time_elapsed         | 2540         |\n",
            "|    total_timesteps      | 1404928      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020260666 |\n",
            "|    clip_fraction        | 0.0642       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.372        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.88         |\n",
            "|    n_updates            | 6840         |\n",
            "|    policy_gradient_loss | -0.00504     |\n",
            "|    value_loss           | 10.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 344          |\n",
            "|    time_elapsed         | 2547         |\n",
            "|    total_timesteps      | 1409024      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020515113 |\n",
            "|    clip_fraction        | 0.0751       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.349        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.97         |\n",
            "|    n_updates            | 6860         |\n",
            "|    policy_gradient_loss | -0.00517     |\n",
            "|    value_loss           | 9.68         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 20.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 345          |\n",
            "|    time_elapsed         | 2555         |\n",
            "|    total_timesteps      | 1413120      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015522493 |\n",
            "|    clip_fraction        | 0.0584       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.261        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.96         |\n",
            "|    n_updates            | 6880         |\n",
            "|    policy_gradient_loss | -0.00392     |\n",
            "|    value_loss           | 8.87         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 20.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 346          |\n",
            "|    time_elapsed         | 2562         |\n",
            "|    total_timesteps      | 1417216      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015068279 |\n",
            "|    clip_fraction        | 0.0309       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.383        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.25         |\n",
            "|    n_updates            | 6900         |\n",
            "|    policy_gradient_loss | -0.00417     |\n",
            "|    value_loss           | 8.42         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 20.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 347          |\n",
            "|    time_elapsed         | 2569         |\n",
            "|    total_timesteps      | 1421312      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016603535 |\n",
            "|    clip_fraction        | 0.0565       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.386        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.45         |\n",
            "|    n_updates            | 6920         |\n",
            "|    policy_gradient_loss | -0.00456     |\n",
            "|    value_loss           | 9.51         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 126         |\n",
            "|    ep_rew_mean          | 19.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 348         |\n",
            "|    time_elapsed         | 2577        |\n",
            "|    total_timesteps      | 1425408     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001789191 |\n",
            "|    clip_fraction        | 0.0519      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.465       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.07        |\n",
            "|    n_updates            | 6940        |\n",
            "|    policy_gradient_loss | -0.00461    |\n",
            "|    value_loss           | 7.39        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 17.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 349          |\n",
            "|    time_elapsed         | 2584         |\n",
            "|    total_timesteps      | 1429504      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014519612 |\n",
            "|    clip_fraction        | 0.0354       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.449        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.49         |\n",
            "|    n_updates            | 6960         |\n",
            "|    policy_gradient_loss | -0.00307     |\n",
            "|    value_loss           | 8.96         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 19.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 350          |\n",
            "|    time_elapsed         | 2592         |\n",
            "|    total_timesteps      | 1433600      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014309515 |\n",
            "|    clip_fraction        | 0.0374       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.578        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.47         |\n",
            "|    n_updates            | 6980         |\n",
            "|    policy_gradient_loss | -0.00441     |\n",
            "|    value_loss           | 6.63         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 136          |\n",
            "|    ep_rew_mean          | 22.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 351          |\n",
            "|    time_elapsed         | 2599         |\n",
            "|    total_timesteps      | 1437696      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018023987 |\n",
            "|    clip_fraction        | 0.0404       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.331        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.49         |\n",
            "|    n_updates            | 7000         |\n",
            "|    policy_gradient_loss | -0.00304     |\n",
            "|    value_loss           | 10.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 140          |\n",
            "|    ep_rew_mean          | 23.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 352          |\n",
            "|    time_elapsed         | 2606         |\n",
            "|    total_timesteps      | 1441792      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013013915 |\n",
            "|    clip_fraction        | 0.0244       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.342        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.19         |\n",
            "|    n_updates            | 7020         |\n",
            "|    policy_gradient_loss | -0.00359     |\n",
            "|    value_loss           | 8.52         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 138          |\n",
            "|    ep_rew_mean          | 22.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 353          |\n",
            "|    time_elapsed         | 2614         |\n",
            "|    total_timesteps      | 1445888      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020449192 |\n",
            "|    clip_fraction        | 0.0729       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.319        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.99         |\n",
            "|    n_updates            | 7040         |\n",
            "|    policy_gradient_loss | -0.00532     |\n",
            "|    value_loss           | 8.59         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 20.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 354          |\n",
            "|    time_elapsed         | 2621         |\n",
            "|    total_timesteps      | 1449984      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016633094 |\n",
            "|    clip_fraction        | 0.0439       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.281        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.06         |\n",
            "|    n_updates            | 7060         |\n",
            "|    policy_gradient_loss | -0.00318     |\n",
            "|    value_loss           | 9.07         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 19.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 355          |\n",
            "|    time_elapsed         | 2629         |\n",
            "|    total_timesteps      | 1454080      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015481452 |\n",
            "|    clip_fraction        | 0.0357       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.323        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.6          |\n",
            "|    n_updates            | 7080         |\n",
            "|    policy_gradient_loss | -0.00424     |\n",
            "|    value_loss           | 9.34         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 20.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 356          |\n",
            "|    time_elapsed         | 2636         |\n",
            "|    total_timesteps      | 1458176      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019392403 |\n",
            "|    clip_fraction        | 0.075        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.463        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.45         |\n",
            "|    n_updates            | 7100         |\n",
            "|    policy_gradient_loss | -0.00586     |\n",
            "|    value_loss           | 8.41         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 19.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 357          |\n",
            "|    time_elapsed         | 2644         |\n",
            "|    total_timesteps      | 1462272      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014392801 |\n",
            "|    clip_fraction        | 0.0347       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.422        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.12         |\n",
            "|    n_updates            | 7120         |\n",
            "|    policy_gradient_loss | -0.00458     |\n",
            "|    value_loss           | 8.41         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 20.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 358          |\n",
            "|    time_elapsed         | 2651         |\n",
            "|    total_timesteps      | 1466368      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017605706 |\n",
            "|    clip_fraction        | 0.0472       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.501        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.41         |\n",
            "|    n_updates            | 7140         |\n",
            "|    policy_gradient_loss | -0.00509     |\n",
            "|    value_loss           | 7.78         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 20.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 359          |\n",
            "|    time_elapsed         | 2658         |\n",
            "|    total_timesteps      | 1470464      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019245084 |\n",
            "|    clip_fraction        | 0.065        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.412        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.85         |\n",
            "|    n_updates            | 7160         |\n",
            "|    policy_gradient_loss | -0.00504     |\n",
            "|    value_loss           | 7.99         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 133         |\n",
            "|    ep_rew_mean          | 22.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 360         |\n",
            "|    time_elapsed         | 2666        |\n",
            "|    total_timesteps      | 1474560     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001653799 |\n",
            "|    clip_fraction        | 0.051       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.371       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 4.27        |\n",
            "|    n_updates            | 7180        |\n",
            "|    policy_gradient_loss | -0.00392    |\n",
            "|    value_loss           | 9.59        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 361          |\n",
            "|    time_elapsed         | 2673         |\n",
            "|    total_timesteps      | 1478656      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017513405 |\n",
            "|    clip_fraction        | 0.057        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.411        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.04         |\n",
            "|    n_updates            | 7200         |\n",
            "|    policy_gradient_loss | -0.00444     |\n",
            "|    value_loss           | 8.05         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 362          |\n",
            "|    time_elapsed         | 2681         |\n",
            "|    total_timesteps      | 1482752      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017791578 |\n",
            "|    clip_fraction        | 0.0607       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.442        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.64         |\n",
            "|    n_updates            | 7220         |\n",
            "|    policy_gradient_loss | -0.00438     |\n",
            "|    value_loss           | 9.55         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 20.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 363          |\n",
            "|    time_elapsed         | 2688         |\n",
            "|    total_timesteps      | 1486848      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019428643 |\n",
            "|    clip_fraction        | 0.0641       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.534        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.8          |\n",
            "|    n_updates            | 7240         |\n",
            "|    policy_gradient_loss | -0.00564     |\n",
            "|    value_loss           | 6.72         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 19.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 364          |\n",
            "|    time_elapsed         | 2695         |\n",
            "|    total_timesteps      | 1490944      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014942699 |\n",
            "|    clip_fraction        | 0.0339       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.544        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.25         |\n",
            "|    n_updates            | 7260         |\n",
            "|    policy_gradient_loss | -0.00447     |\n",
            "|    value_loss           | 8.63         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 20.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 365          |\n",
            "|    time_elapsed         | 2703         |\n",
            "|    total_timesteps      | 1495040      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015388659 |\n",
            "|    clip_fraction        | 0.0436       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.468        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.85         |\n",
            "|    n_updates            | 7280         |\n",
            "|    policy_gradient_loss | -0.00374     |\n",
            "|    value_loss           | 9.84         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 20.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 366          |\n",
            "|    time_elapsed         | 2710         |\n",
            "|    total_timesteps      | 1499136      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017851427 |\n",
            "|    clip_fraction        | 0.0632       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.488        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.99         |\n",
            "|    n_updates            | 7300         |\n",
            "|    policy_gradient_loss | -0.00564     |\n",
            "|    value_loss           | 7.44         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 20.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 367          |\n",
            "|    time_elapsed         | 2717         |\n",
            "|    total_timesteps      | 1503232      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014744552 |\n",
            "|    clip_fraction        | 0.0332       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.465        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.11         |\n",
            "|    n_updates            | 7320         |\n",
            "|    policy_gradient_loss | -0.00396     |\n",
            "|    value_loss           | 7.28         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 137         |\n",
            "|    ep_rew_mean          | 21.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 368         |\n",
            "|    time_elapsed         | 2725        |\n",
            "|    total_timesteps      | 1507328     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001460901 |\n",
            "|    clip_fraction        | 0.0396      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.507       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.19        |\n",
            "|    n_updates            | 7340        |\n",
            "|    policy_gradient_loss | -0.00362    |\n",
            "|    value_loss           | 7           |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 20           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 369          |\n",
            "|    time_elapsed         | 2732         |\n",
            "|    total_timesteps      | 1511424      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014692035 |\n",
            "|    clip_fraction        | 0.0373       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.486        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.48         |\n",
            "|    n_updates            | 7360         |\n",
            "|    policy_gradient_loss | -0.00373     |\n",
            "|    value_loss           | 8.91         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 19.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 370          |\n",
            "|    time_elapsed         | 2739         |\n",
            "|    total_timesteps      | 1515520      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016011497 |\n",
            "|    clip_fraction        | 0.0365       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.55         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.63         |\n",
            "|    n_updates            | 7380         |\n",
            "|    policy_gradient_loss | -0.0036      |\n",
            "|    value_loss           | 7.93         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 19.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 371          |\n",
            "|    time_elapsed         | 2747         |\n",
            "|    total_timesteps      | 1519616      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018257301 |\n",
            "|    clip_fraction        | 0.0616       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.498        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.81         |\n",
            "|    n_updates            | 7400         |\n",
            "|    policy_gradient_loss | -0.00433     |\n",
            "|    value_loss           | 8.66         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 19           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 372          |\n",
            "|    time_elapsed         | 2754         |\n",
            "|    total_timesteps      | 1523712      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019131026 |\n",
            "|    clip_fraction        | 0.0753       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.434        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.29         |\n",
            "|    n_updates            | 7420         |\n",
            "|    policy_gradient_loss | -0.0051      |\n",
            "|    value_loss           | 8.17         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 373          |\n",
            "|    time_elapsed         | 2762         |\n",
            "|    total_timesteps      | 1527808      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016423634 |\n",
            "|    clip_fraction        | 0.0527       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.469        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.15         |\n",
            "|    n_updates            | 7440         |\n",
            "|    policy_gradient_loss | -0.00539     |\n",
            "|    value_loss           | 7.16         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 19.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 374          |\n",
            "|    time_elapsed         | 2769         |\n",
            "|    total_timesteps      | 1531904      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014280515 |\n",
            "|    clip_fraction        | 0.0363       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.394        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.12         |\n",
            "|    n_updates            | 7460         |\n",
            "|    policy_gradient_loss | -0.00403     |\n",
            "|    value_loss           | 9.45         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 21.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 375          |\n",
            "|    time_elapsed         | 2777         |\n",
            "|    total_timesteps      | 1536000      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018493568 |\n",
            "|    clip_fraction        | 0.0725       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.455        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.71         |\n",
            "|    n_updates            | 7480         |\n",
            "|    policy_gradient_loss | -0.00413     |\n",
            "|    value_loss           | 7.18         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 20.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 376          |\n",
            "|    time_elapsed         | 2784         |\n",
            "|    total_timesteps      | 1540096      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014524697 |\n",
            "|    clip_fraction        | 0.0374       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.438        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.2          |\n",
            "|    n_updates            | 7500         |\n",
            "|    policy_gradient_loss | -0.00389     |\n",
            "|    value_loss           | 8.17         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 131         |\n",
            "|    ep_rew_mean          | 19.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 377         |\n",
            "|    time_elapsed         | 2791        |\n",
            "|    total_timesteps      | 1544192     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001727113 |\n",
            "|    clip_fraction        | 0.0482      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.484       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.53        |\n",
            "|    n_updates            | 7520        |\n",
            "|    policy_gradient_loss | -0.00403    |\n",
            "|    value_loss           | 7.82        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 19.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 378          |\n",
            "|    time_elapsed         | 2799         |\n",
            "|    total_timesteps      | 1548288      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017916623 |\n",
            "|    clip_fraction        | 0.0676       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.534        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.5          |\n",
            "|    n_updates            | 7540         |\n",
            "|    policy_gradient_loss | -0.00573     |\n",
            "|    value_loss           | 7.8          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 19.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 379          |\n",
            "|    time_elapsed         | 2806         |\n",
            "|    total_timesteps      | 1552384      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016559295 |\n",
            "|    clip_fraction        | 0.057        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.535        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.46         |\n",
            "|    n_updates            | 7560         |\n",
            "|    policy_gradient_loss | -0.00516     |\n",
            "|    value_loss           | 8.42         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 21           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 380          |\n",
            "|    time_elapsed         | 2813         |\n",
            "|    total_timesteps      | 1556480      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020879852 |\n",
            "|    clip_fraction        | 0.0707       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.495        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.54         |\n",
            "|    n_updates            | 7580         |\n",
            "|    policy_gradient_loss | -0.00536     |\n",
            "|    value_loss           | 8.07         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 127         |\n",
            "|    ep_rew_mean          | 20.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 381         |\n",
            "|    time_elapsed         | 2821        |\n",
            "|    total_timesteps      | 1560576     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001454019 |\n",
            "|    clip_fraction        | 0.0262      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.483       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.15        |\n",
            "|    n_updates            | 7600        |\n",
            "|    policy_gradient_loss | -0.00331    |\n",
            "|    value_loss           | 7.75        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 382          |\n",
            "|    time_elapsed         | 2828         |\n",
            "|    total_timesteps      | 1564672      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018401406 |\n",
            "|    clip_fraction        | 0.057        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.561        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.56         |\n",
            "|    n_updates            | 7620         |\n",
            "|    policy_gradient_loss | -0.00323     |\n",
            "|    value_loss           | 6.43         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 19           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 383          |\n",
            "|    time_elapsed         | 2836         |\n",
            "|    total_timesteps      | 1568768      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013674808 |\n",
            "|    clip_fraction        | 0.0346       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.501        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.74         |\n",
            "|    n_updates            | 7640         |\n",
            "|    policy_gradient_loss | -0.00344     |\n",
            "|    value_loss           | 8.96         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 19.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 384          |\n",
            "|    time_elapsed         | 2843         |\n",
            "|    total_timesteps      | 1572864      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016632425 |\n",
            "|    clip_fraction        | 0.0391       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.578        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.38         |\n",
            "|    n_updates            | 7660         |\n",
            "|    policy_gradient_loss | -0.00436     |\n",
            "|    value_loss           | 6.14         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 130         |\n",
            "|    ep_rew_mean          | 19.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 385         |\n",
            "|    time_elapsed         | 2850        |\n",
            "|    total_timesteps      | 1576960     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001785344 |\n",
            "|    clip_fraction        | 0.0519      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.602       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.22        |\n",
            "|    n_updates            | 7680        |\n",
            "|    policy_gradient_loss | -0.0038     |\n",
            "|    value_loss           | 7.53        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 18.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 386          |\n",
            "|    time_elapsed         | 2858         |\n",
            "|    total_timesteps      | 1581056      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014710586 |\n",
            "|    clip_fraction        | 0.0521       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.559        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.13         |\n",
            "|    n_updates            | 7700         |\n",
            "|    policy_gradient_loss | -0.00524     |\n",
            "|    value_loss           | 7.94         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 18.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 387          |\n",
            "|    time_elapsed         | 2865         |\n",
            "|    total_timesteps      | 1585152      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016973563 |\n",
            "|    clip_fraction        | 0.0564       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.574        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.74         |\n",
            "|    n_updates            | 7720         |\n",
            "|    policy_gradient_loss | -0.00515     |\n",
            "|    value_loss           | 6.18         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 18.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 388          |\n",
            "|    time_elapsed         | 2873         |\n",
            "|    total_timesteps      | 1589248      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016433463 |\n",
            "|    clip_fraction        | 0.0427       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.55         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.46         |\n",
            "|    n_updates            | 7740         |\n",
            "|    policy_gradient_loss | -0.00403     |\n",
            "|    value_loss           | 7.59         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 19.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 389          |\n",
            "|    time_elapsed         | 2880         |\n",
            "|    total_timesteps      | 1593344      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017531198 |\n",
            "|    clip_fraction        | 0.0527       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.51         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.9          |\n",
            "|    n_updates            | 7760         |\n",
            "|    policy_gradient_loss | -0.00409     |\n",
            "|    value_loss           | 7.29         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 390          |\n",
            "|    time_elapsed         | 2887         |\n",
            "|    total_timesteps      | 1597440      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014443075 |\n",
            "|    clip_fraction        | 0.0486       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.466        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.98         |\n",
            "|    n_updates            | 7780         |\n",
            "|    policy_gradient_loss | -0.00395     |\n",
            "|    value_loss           | 7.48         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 20.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 391          |\n",
            "|    time_elapsed         | 2895         |\n",
            "|    total_timesteps      | 1601536      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014707593 |\n",
            "|    clip_fraction        | 0.0411       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.572        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.74         |\n",
            "|    n_updates            | 7800         |\n",
            "|    policy_gradient_loss | -0.00458     |\n",
            "|    value_loss           | 6.16         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 20.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 392          |\n",
            "|    time_elapsed         | 2902         |\n",
            "|    total_timesteps      | 1605632      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016779139 |\n",
            "|    clip_fraction        | 0.0498       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.528        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.56         |\n",
            "|    n_updates            | 7820         |\n",
            "|    policy_gradient_loss | -0.00445     |\n",
            "|    value_loss           | 7.46         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 393          |\n",
            "|    time_elapsed         | 2910         |\n",
            "|    total_timesteps      | 1609728      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013665988 |\n",
            "|    clip_fraction        | 0.0343       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.304        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.79         |\n",
            "|    n_updates            | 7840         |\n",
            "|    policy_gradient_loss | -0.00304     |\n",
            "|    value_loss           | 9.8          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 22.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 394          |\n",
            "|    time_elapsed         | 2917         |\n",
            "|    total_timesteps      | 1613824      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018818281 |\n",
            "|    clip_fraction        | 0.0632       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.28         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.53         |\n",
            "|    n_updates            | 7860         |\n",
            "|    policy_gradient_loss | -0.00627     |\n",
            "|    value_loss           | 8.97         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 22           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 395          |\n",
            "|    time_elapsed         | 2925         |\n",
            "|    total_timesteps      | 1617920      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020225546 |\n",
            "|    clip_fraction        | 0.0668       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.358        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.56         |\n",
            "|    n_updates            | 7880         |\n",
            "|    policy_gradient_loss | -0.00485     |\n",
            "|    value_loss           | 9.79         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 22.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 396          |\n",
            "|    time_elapsed         | 2932         |\n",
            "|    total_timesteps      | 1622016      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015201925 |\n",
            "|    clip_fraction        | 0.0457       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.409        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.99         |\n",
            "|    n_updates            | 7900         |\n",
            "|    policy_gradient_loss | -0.00458     |\n",
            "|    value_loss           | 7.39         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 21.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 397          |\n",
            "|    time_elapsed         | 2939         |\n",
            "|    total_timesteps      | 1626112      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018255205 |\n",
            "|    clip_fraction        | 0.0676       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.428        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.74         |\n",
            "|    n_updates            | 7920         |\n",
            "|    policy_gradient_loss | -0.00483     |\n",
            "|    value_loss           | 8.93         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 398          |\n",
            "|    time_elapsed         | 2947         |\n",
            "|    total_timesteps      | 1630208      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015297593 |\n",
            "|    clip_fraction        | 0.0363       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.432        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.32         |\n",
            "|    n_updates            | 7940         |\n",
            "|    policy_gradient_loss | -0.0031      |\n",
            "|    value_loss           | 7.73         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 135          |\n",
            "|    ep_rew_mean          | 21.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 399          |\n",
            "|    time_elapsed         | 2954         |\n",
            "|    total_timesteps      | 1634304      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012632319 |\n",
            "|    clip_fraction        | 0.0368       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.539        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.21         |\n",
            "|    n_updates            | 7960         |\n",
            "|    policy_gradient_loss | -0.00394     |\n",
            "|    value_loss           | 7.79         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 21.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 400          |\n",
            "|    time_elapsed         | 2962         |\n",
            "|    total_timesteps      | 1638400      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014650438 |\n",
            "|    clip_fraction        | 0.0327       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.471        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.46         |\n",
            "|    n_updates            | 7980         |\n",
            "|    policy_gradient_loss | -0.00273     |\n",
            "|    value_loss           | 8.92         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 401          |\n",
            "|    time_elapsed         | 2969         |\n",
            "|    total_timesteps      | 1642496      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016600941 |\n",
            "|    clip_fraction        | 0.051        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.474        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.53         |\n",
            "|    n_updates            | 8000         |\n",
            "|    policy_gradient_loss | -0.0043      |\n",
            "|    value_loss           | 8.87         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 18.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 402          |\n",
            "|    time_elapsed         | 2976         |\n",
            "|    total_timesteps      | 1646592      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016095438 |\n",
            "|    clip_fraction        | 0.0453       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.548        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.85         |\n",
            "|    n_updates            | 8020         |\n",
            "|    policy_gradient_loss | -0.00446     |\n",
            "|    value_loss           | 6.81         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 18.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 403          |\n",
            "|    time_elapsed         | 2984         |\n",
            "|    total_timesteps      | 1650688      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018727991 |\n",
            "|    clip_fraction        | 0.0566       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.575        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.3          |\n",
            "|    n_updates            | 8040         |\n",
            "|    policy_gradient_loss | -0.0046      |\n",
            "|    value_loss           | 7.55         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 18.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 404          |\n",
            "|    time_elapsed         | 2991         |\n",
            "|    total_timesteps      | 1654784      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016273423 |\n",
            "|    clip_fraction        | 0.0532       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.527        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.33         |\n",
            "|    n_updates            | 8060         |\n",
            "|    policy_gradient_loss | -0.00461     |\n",
            "|    value_loss           | 7.45         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 133         |\n",
            "|    ep_rew_mean          | 18.5        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 405         |\n",
            "|    time_elapsed         | 2999        |\n",
            "|    total_timesteps      | 1658880     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001730248 |\n",
            "|    clip_fraction        | 0.0542      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.477       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.71        |\n",
            "|    n_updates            | 8080        |\n",
            "|    policy_gradient_loss | -0.00464    |\n",
            "|    value_loss           | 8.03        |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 138         |\n",
            "|    ep_rew_mean          | 20.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 406         |\n",
            "|    time_elapsed         | 3006        |\n",
            "|    total_timesteps      | 1662976     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001513234 |\n",
            "|    clip_fraction        | 0.0454      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.507       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.44        |\n",
            "|    n_updates            | 8100        |\n",
            "|    policy_gradient_loss | -0.00467    |\n",
            "|    value_loss           | 7.81        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 136          |\n",
            "|    ep_rew_mean          | 20.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 407          |\n",
            "|    time_elapsed         | 3013         |\n",
            "|    total_timesteps      | 1667072      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014887201 |\n",
            "|    clip_fraction        | 0.0457       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.417        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.71         |\n",
            "|    n_updates            | 8120         |\n",
            "|    policy_gradient_loss | -0.00394     |\n",
            "|    value_loss           | 8.28         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 137          |\n",
            "|    ep_rew_mean          | 22.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 408          |\n",
            "|    time_elapsed         | 3021         |\n",
            "|    total_timesteps      | 1671168      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016132019 |\n",
            "|    clip_fraction        | 0.0332       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.525        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.96         |\n",
            "|    n_updates            | 8140         |\n",
            "|    policy_gradient_loss | -0.00289     |\n",
            "|    value_loss           | 6.42         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 20.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 409          |\n",
            "|    time_elapsed         | 3028         |\n",
            "|    total_timesteps      | 1675264      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014868494 |\n",
            "|    clip_fraction        | 0.0349       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.403        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.77         |\n",
            "|    n_updates            | 8160         |\n",
            "|    policy_gradient_loss | -0.00345     |\n",
            "|    value_loss           | 8.56         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 20.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 410          |\n",
            "|    time_elapsed         | 3036         |\n",
            "|    total_timesteps      | 1679360      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016765607 |\n",
            "|    clip_fraction        | 0.0476       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.465        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.94         |\n",
            "|    n_updates            | 8180         |\n",
            "|    policy_gradient_loss | -0.00392     |\n",
            "|    value_loss           | 7.7          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 19.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 411          |\n",
            "|    time_elapsed         | 3043         |\n",
            "|    total_timesteps      | 1683456      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017570814 |\n",
            "|    clip_fraction        | 0.0582       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.414        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.32         |\n",
            "|    n_updates            | 8200         |\n",
            "|    policy_gradient_loss | -0.00551     |\n",
            "|    value_loss           | 8.2          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 412          |\n",
            "|    time_elapsed         | 3051         |\n",
            "|    total_timesteps      | 1687552      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016246665 |\n",
            "|    clip_fraction        | 0.0487       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.451        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.42         |\n",
            "|    n_updates            | 8220         |\n",
            "|    policy_gradient_loss | -0.00434     |\n",
            "|    value_loss           | 6.94         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 20.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 413          |\n",
            "|    time_elapsed         | 3058         |\n",
            "|    total_timesteps      | 1691648      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019327991 |\n",
            "|    clip_fraction        | 0.0585       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.403        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.4          |\n",
            "|    n_updates            | 8240         |\n",
            "|    policy_gradient_loss | -0.00493     |\n",
            "|    value_loss           | 8.64         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 21.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 414          |\n",
            "|    time_elapsed         | 3065         |\n",
            "|    total_timesteps      | 1695744      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015731899 |\n",
            "|    clip_fraction        | 0.0504       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.523        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.5          |\n",
            "|    n_updates            | 8260         |\n",
            "|    policy_gradient_loss | -0.0039      |\n",
            "|    value_loss           | 6.76         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 20.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 415          |\n",
            "|    time_elapsed         | 3073         |\n",
            "|    total_timesteps      | 1699840      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016356332 |\n",
            "|    clip_fraction        | 0.0506       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.528        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.1          |\n",
            "|    n_updates            | 8280         |\n",
            "|    policy_gradient_loss | -0.00478     |\n",
            "|    value_loss           | 7.44         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 129         |\n",
            "|    ep_rew_mean          | 20.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 416         |\n",
            "|    time_elapsed         | 3080        |\n",
            "|    total_timesteps      | 1703936     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001365937 |\n",
            "|    clip_fraction        | 0.0378      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.526       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.21        |\n",
            "|    n_updates            | 8300        |\n",
            "|    policy_gradient_loss | -0.00321    |\n",
            "|    value_loss           | 7.88        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 19.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 417          |\n",
            "|    time_elapsed         | 3087         |\n",
            "|    total_timesteps      | 1708032      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014415354 |\n",
            "|    clip_fraction        | 0.0541       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.341        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.51         |\n",
            "|    n_updates            | 8320         |\n",
            "|    policy_gradient_loss | -0.0048      |\n",
            "|    value_loss           | 11.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 18.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 418          |\n",
            "|    time_elapsed         | 3095         |\n",
            "|    total_timesteps      | 1712128      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018038601 |\n",
            "|    clip_fraction        | 0.051        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.473        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.49         |\n",
            "|    n_updates            | 8340         |\n",
            "|    policy_gradient_loss | -0.00441     |\n",
            "|    value_loss           | 6.97         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 18.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 419          |\n",
            "|    time_elapsed         | 3102         |\n",
            "|    total_timesteps      | 1716224      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019459103 |\n",
            "|    clip_fraction        | 0.0741       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.42         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.72         |\n",
            "|    n_updates            | 8360         |\n",
            "|    policy_gradient_loss | -0.00496     |\n",
            "|    value_loss           | 7.71         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 18.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 420          |\n",
            "|    time_elapsed         | 3109         |\n",
            "|    total_timesteps      | 1720320      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016433318 |\n",
            "|    clip_fraction        | 0.0474       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.476        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.55         |\n",
            "|    n_updates            | 8380         |\n",
            "|    policy_gradient_loss | -0.00426     |\n",
            "|    value_loss           | 6.53         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 128         |\n",
            "|    ep_rew_mean          | 19.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 421         |\n",
            "|    time_elapsed         | 3117        |\n",
            "|    total_timesteps      | 1724416     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001980411 |\n",
            "|    clip_fraction        | 0.0569      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.31       |\n",
            "|    explained_variance   | 0.435       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 2.61        |\n",
            "|    n_updates            | 8400        |\n",
            "|    policy_gradient_loss | -0.0043     |\n",
            "|    value_loss           | 6.79        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 20.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 422          |\n",
            "|    time_elapsed         | 3124         |\n",
            "|    total_timesteps      | 1728512      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018753342 |\n",
            "|    clip_fraction        | 0.0615       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.439        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.73         |\n",
            "|    n_updates            | 8420         |\n",
            "|    policy_gradient_loss | -0.00457     |\n",
            "|    value_loss           | 8.11         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 423          |\n",
            "|    time_elapsed         | 3132         |\n",
            "|    total_timesteps      | 1732608      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015877474 |\n",
            "|    clip_fraction        | 0.0616       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.387        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.39         |\n",
            "|    n_updates            | 8440         |\n",
            "|    policy_gradient_loss | -0.00474     |\n",
            "|    value_loss           | 8.04         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 21.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 424          |\n",
            "|    time_elapsed         | 3139         |\n",
            "|    total_timesteps      | 1736704      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014350028 |\n",
            "|    clip_fraction        | 0.0339       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.315        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.65         |\n",
            "|    n_updates            | 8460         |\n",
            "|    policy_gradient_loss | -0.00403     |\n",
            "|    value_loss           | 9.01         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 19.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 425          |\n",
            "|    time_elapsed         | 3146         |\n",
            "|    total_timesteps      | 1740800      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015858077 |\n",
            "|    clip_fraction        | 0.0464       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.525        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.4          |\n",
            "|    n_updates            | 8480         |\n",
            "|    policy_gradient_loss | -0.0046      |\n",
            "|    value_loss           | 6.31         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 21.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 426          |\n",
            "|    time_elapsed         | 3154         |\n",
            "|    total_timesteps      | 1744896      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018371707 |\n",
            "|    clip_fraction        | 0.062        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.439        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.93         |\n",
            "|    n_updates            | 8500         |\n",
            "|    policy_gradient_loss | -0.00519     |\n",
            "|    value_loss           | 8.55         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 20.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 427          |\n",
            "|    time_elapsed         | 3161         |\n",
            "|    total_timesteps      | 1748992      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014579825 |\n",
            "|    clip_fraction        | 0.0403       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.369        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.42         |\n",
            "|    n_updates            | 8520         |\n",
            "|    policy_gradient_loss | -0.00418     |\n",
            "|    value_loss           | 11           |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 129         |\n",
            "|    ep_rew_mean          | 21.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 428         |\n",
            "|    time_elapsed         | 3169        |\n",
            "|    total_timesteps      | 1753088     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001359668 |\n",
            "|    clip_fraction        | 0.0461      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.367       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.42        |\n",
            "|    n_updates            | 8540        |\n",
            "|    policy_gradient_loss | -0.00444    |\n",
            "|    value_loss           | 8.71        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 19           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 429          |\n",
            "|    time_elapsed         | 3176         |\n",
            "|    total_timesteps      | 1757184      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016879542 |\n",
            "|    clip_fraction        | 0.0436       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.318        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.59         |\n",
            "|    n_updates            | 8560         |\n",
            "|    policy_gradient_loss | -0.00447     |\n",
            "|    value_loss           | 8.41         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 122         |\n",
            "|    ep_rew_mean          | 19.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 430         |\n",
            "|    time_elapsed         | 3183        |\n",
            "|    total_timesteps      | 1761280     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001612234 |\n",
            "|    clip_fraction        | 0.0499      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.417       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.47        |\n",
            "|    n_updates            | 8580        |\n",
            "|    policy_gradient_loss | -0.00464    |\n",
            "|    value_loss           | 7.68        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 20.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 431          |\n",
            "|    time_elapsed         | 3191         |\n",
            "|    total_timesteps      | 1765376      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018498782 |\n",
            "|    clip_fraction        | 0.0538       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.425        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.71         |\n",
            "|    n_updates            | 8600         |\n",
            "|    policy_gradient_loss | -0.00545     |\n",
            "|    value_loss           | 7.99         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 21           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 432          |\n",
            "|    time_elapsed         | 3198         |\n",
            "|    total_timesteps      | 1769472      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015131726 |\n",
            "|    clip_fraction        | 0.0481       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.487        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.73         |\n",
            "|    n_updates            | 8620         |\n",
            "|    policy_gradient_loss | -0.00456     |\n",
            "|    value_loss           | 6.87         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 20.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 433          |\n",
            "|    time_elapsed         | 3205         |\n",
            "|    total_timesteps      | 1773568      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015566101 |\n",
            "|    clip_fraction        | 0.0416       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.447        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.45         |\n",
            "|    n_updates            | 8640         |\n",
            "|    policy_gradient_loss | -0.00363     |\n",
            "|    value_loss           | 8.78         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 20.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 434          |\n",
            "|    time_elapsed         | 3213         |\n",
            "|    total_timesteps      | 1777664      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018759607 |\n",
            "|    clip_fraction        | 0.0535       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.28        |\n",
            "|    explained_variance   | 0.542        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.7          |\n",
            "|    n_updates            | 8660         |\n",
            "|    policy_gradient_loss | -0.00599     |\n",
            "|    value_loss           | 7.32         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 435          |\n",
            "|    time_elapsed         | 3220         |\n",
            "|    total_timesteps      | 1781760      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019312619 |\n",
            "|    clip_fraction        | 0.0487       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.59         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.96         |\n",
            "|    n_updates            | 8680         |\n",
            "|    policy_gradient_loss | -0.00433     |\n",
            "|    value_loss           | 6.87         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 21.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 436          |\n",
            "|    time_elapsed         | 3227         |\n",
            "|    total_timesteps      | 1785856      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012737277 |\n",
            "|    clip_fraction        | 0.0335       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.554        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.62         |\n",
            "|    n_updates            | 8700         |\n",
            "|    policy_gradient_loss | -0.0025      |\n",
            "|    value_loss           | 8.61         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 23.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 437          |\n",
            "|    time_elapsed         | 3235         |\n",
            "|    total_timesteps      | 1789952      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020230475 |\n",
            "|    clip_fraction        | 0.0598       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.486        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.16         |\n",
            "|    n_updates            | 8720         |\n",
            "|    policy_gradient_loss | -0.00373     |\n",
            "|    value_loss           | 7.99         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 21.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 438          |\n",
            "|    time_elapsed         | 3242         |\n",
            "|    total_timesteps      | 1794048      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018715671 |\n",
            "|    clip_fraction        | 0.0634       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.56         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.57         |\n",
            "|    n_updates            | 8740         |\n",
            "|    policy_gradient_loss | -0.00454     |\n",
            "|    value_loss           | 8.23         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 21.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 439          |\n",
            "|    time_elapsed         | 3250         |\n",
            "|    total_timesteps      | 1798144      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018756452 |\n",
            "|    clip_fraction        | 0.0571       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.524        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.62         |\n",
            "|    n_updates            | 8760         |\n",
            "|    policy_gradient_loss | -0.00468     |\n",
            "|    value_loss           | 8.46         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 20.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 440          |\n",
            "|    time_elapsed         | 3257         |\n",
            "|    total_timesteps      | 1802240      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016313246 |\n",
            "|    clip_fraction        | 0.0478       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.438        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.36         |\n",
            "|    n_updates            | 8780         |\n",
            "|    policy_gradient_loss | -0.00445     |\n",
            "|    value_loss           | 8.1          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 21.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 441          |\n",
            "|    time_elapsed         | 3265         |\n",
            "|    total_timesteps      | 1806336      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012314102 |\n",
            "|    clip_fraction        | 0.0381       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.473        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.76         |\n",
            "|    n_updates            | 8800         |\n",
            "|    policy_gradient_loss | -0.00389     |\n",
            "|    value_loss           | 9.44         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 19.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 442          |\n",
            "|    time_elapsed         | 3272         |\n",
            "|    total_timesteps      | 1810432      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015815524 |\n",
            "|    clip_fraction        | 0.0387       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.435        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.98         |\n",
            "|    n_updates            | 8820         |\n",
            "|    policy_gradient_loss | -0.00413     |\n",
            "|    value_loss           | 9.3          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 20.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 443          |\n",
            "|    time_elapsed         | 3279         |\n",
            "|    total_timesteps      | 1814528      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013951871 |\n",
            "|    clip_fraction        | 0.0384       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.471        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.04         |\n",
            "|    n_updates            | 8840         |\n",
            "|    policy_gradient_loss | -0.00416     |\n",
            "|    value_loss           | 8.03         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 20.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 444          |\n",
            "|    time_elapsed         | 3287         |\n",
            "|    total_timesteps      | 1818624      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015182537 |\n",
            "|    clip_fraction        | 0.0326       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.435        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.19         |\n",
            "|    n_updates            | 8860         |\n",
            "|    policy_gradient_loss | -0.00321     |\n",
            "|    value_loss           | 8.86         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 20.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 445          |\n",
            "|    time_elapsed         | 3294         |\n",
            "|    total_timesteps      | 1822720      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014634305 |\n",
            "|    clip_fraction        | 0.0401       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.47         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.44         |\n",
            "|    n_updates            | 8880         |\n",
            "|    policy_gradient_loss | -0.00481     |\n",
            "|    value_loss           | 9.07         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 21.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 446          |\n",
            "|    time_elapsed         | 3301         |\n",
            "|    total_timesteps      | 1826816      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021123933 |\n",
            "|    clip_fraction        | 0.0648       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.516        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.38         |\n",
            "|    n_updates            | 8900         |\n",
            "|    policy_gradient_loss | -0.00501     |\n",
            "|    value_loss           | 6.72         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 21.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 447          |\n",
            "|    time_elapsed         | 3309         |\n",
            "|    total_timesteps      | 1830912      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017526101 |\n",
            "|    clip_fraction        | 0.0546       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.355        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.49         |\n",
            "|    n_updates            | 8920         |\n",
            "|    policy_gradient_loss | -0.0048      |\n",
            "|    value_loss           | 9.3          |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 448          |\n",
            "|    time_elapsed         | 3316         |\n",
            "|    total_timesteps      | 1835008      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018994047 |\n",
            "|    clip_fraction        | 0.0747       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.438        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.38         |\n",
            "|    n_updates            | 8940         |\n",
            "|    policy_gradient_loss | -0.00655     |\n",
            "|    value_loss           | 9.05         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 19.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 449          |\n",
            "|    time_elapsed         | 3324         |\n",
            "|    total_timesteps      | 1839104      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015281041 |\n",
            "|    clip_fraction        | 0.0424       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.514        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.12         |\n",
            "|    n_updates            | 8960         |\n",
            "|    policy_gradient_loss | -0.00498     |\n",
            "|    value_loss           | 7.59         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 19.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 450          |\n",
            "|    time_elapsed         | 3331         |\n",
            "|    total_timesteps      | 1843200      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016858052 |\n",
            "|    clip_fraction        | 0.0475       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.274        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.69         |\n",
            "|    n_updates            | 8980         |\n",
            "|    policy_gradient_loss | -0.00451     |\n",
            "|    value_loss           | 10.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 19.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 451          |\n",
            "|    time_elapsed         | 3338         |\n",
            "|    total_timesteps      | 1847296      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019695424 |\n",
            "|    clip_fraction        | 0.0475       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.405        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.24         |\n",
            "|    n_updates            | 9000         |\n",
            "|    policy_gradient_loss | -0.00556     |\n",
            "|    value_loss           | 7.79         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 20           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 452          |\n",
            "|    time_elapsed         | 3346         |\n",
            "|    total_timesteps      | 1851392      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016864019 |\n",
            "|    clip_fraction        | 0.0428       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.434        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.36         |\n",
            "|    n_updates            | 9020         |\n",
            "|    policy_gradient_loss | -0.0041      |\n",
            "|    value_loss           | 7.57         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 20           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 453          |\n",
            "|    time_elapsed         | 3353         |\n",
            "|    total_timesteps      | 1855488      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015783783 |\n",
            "|    clip_fraction        | 0.0681       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.572        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.94         |\n",
            "|    n_updates            | 9040         |\n",
            "|    policy_gradient_loss | -0.00422     |\n",
            "|    value_loss           | 7.43         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 454          |\n",
            "|    time_elapsed         | 3361         |\n",
            "|    total_timesteps      | 1859584      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018084827 |\n",
            "|    clip_fraction        | 0.0534       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.571        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.42         |\n",
            "|    n_updates            | 9060         |\n",
            "|    policy_gradient_loss | -0.00467     |\n",
            "|    value_loss           | 7.89         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 21.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 455          |\n",
            "|    time_elapsed         | 3368         |\n",
            "|    total_timesteps      | 1863680      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016908774 |\n",
            "|    clip_fraction        | 0.0526       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.434        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.36         |\n",
            "|    n_updates            | 9080         |\n",
            "|    policy_gradient_loss | -0.00414     |\n",
            "|    value_loss           | 8.37         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 19.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 456          |\n",
            "|    time_elapsed         | 3376         |\n",
            "|    total_timesteps      | 1867776      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014839567 |\n",
            "|    clip_fraction        | 0.0398       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.488        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.11         |\n",
            "|    n_updates            | 9100         |\n",
            "|    policy_gradient_loss | -0.0041      |\n",
            "|    value_loss           | 6.91         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 19.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 457          |\n",
            "|    time_elapsed         | 3383         |\n",
            "|    total_timesteps      | 1871872      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018418275 |\n",
            "|    clip_fraction        | 0.0536       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.522        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.03         |\n",
            "|    n_updates            | 9120         |\n",
            "|    policy_gradient_loss | -0.00465     |\n",
            "|    value_loss           | 6.97         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 19.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 458          |\n",
            "|    time_elapsed         | 3390         |\n",
            "|    total_timesteps      | 1875968      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015803365 |\n",
            "|    clip_fraction        | 0.0599       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.563        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3            |\n",
            "|    n_updates            | 9140         |\n",
            "|    policy_gradient_loss | -0.00481     |\n",
            "|    value_loss           | 6.74         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 21.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 459          |\n",
            "|    time_elapsed         | 3398         |\n",
            "|    total_timesteps      | 1880064      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018675387 |\n",
            "|    clip_fraction        | 0.0697       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.52         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.96         |\n",
            "|    n_updates            | 9160         |\n",
            "|    policy_gradient_loss | -0.00483     |\n",
            "|    value_loss           | 7.83         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 133         |\n",
            "|    ep_rew_mean          | 21.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 460         |\n",
            "|    time_elapsed         | 3405        |\n",
            "|    total_timesteps      | 1884160     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001499814 |\n",
            "|    clip_fraction        | 0.0475      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.31       |\n",
            "|    explained_variance   | 0.521       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 2.91        |\n",
            "|    n_updates            | 9180        |\n",
            "|    policy_gradient_loss | -0.00514    |\n",
            "|    value_loss           | 7.17        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 131          |\n",
            "|    ep_rew_mean          | 20.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 461          |\n",
            "|    time_elapsed         | 3413         |\n",
            "|    total_timesteps      | 1888256      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019364199 |\n",
            "|    clip_fraction        | 0.0659       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.419        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.9          |\n",
            "|    n_updates            | 9200         |\n",
            "|    policy_gradient_loss | -0.00492     |\n",
            "|    value_loss           | 9.09         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 19.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 462          |\n",
            "|    time_elapsed         | 3420         |\n",
            "|    total_timesteps      | 1892352      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014792056 |\n",
            "|    clip_fraction        | 0.0473       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.516        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.54         |\n",
            "|    n_updates            | 9220         |\n",
            "|    policy_gradient_loss | -0.00569     |\n",
            "|    value_loss           | 7.42         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 19.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 463          |\n",
            "|    time_elapsed         | 3427         |\n",
            "|    total_timesteps      | 1896448      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016482144 |\n",
            "|    clip_fraction        | 0.0403       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.427        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.39         |\n",
            "|    n_updates            | 9240         |\n",
            "|    policy_gradient_loss | -0.0047      |\n",
            "|    value_loss           | 9.42         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 20.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 464          |\n",
            "|    time_elapsed         | 3435         |\n",
            "|    total_timesteps      | 1900544      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019354722 |\n",
            "|    clip_fraction        | 0.0674       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.475        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.95         |\n",
            "|    n_updates            | 9260         |\n",
            "|    policy_gradient_loss | -0.00542     |\n",
            "|    value_loss           | 7.62         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 19.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 465          |\n",
            "|    time_elapsed         | 3442         |\n",
            "|    total_timesteps      | 1904640      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020145355 |\n",
            "|    clip_fraction        | 0.0798       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.368        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.94         |\n",
            "|    n_updates            | 9280         |\n",
            "|    policy_gradient_loss | -0.00519     |\n",
            "|    value_loss           | 9.05         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 133          |\n",
            "|    ep_rew_mean          | 20.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 466          |\n",
            "|    time_elapsed         | 3449         |\n",
            "|    total_timesteps      | 1908736      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019388867 |\n",
            "|    clip_fraction        | 0.0589       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.421        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.97         |\n",
            "|    n_updates            | 9300         |\n",
            "|    policy_gradient_loss | -0.00527     |\n",
            "|    value_loss           | 6.97         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 139          |\n",
            "|    ep_rew_mean          | 21.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 467          |\n",
            "|    time_elapsed         | 3457         |\n",
            "|    total_timesteps      | 1912832      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016071885 |\n",
            "|    clip_fraction        | 0.0391       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.476        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.02         |\n",
            "|    n_updates            | 9320         |\n",
            "|    policy_gradient_loss | -0.00408     |\n",
            "|    value_loss           | 6.97         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 137          |\n",
            "|    ep_rew_mean          | 21.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 468          |\n",
            "|    time_elapsed         | 3464         |\n",
            "|    total_timesteps      | 1916928      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0021254634 |\n",
            "|    clip_fraction        | 0.0961       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.438        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.78         |\n",
            "|    n_updates            | 9340         |\n",
            "|    policy_gradient_loss | -0.00607     |\n",
            "|    value_loss           | 8.58         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 140          |\n",
            "|    ep_rew_mean          | 22.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 469          |\n",
            "|    time_elapsed         | 3471         |\n",
            "|    total_timesteps      | 1921024      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015009079 |\n",
            "|    clip_fraction        | 0.0337       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.324        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.05         |\n",
            "|    n_updates            | 9360         |\n",
            "|    policy_gradient_loss | -0.00401     |\n",
            "|    value_loss           | 9.11         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 147          |\n",
            "|    ep_rew_mean          | 23.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 470          |\n",
            "|    time_elapsed         | 3479         |\n",
            "|    total_timesteps      | 1925120      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017235719 |\n",
            "|    clip_fraction        | 0.0487       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.28         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.1          |\n",
            "|    n_updates            | 9380         |\n",
            "|    policy_gradient_loss | -0.00444     |\n",
            "|    value_loss           | 8.84         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 134          |\n",
            "|    ep_rew_mean          | 21.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 471          |\n",
            "|    time_elapsed         | 3486         |\n",
            "|    total_timesteps      | 1929216      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012902614 |\n",
            "|    clip_fraction        | 0.0181       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.302        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.92         |\n",
            "|    n_updates            | 9400         |\n",
            "|    policy_gradient_loss | -0.00323     |\n",
            "|    value_loss           | 9.51         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 138          |\n",
            "|    ep_rew_mean          | 22.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 472          |\n",
            "|    time_elapsed         | 3493         |\n",
            "|    total_timesteps      | 1933312      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016161005 |\n",
            "|    clip_fraction        | 0.0501       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.31        |\n",
            "|    explained_variance   | 0.529        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 2.91         |\n",
            "|    n_updates            | 9420         |\n",
            "|    policy_gradient_loss | -0.00413     |\n",
            "|    value_loss           | 6.63         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 20.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 473          |\n",
            "|    time_elapsed         | 3501         |\n",
            "|    total_timesteps      | 1937408      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015325283 |\n",
            "|    clip_fraction        | 0.0458       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.44         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.77         |\n",
            "|    n_updates            | 9440         |\n",
            "|    policy_gradient_loss | -0.00488     |\n",
            "|    value_loss           | 10           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 20.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 474          |\n",
            "|    time_elapsed         | 3508         |\n",
            "|    total_timesteps      | 1941504      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016726999 |\n",
            "|    clip_fraction        | 0.0446       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.538        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.06         |\n",
            "|    n_updates            | 9460         |\n",
            "|    policy_gradient_loss | -0.00336     |\n",
            "|    value_loss           | 7.26         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 18.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 475          |\n",
            "|    time_elapsed         | 3516         |\n",
            "|    total_timesteps      | 1945600      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017708342 |\n",
            "|    clip_fraction        | 0.0619       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.551        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.03         |\n",
            "|    n_updates            | 9480         |\n",
            "|    policy_gradient_loss | -0.00641     |\n",
            "|    value_loss           | 7.72         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 127         |\n",
            "|    ep_rew_mean          | 19.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 476         |\n",
            "|    time_elapsed         | 3523        |\n",
            "|    total_timesteps      | 1949696     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001610996 |\n",
            "|    clip_fraction        | 0.0525      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.512       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 4.2         |\n",
            "|    n_updates            | 9500        |\n",
            "|    policy_gradient_loss | -0.00448    |\n",
            "|    value_loss           | 8.17        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 20.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 477          |\n",
            "|    time_elapsed         | 3530         |\n",
            "|    total_timesteps      | 1953792      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016764125 |\n",
            "|    clip_fraction        | 0.0489       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.544        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.3          |\n",
            "|    n_updates            | 9520         |\n",
            "|    policy_gradient_loss | -0.00435     |\n",
            "|    value_loss           | 8.18         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 131         |\n",
            "|    ep_rew_mean          | 20.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 478         |\n",
            "|    time_elapsed         | 3538        |\n",
            "|    total_timesteps      | 1957888     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002254674 |\n",
            "|    clip_fraction        | 0.0717      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.407       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.58        |\n",
            "|    n_updates            | 9540        |\n",
            "|    policy_gradient_loss | -0.00535    |\n",
            "|    value_loss           | 9.31        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 129          |\n",
            "|    ep_rew_mean          | 19.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 479          |\n",
            "|    time_elapsed         | 3545         |\n",
            "|    total_timesteps      | 1961984      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017368938 |\n",
            "|    clip_fraction        | 0.0503       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.544        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.24         |\n",
            "|    n_updates            | 9560         |\n",
            "|    policy_gradient_loss | -0.00502     |\n",
            "|    value_loss           | 6.83         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 19.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 480          |\n",
            "|    time_elapsed         | 3552         |\n",
            "|    total_timesteps      | 1966080      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013176985 |\n",
            "|    clip_fraction        | 0.0293       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.443        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.89         |\n",
            "|    n_updates            | 9580         |\n",
            "|    policy_gradient_loss | -0.00377     |\n",
            "|    value_loss           | 8.38         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 18.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 481          |\n",
            "|    time_elapsed         | 3560         |\n",
            "|    total_timesteps      | 1970176      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017868653 |\n",
            "|    clip_fraction        | 0.0557       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.528        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.29         |\n",
            "|    n_updates            | 9600         |\n",
            "|    policy_gradient_loss | -0.00465     |\n",
            "|    value_loss           | 7.36         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 18.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 482          |\n",
            "|    time_elapsed         | 3567         |\n",
            "|    total_timesteps      | 1974272      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016062132 |\n",
            "|    clip_fraction        | 0.0502       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.429        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.09         |\n",
            "|    n_updates            | 9620         |\n",
            "|    policy_gradient_loss | -0.00512     |\n",
            "|    value_loss           | 9.19         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 19.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 483          |\n",
            "|    time_elapsed         | 3575         |\n",
            "|    total_timesteps      | 1978368      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017922453 |\n",
            "|    clip_fraction        | 0.0532       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.419        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.28         |\n",
            "|    n_updates            | 9640         |\n",
            "|    policy_gradient_loss | -0.00543     |\n",
            "|    value_loss           | 8.38         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 20.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 484          |\n",
            "|    time_elapsed         | 3582         |\n",
            "|    total_timesteps      | 1982464      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017143842 |\n",
            "|    clip_fraction        | 0.058        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.304        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.94         |\n",
            "|    n_updates            | 9660         |\n",
            "|    policy_gradient_loss | -0.00437     |\n",
            "|    value_loss           | 10.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 19.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 485          |\n",
            "|    time_elapsed         | 3589         |\n",
            "|    total_timesteps      | 1986560      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018469848 |\n",
            "|    clip_fraction        | 0.0693       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.443        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.55         |\n",
            "|    n_updates            | 9680         |\n",
            "|    policy_gradient_loss | -0.00594     |\n",
            "|    value_loss           | 7.73         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 19.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 486          |\n",
            "|    time_elapsed         | 3597         |\n",
            "|    total_timesteps      | 1990656      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017361763 |\n",
            "|    clip_fraction        | 0.0619       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.29        |\n",
            "|    explained_variance   | 0.471        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.34         |\n",
            "|    n_updates            | 9700         |\n",
            "|    policy_gradient_loss | -0.00561     |\n",
            "|    value_loss           | 7.24         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 126         |\n",
            "|    ep_rew_mean          | 18.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 487         |\n",
            "|    time_elapsed         | 3604        |\n",
            "|    total_timesteps      | 1994752     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001813197 |\n",
            "|    clip_fraction        | 0.0622      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.29       |\n",
            "|    explained_variance   | 0.474       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.52        |\n",
            "|    n_updates            | 9720        |\n",
            "|    policy_gradient_loss | -0.00507    |\n",
            "|    value_loss           | 7.76        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 132          |\n",
            "|    ep_rew_mean          | 20.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 553          |\n",
            "|    iterations           | 488          |\n",
            "|    time_elapsed         | 3611         |\n",
            "|    total_timesteps      | 1998848      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016611791 |\n",
            "|    clip_fraction        | 0.0488       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.3         |\n",
            "|    explained_variance   | 0.533        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.79         |\n",
            "|    n_updates            | 9740         |\n",
            "|    policy_gradient_loss | -0.00444     |\n",
            "|    value_loss           | 8.34         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 131         |\n",
            "|    ep_rew_mean          | 21.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 553         |\n",
            "|    iterations           | 489         |\n",
            "|    time_elapsed         | 3619        |\n",
            "|    total_timesteps      | 2002944     |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001367301 |\n",
            "|    clip_fraction        | 0.0339      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.3        |\n",
            "|    explained_variance   | 0.335       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 4.23        |\n",
            "|    n_updates            | 9760        |\n",
            "|    policy_gradient_loss | -0.00378    |\n",
            "|    value_loss           | 9.54        |\n",
            "-----------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from stable_baselines3.common.preprocessing import preprocess_obs\n",
        "\n",
        "env_dbg = Game2048EnvV3()\n",
        "obs, info = env_dbg.reset()\n",
        "\n",
        "for i in range(5):\n",
        "    obs_tensor = torch.as_tensor([obs], device=model.device)\n",
        "    obs_tensor = preprocess_obs(obs_tensor, env_dbg.observation_space)\n",
        "\n",
        "    dist = model.policy.get_distribution(obs_tensor)\n",
        "    probs = dist.distribution.probs.detach().cpu().numpy()[0]\n",
        "\n",
        "    print(f\"Probs on state {i}: {probs}\")\n",
        "\n",
        "    # create new state\n",
        "    action = env_dbg.action_space.sample()\n",
        "    obs, _, done, _, _ = env_dbg.step(action)\n",
        "    if done:\n",
        "        obs, info = env_dbg.reset()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BnWGzbW4Z5Ay",
        "outputId": "51be4c7d-05a7-431b-94c1-f5b7420fd97c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Probs on state 0: [0.26067576 0.22560897 0.22320904 0.29050624]\n",
            "Probs on state 1: [0.18664004 0.33731905 0.13716367 0.33887723]\n",
            "Probs on state 2: [0.14637853 0.40725422 0.15866132 0.2877059 ]\n",
            "Probs on state 3: [0.11983611 0.4621226  0.31354755 0.10449384]\n",
            "Probs on state 4: [0.10891565 0.47009867 0.3245981  0.09638763]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/tmp/ipython-input-2830357457.py:8: UserWarning: Creating a tensor from a list of numpy.ndarrays is extremely slow. Please consider converting the list to a single numpy.ndarray with numpy.array() before converting to a tensor. (Triggered internally at /pytorch/torch/csrc/utils/tensor_new.cpp:253.)\n",
            "  obs_tensor = torch.as_tensor([obs], device=model.device)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = Game2048EnvV3()\n",
        "obs, info = env.reset()\n",
        "\n",
        "print(\"Initial obs:\", obs.flatten())\n",
        "\n",
        "for i in range(5):\n",
        "    action = env.action_space.sample()\n",
        "    obs, reward, term, trunc, info = env.step(action)\n",
        "    print(f\"Step {i}, action={action}, obs:\", obs.flatten())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AQVOugRQZ9q1",
        "outputId": "768c2f21-1982-4d64-d782-0c917684a532"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial obs: [0.06666667 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.06666667 0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n",
            "Step 0, action=2, obs: [0.06666667 0.         0.         0.         0.         0.\n",
            " 0.         0.         0.06666667 0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n",
            "Step 1, action=1, obs: [0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.06666667\n",
            " 0.13333334 0.         0.         0.        ]\n",
            "Step 2, action=0, obs: [0.13333334 0.06666667 0.         0.06666667 0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n",
            "Step 3, action=2, obs: [0.13333334 0.13333334 0.         0.         0.         0.\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.06666667 0.         0.         0.        ]\n",
            "Step 4, action=0, obs: [0.13333334 0.13333334 0.         0.         0.06666667 0.06666667\n",
            " 0.         0.         0.         0.         0.         0.\n",
            " 0.         0.         0.         0.        ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "# Load the trained model\n",
        "model = PPO.load(\"ppo_2048_v3_big\")\n",
        "\n",
        "# Create a fresh environment for playing\n",
        "env = Game2048EnvV3(render_mode=\"ansi\")\n",
        "\n",
        "obs, info = env.reset()\n",
        "done = False\n",
        "step = 0\n",
        "\n",
        "print(\"Initial Board:\")\n",
        "print(env._board_to_string())\n",
        "\n",
        "while not done:\n",
        "    # deterministic=True = best move according to PPO\n",
        "    # deterministic=False = allows exploration\n",
        "    action, _ = model.predict(obs, deterministic=False)\n",
        "\n",
        "    obs, reward, terminated, truncated, info = env.step(int(action))\n",
        "    done = terminated or truncated\n",
        "\n",
        "    print(f\"\\nStep {step} | Action: {int(action)} | Reward: {reward:.2f} | Score: {info['score']:.1f}\")\n",
        "    print(env._board_to_string())\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    step += 1\n",
        "    time.sleep(0.15)  # Slow down for visual effect (optional)\n",
        "\n",
        "print(\"\\n=== Episode Finished ===\")\n",
        "print(f\"Final Score: {info['score']}\")\n",
        "print(f\"Max Tile: {info['max_tile']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "50LatPD7WOO1",
        "outputId": "b11f044d-78a2-49b6-ceae-2f06e47c5538"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Board:\n",
            ".\t.\t.\t.\n",
            ".\t2\t.\t.\n",
            ".\t.\t2\t.\n",
            ".\t.\t.\t.\n",
            "\n",
            "Step 0 | Action: 0 | Reward: 0.00 | Score: 0.0\n",
            ".\t2\t2\t.\n",
            ".\t.\t4\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 1 | Action: 1 | Reward: 0.00 | Score: 0.0\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            ".\t2\t2\t.\n",
            ".\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 2 | Action: 1 | Reward: 0.12 | Score: 4.0\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t2\t2\n",
            ".\t4\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 3 | Action: 1 | Reward: 0.00 | Score: 4.0\n",
            ".\t.\t.\t2\n",
            ".\t.\t.\t.\n",
            ".\t.\t2\t.\n",
            ".\t4\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 4 | Action: 0 | Reward: 0.12 | Score: 8.0\n",
            ".\t4\t2\t4\n",
            ".\t.\t4\t.\n",
            ".\t.\t.\t.\n",
            ".\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 5 | Action: 0 | Reward: 0.00 | Score: 8.0\n",
            ".\t4\t2\t4\n",
            ".\t2\t4\t4\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 6 | Action: 2 | Reward: 0.25 | Score: 16.0\n",
            "4\t2\t4\t.\n",
            "2\t8\t.\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 7 | Action: 1 | Reward: 0.00 | Score: 16.0\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "4\t2\t.\t2\n",
            "2\t8\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 8 | Action: 0 | Reward: 0.12 | Score: 20.0\n",
            "4\t2\t4\t4\n",
            "2\t8\t.\t.\n",
            "2\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 9 | Action: 3 | Reward: 0.25 | Score: 28.0\n",
            ".\t4\t2\t8\n",
            "2\t.\t2\t8\n",
            ".\t.\t.\t2\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 10 | Action: 2 | Reward: 0.12 | Score: 32.0\n",
            "4\t2\t8\t.\n",
            "4\t8\t.\t.\n",
            "2\t.\t2\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 11 | Action: 3 | Reward: 0.12 | Score: 36.0\n",
            "2\t4\t2\t8\n",
            ".\t.\t4\t8\n",
            ".\t.\t.\t4\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 12 | Action: 2 | Reward: 0.00 | Score: 36.0\n",
            "2\t4\t2\t8\n",
            "4\t8\t.\t4\n",
            "4\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 13 | Action: 1 | Reward: 0.25 | Score: 44.0\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t2\n",
            "2\t4\t.\t8\n",
            "8\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 14 | Action: 2 | Reward: 0.50 | Score: 60.0\n",
            "2\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "2\t4\t8\t.\n",
            "16\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 15 | Action: 0 | Reward: 0.12 | Score: 64.0\n",
            "4\t4\t8\t.\n",
            "2\t2\t4\t.\n",
            "16\t.\t2\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 16 | Action: 1 | Reward: 0.00 | Score: 64.0\n",
            ".\t2\t.\t.\n",
            "4\t.\t8\t.\n",
            "2\t4\t4\t.\n",
            "16\t2\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 17 | Action: 0 | Reward: 0.00 | Score: 64.0\n",
            "4\t2\t8\t.\n",
            "2\t4\t4\t.\n",
            "16\t2\t2\t.\n",
            "2\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 18 | Action: 1 | Reward: 0.00 | Score: 64.0\n",
            "4\t.\t.\t.\n",
            "2\t2\t8\t.\n",
            "16\t4\t4\t.\n",
            "2\t2\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 19 | Action: 3 | Reward: 0.50 | Score: 80.0\n",
            ".\t.\t2\t4\n",
            ".\t.\t4\t8\n",
            ".\t.\t16\t8\n",
            ".\t2\t4\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 20 | Action: 0 | Reward: 0.50 | Score: 96.0\n",
            ".\t2\t2\t4\n",
            ".\t.\t4\t16\n",
            ".\t.\t16\t4\n",
            ".\t.\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 21 | Action: 2 | Reward: 0.12 | Score: 100.0\n",
            "4\t4\t2\t.\n",
            "4\t16\t.\t.\n",
            "16\t4\t.\t.\n",
            "4\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 22 | Action: 3 | Reward: 0.25 | Score: 108.0\n",
            ".\t.\t8\t2\n",
            ".\t.\t4\t16\n",
            "2\t.\t16\t4\n",
            ".\t.\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 23 | Action: 1 | Reward: 0.00 | Score: 108.0\n",
            ".\t.\t8\t2\n",
            ".\t.\t4\t16\n",
            ".\t.\t16\t4\n",
            "2\t2\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 24 | Action: 3 | Reward: 0.12 | Score: 112.0\n",
            ".\t.\t8\t2\n",
            ".\t2\t4\t16\n",
            ".\t.\t16\t4\n",
            ".\t4\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 25 | Action: 3 | Reward: 0.25 | Score: 120.0\n",
            ".\t.\t8\t2\n",
            ".\t2\t4\t16\n",
            ".\t.\t16\t4\n",
            "2\t.\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 26 | Action: 0 | Reward: 0.00 | Score: 120.0\n",
            "2\t2\t8\t2\n",
            ".\t.\t4\t16\n",
            ".\t.\t16\t4\n",
            ".\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 27 | Action: 1 | Reward: 0.12 | Score: 124.0\n",
            ".\t.\t8\t2\n",
            "2\t.\t4\t16\n",
            ".\t.\t16\t4\n",
            "2\t4\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 28 | Action: 0 | Reward: 0.12 | Score: 128.0\n",
            "4\t4\t8\t2\n",
            ".\t.\t4\t16\n",
            ".\t2\t16\t4\n",
            ".\t.\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 29 | Action: 1 | Reward: 0.00 | Score: 128.0\n",
            "2\t.\t8\t2\n",
            ".\t.\t4\t16\n",
            ".\t4\t16\t4\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 30 | Action: 0 | Reward: 0.00 | Score: 128.0\n",
            "2\t4\t8\t2\n",
            "4\t2\t4\t16\n",
            ".\t.\t16\t4\n",
            ".\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 31 | Action: 1 | Reward: 0.12 | Score: 132.0\n",
            ".\t.\t8\t2\n",
            "2\t.\t4\t16\n",
            "2\t4\t16\t4\n",
            "4\t4\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 32 | Action: 3 | Reward: 0.25 | Score: 140.0\n",
            ".\t.\t8\t2\n",
            "2\t2\t4\t16\n",
            "2\t4\t16\t4\n",
            ".\t8\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 33 | Action: 1 | Reward: 0.12 | Score: 144.0\n",
            "2\t.\t8\t2\n",
            ".\t2\t4\t16\n",
            ".\t4\t16\t4\n",
            "4\t8\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 34 | Action: 1 | Reward: 0.00 | Score: 144.0\n",
            ".\t2\t8\t2\n",
            ".\t2\t4\t16\n",
            "2\t4\t16\t4\n",
            "4\t8\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 35 | Action: 2 | Reward: 0.50 | Score: 160.0\n",
            "2\t8\t2\t2\n",
            "2\t4\t16\t.\n",
            "2\t4\t16\t4\n",
            "4\t16\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 36 | Action: 3 | Reward: 0.12 | Score: 164.0\n",
            ".\t2\t8\t4\n",
            "2\t2\t4\t16\n",
            "2\t4\t16\t4\n",
            ".\t4\t16\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 37 | Action: 1 | Reward: 1.50 | Score: 212.0\n",
            ".\t.\t.\t4\n",
            ".\t2\t8\t16\n",
            ".\t4\t4\t4\n",
            "4\t8\t32\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 38 | Action: 3 | Reward: 0.25 | Score: 220.0\n",
            ".\t.\t.\t4\n",
            "2\t2\t8\t16\n",
            ".\t.\t4\t8\n",
            "4\t8\t32\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 39 | Action: 0 | Reward: 0.00 | Score: 220.0\n",
            "2\t2\t8\t4\n",
            "4\t8\t4\t16\n",
            ".\t.\t32\t8\n",
            "2\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 40 | Action: 2 | Reward: 0.25 | Score: 228.0\n",
            "4\t8\t4\t.\n",
            "4\t8\t4\t16\n",
            "32\t8\t.\t.\n",
            "4\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 41 | Action: 1 | Reward: 1.00 | Score: 260.0\n",
            ".\t.\t.\t.\n",
            "8\t8\t2\t.\n",
            "32\t16\t.\t.\n",
            "4\t2\t8\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 42 | Action: 0 | Reward: 0.00 | Score: 260.0\n",
            "8\t8\t2\t16\n",
            "32\t16\t8\t2\n",
            "4\t2\t.\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 43 | Action: 3 | Reward: 0.50 | Score: 276.0\n",
            ".\t16\t2\t16\n",
            "32\t16\t8\t2\n",
            ".\t.\t4\t2\n",
            ".\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 44 | Action: 2 | Reward: 0.00 | Score: 276.0\n",
            "16\t2\t16\t2\n",
            "32\t16\t8\t2\n",
            "4\t2\t.\t.\n",
            "2\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 45 | Action: 3 | Reward: 0.00 | Score: 276.0\n",
            "16\t2\t16\t2\n",
            "32\t16\t8\t2\n",
            ".\t.\t4\t2\n",
            ".\t2\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 46 | Action: 3 | Reward: 0.12 | Score: 280.0\n",
            "16\t2\t16\t2\n",
            "32\t16\t8\t2\n",
            ".\t2\t4\t2\n",
            ".\t.\t.\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 47 | Action: 0 | Reward: 0.12 | Score: 284.0\n",
            "16\t2\t16\t4\n",
            "32\t16\t8\t2\n",
            ".\t2\t4\t4\n",
            ".\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 48 | Action: 3 | Reward: 0.25 | Score: 292.0\n",
            "16\t2\t16\t4\n",
            "32\t16\t8\t2\n",
            ".\t2\t2\t8\n",
            ".\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 49 | Action: 2 | Reward: 0.12 | Score: 296.0\n",
            "16\t2\t16\t4\n",
            "32\t16\t8\t2\n",
            "4\t8\t.\t.\n",
            "2\t.\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 50 | Action: 3 | Reward: 0.12 | Score: 300.0\n",
            "16\t2\t16\t4\n",
            "32\t16\t8\t2\n",
            "2\t.\t4\t8\n",
            ".\t.\t.\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 51 | Action: 1 | Reward: 0.00 | Score: 300.0\n",
            ".\t.\t.\t4\n",
            "16\t2\t16\t2\n",
            "32\t2\t8\t8\n",
            "2\t16\t4\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 52 | Action: 3 | Reward: 0.75 | Score: 324.0\n",
            ".\t.\t.\t4\n",
            "16\t2\t16\t2\n",
            "4\t32\t2\t16\n",
            ".\t2\t16\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 53 | Action: 1 | Reward: 0.00 | Score: 324.0\n",
            ".\t.\t2\t4\n",
            ".\t2\t16\t2\n",
            "16\t32\t2\t16\n",
            "4\t2\t16\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 54 | Action: 0 | Reward: 0.00 | Score: 324.0\n",
            "16\t2\t2\t4\n",
            "4\t32\t16\t2\n",
            ".\t2\t2\t16\n",
            ".\t2\t16\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 55 | Action: 2 | Reward: 0.25 | Score: 332.0\n",
            "16\t4\t4\t.\n",
            "4\t32\t16\t2\n",
            "4\t16\t2\t.\n",
            "2\t16\t8\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 56 | Action: 2 | Reward: 0.25 | Score: 340.0\n",
            "16\t8\t.\t.\n",
            "4\t32\t16\t2\n",
            "4\t16\t2\t.\n",
            "2\t16\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 57 | Action: 3 | Reward: 0.00 | Score: 340.0\n",
            ".\t.\t16\t8\n",
            "4\t32\t16\t2\n",
            "2\t4\t16\t2\n",
            "2\t16\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 58 | Action: 1 | Reward: 1.25 | Score: 380.0\n",
            ".\t.\t4\t.\n",
            ".\t32\t16\t8\n",
            "4\t4\t32\t2\n",
            "4\t16\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 59 | Action: 2 | Reward: 0.25 | Score: 388.0\n",
            "4\t4\t.\t.\n",
            "32\t16\t8\t.\n",
            "8\t32\t2\t.\n",
            "4\t16\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 60 | Action: 3 | Reward: 0.25 | Score: 396.0\n",
            ".\t2\t.\t8\n",
            ".\t32\t16\t8\n",
            ".\t8\t32\t2\n",
            "4\t16\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 61 | Action: 0 | Reward: 0.50 | Score: 412.0\n",
            "4\t2\t16\t16\n",
            ".\t32\t32\t2\n",
            ".\t8\t8\t4\n",
            "2\t16\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 62 | Action: 2 | Reward: 3.50 | Score: 524.0\n",
            "4\t2\t32\t2\n",
            "64\t2\t.\t.\n",
            "16\t4\t.\t.\n",
            "2\t16\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 63 | Action: 2 | Reward: -1.00 | Score: 524.0\n",
            "4\t2\t32\t2\n",
            "64\t2\t.\t.\n",
            "16\t4\t.\t.\n",
            "2\t16\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 64 | Action: 3 | Reward: 0.00 | Score: 524.0\n",
            "4\t2\t32\t2\n",
            ".\t.\t64\t2\n",
            "2\t.\t16\t4\n",
            ".\t.\t2\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 65 | Action: 2 | Reward: 0.00 | Score: 524.0\n",
            "4\t2\t32\t2\n",
            "64\t2\t.\t2\n",
            "2\t16\t4\t.\n",
            "2\t16\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 66 | Action: 2 | Reward: 0.12 | Score: 528.0\n",
            "4\t2\t32\t2\n",
            "64\t4\t.\t.\n",
            "2\t16\t4\t2\n",
            "2\t16\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 67 | Action: 1 | Reward: 1.25 | Score: 568.0\n",
            ".\t.\t.\t2\n",
            "4\t2\t.\t.\n",
            "64\t4\t32\t.\n",
            "4\t32\t4\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 68 | Action: 3 | Reward: 0.25 | Score: 576.0\n",
            ".\t.\t.\t2\n",
            "2\t.\t4\t2\n",
            ".\t64\t4\t32\n",
            ".\t4\t32\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 69 | Action: 0 | Reward: 0.38 | Score: 588.0\n",
            "2\t64\t8\t4\n",
            ".\t4\t32\t32\n",
            ".\t.\t.\t8\n",
            ".\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 70 | Action: 2 | Reward: 2.00 | Score: 652.0\n",
            "2\t64\t8\t4\n",
            "4\t64\t.\t.\n",
            "8\t.\t.\t.\n",
            "2\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 71 | Action: 1 | Reward: 4.00 | Score: 780.0\n",
            "2\t.\t2\t.\n",
            "4\t.\t.\t.\n",
            "8\t128\t.\t.\n",
            "2\t2\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 72 | Action: 3 | Reward: 0.25 | Score: 788.0\n",
            ".\t2\t.\t4\n",
            ".\t.\t.\t4\n",
            ".\t.\t8\t128\n",
            ".\t4\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 73 | Action: 1 | Reward: 0.75 | Score: 812.0\n",
            ".\t.\t.\t.\n",
            "2\t.\t.\t8\n",
            ".\t2\t.\t128\n",
            ".\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 74 | Action: 0 | Reward: 0.00 | Score: 812.0\n",
            "2\t2\t16\t8\n",
            "2\t4\t.\t128\n",
            ".\t.\t.\t4\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 75 | Action: 2 | Reward: 0.12 | Score: 816.0\n",
            "4\t16\t8\t.\n",
            "2\t4\t128\t.\n",
            "4\t.\t.\t2\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 76 | Action: 0 | Reward: 0.00 | Score: 816.0\n",
            "4\t16\t8\t2\n",
            "2\t4\t128\t.\n",
            "4\t.\t.\t2\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 77 | Action: 3 | Reward: 0.00 | Score: 816.0\n",
            "4\t16\t8\t2\n",
            ".\t2\t4\t128\n",
            ".\t.\t4\t2\n",
            "2\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 78 | Action: 1 | Reward: 0.25 | Score: 824.0\n",
            "2\t.\t.\t.\n",
            ".\t.\t.\t2\n",
            "4\t16\t8\t128\n",
            "2\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 79 | Action: 0 | Reward: 0.50 | Score: 840.0\n",
            "2\t16\t16\t2\n",
            "4\t2\t.\t128\n",
            "2\t.\t.\t2\n",
            ".\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 80 | Action: 3 | Reward: 1.12 | Score: 876.0\n",
            ".\t2\t32\t2\n",
            ".\t4\t2\t128\n",
            ".\t.\t2\t4\n",
            ".\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 81 | Action: 0 | Reward: 0.12 | Score: 880.0\n",
            ".\t2\t32\t2\n",
            "2\t4\t4\t128\n",
            ".\t.\t.\t4\n",
            ".\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 82 | Action: 1 | Reward: 0.00 | Score: 880.0\n",
            ".\t.\t.\t2\n",
            ".\t.\t2\t128\n",
            ".\t2\t32\t4\n",
            "2\t4\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 83 | Action: 1 | Reward: -1.00 | Score: 880.0\n",
            ".\t.\t.\t2\n",
            ".\t.\t2\t128\n",
            ".\t2\t32\t4\n",
            "2\t4\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 84 | Action: 2 | Reward: 0.25 | Score: 888.0\n",
            "2\t2\t.\t.\n",
            "2\t128\t.\t.\n",
            "2\t32\t4\t.\n",
            "2\t8\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 85 | Action: 2 | Reward: 0.12 | Score: 892.0\n",
            "4\t.\t.\t.\n",
            "2\t128\t2\t.\n",
            "2\t32\t4\t.\n",
            "2\t8\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 86 | Action: 0 | Reward: 0.12 | Score: 896.0\n",
            "4\t128\t2\t.\n",
            "4\t32\t4\t.\n",
            "2\t8\t2\t.\n",
            ".\t.\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 87 | Action: 1 | Reward: 0.38 | Score: 908.0\n",
            ".\t.\t.\t.\n",
            ".\t128\t2\t.\n",
            "8\t32\t4\t.\n",
            "2\t8\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 88 | Action: 0 | Reward: 0.25 | Score: 916.0\n",
            "8\t128\t2\t2\n",
            "2\t32\t8\t.\n",
            "4\t8\t.\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 89 | Action: 0 | Reward: -1.00 | Score: 916.0\n",
            "8\t128\t2\t2\n",
            "2\t32\t8\t.\n",
            "4\t8\t.\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 90 | Action: 1 | Reward: 0.00 | Score: 916.0\n",
            "2\t.\t.\t.\n",
            "8\t128\t.\t.\n",
            "2\t32\t2\t.\n",
            "4\t8\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 91 | Action: 3 | Reward: 0.50 | Score: 932.0\n",
            ".\t.\t2\t2\n",
            ".\t.\t8\t128\n",
            ".\t2\t32\t2\n",
            ".\t4\t16\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 92 | Action: 1 | Reward: 0.12 | Score: 936.0\n",
            ".\t2\t2\t.\n",
            ".\t.\t8\t2\n",
            ".\t2\t32\t128\n",
            ".\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 93 | Action: 1 | Reward: 0.12 | Score: 940.0\n",
            ".\t.\t2\t.\n",
            ".\t.\t8\t2\n",
            ".\t4\t32\t128\n",
            "2\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 94 | Action: 0 | Reward: 0.25 | Score: 948.0\n",
            "2\t8\t2\t2\n",
            "2\t.\t8\t128\n",
            ".\t.\t32\t4\n",
            ".\t.\t16\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 95 | Action: 1 | Reward: 0.12 | Score: 952.0\n",
            ".\t.\t2\t.\n",
            ".\t2\t8\t2\n",
            ".\t.\t32\t128\n",
            "4\t8\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 96 | Action: 0 | Reward: 0.00 | Score: 952.0\n",
            "4\t2\t2\t2\n",
            ".\t8\t8\t128\n",
            ".\t.\t32\t4\n",
            ".\t.\t16\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 97 | Action: 1 | Reward: 0.00 | Score: 952.0\n",
            ".\t.\t2\t2\n",
            ".\t.\t8\t128\n",
            "2\t2\t32\t4\n",
            "4\t8\t16\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 98 | Action: 2 | Reward: 0.25 | Score: 960.0\n",
            "4\t.\t.\t.\n",
            "8\t128\t2\t.\n",
            "4\t32\t4\t.\n",
            "4\t8\t16\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 99 | Action: 0 | Reward: 0.25 | Score: 968.0\n",
            "4\t128\t2\t2\n",
            "8\t32\t4\t.\n",
            "8\t8\t16\t.\n",
            "4\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 100 | Action: 3 | Reward: 0.62 | Score: 988.0\n",
            ".\t4\t128\t4\n",
            ".\t8\t32\t4\n",
            ".\t.\t16\t16\n",
            ".\t.\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 101 | Action: 1 | Reward: 0.25 | Score: 996.0\n",
            ".\t.\t128\t.\n",
            ".\t2\t32\t8\n",
            ".\t4\t16\t16\n",
            ".\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 102 | Action: 1 | Reward: -1.00 | Score: 996.0\n",
            ".\t.\t128\t.\n",
            ".\t2\t32\t8\n",
            ".\t4\t16\t16\n",
            ".\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 103 | Action: 0 | Reward: 0.00 | Score: 996.0\n",
            "2\t2\t128\t8\n",
            ".\t4\t32\t16\n",
            ".\t8\t16\t4\n",
            ".\t.\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 104 | Action: 1 | Reward: 0.00 | Score: 996.0\n",
            ".\t.\t128\t2\n",
            ".\t2\t32\t8\n",
            ".\t4\t16\t16\n",
            "2\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 105 | Action: 0 | Reward: 0.00 | Score: 996.0\n",
            "2\t2\t128\t2\n",
            ".\t4\t32\t8\n",
            ".\t8\t16\t16\n",
            ".\t2\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 106 | Action: 0 | Reward: -1.00 | Score: 996.0\n",
            "2\t2\t128\t2\n",
            ".\t4\t32\t8\n",
            ".\t8\t16\t16\n",
            ".\t2\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 107 | Action: 0 | Reward: -1.00 | Score: 996.0\n",
            "2\t2\t128\t2\n",
            ".\t4\t32\t8\n",
            ".\t8\t16\t16\n",
            ".\t2\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 108 | Action: 0 | Reward: -1.00 | Score: 996.0\n",
            "2\t2\t128\t2\n",
            ".\t4\t32\t8\n",
            ".\t8\t16\t16\n",
            ".\t2\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 109 | Action: 1 | Reward: 0.00 | Score: 996.0\n",
            ".\t2\t128\t2\n",
            ".\t4\t32\t8\n",
            "2\t8\t16\t16\n",
            "2\t2\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 110 | Action: 2 | Reward: 1.12 | Score: 1032.0\n",
            "2\t128\t2\t.\n",
            "4\t32\t8\t.\n",
            "2\t8\t32\t4\n",
            "4\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 111 | Action: 2 | Reward: -1.00 | Score: 1032.0\n",
            "2\t128\t2\t.\n",
            "4\t32\t8\t.\n",
            "2\t8\t32\t4\n",
            "4\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 112 | Action: 0 | Reward: 0.00 | Score: 1032.0\n",
            "2\t128\t2\t4\n",
            "4\t32\t8\t.\n",
            "2\t8\t32\t2\n",
            "4\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 113 | Action: 3 | Reward: 0.00 | Score: 1032.0\n",
            "2\t128\t2\t4\n",
            "2\t4\t32\t8\n",
            "2\t8\t32\t2\n",
            ".\t4\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 114 | Action: 0 | Reward: 2.12 | Score: 1100.0\n",
            "4\t128\t2\t4\n",
            "2\t4\t64\t8\n",
            ".\t8\t2\t2\n",
            "2\t4\t.\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 115 | Action: 2 | Reward: 0.38 | Score: 1112.0\n",
            "4\t128\t2\t4\n",
            "2\t4\t64\t8\n",
            "8\t4\t.\t.\n",
            "2\t8\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 116 | Action: 2 | Reward: 0.00 | Score: 1112.0\n",
            "4\t128\t2\t4\n",
            "2\t4\t64\t8\n",
            "8\t4\t.\t.\n",
            "2\t8\t2\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 117 | Action: 1 | Reward: 0.25 | Score: 1120.0\n",
            "4\t.\t.\t4\n",
            "2\t128\t2\t4\n",
            "8\t8\t64\t8\n",
            "2\t8\t2\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 118 | Action: 3 | Reward: 0.88 | Score: 1148.0\n",
            ".\t.\t4\t8\n",
            "2\t128\t2\t4\n",
            ".\t16\t64\t8\n",
            ".\t2\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 119 | Action: 2 | Reward: 0.00 | Score: 1148.0\n",
            "4\t8\t.\t.\n",
            "2\t128\t2\t4\n",
            "16\t64\t8\t.\n",
            "2\t8\t4\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 120 | Action: 1 | Reward: 0.25 | Score: 1156.0\n",
            "4\t8\t.\t2\n",
            "2\t128\t2\t.\n",
            "16\t64\t8\t.\n",
            "2\t8\t4\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 121 | Action: 0 | Reward: 0.00 | Score: 1156.0\n",
            "4\t8\t2\t2\n",
            "2\t128\t8\t8\n",
            "16\t64\t4\t.\n",
            "2\t8\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 122 | Action: 2 | Reward: 0.62 | Score: 1176.0\n",
            "4\t8\t4\t.\n",
            "2\t128\t16\t2\n",
            "16\t64\t4\t.\n",
            "2\t8\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 123 | Action: 1 | Reward: 0.00 | Score: 1176.0\n",
            "4\t8\t4\t2\n",
            "2\t128\t16\t.\n",
            "16\t64\t4\t.\n",
            "2\t8\t2\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 124 | Action: 3 | Reward: 0.12 | Score: 1180.0\n",
            "4\t8\t4\t2\n",
            ".\t2\t128\t16\n",
            ".\t16\t64\t4\n",
            "2\t2\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 125 | Action: 1 | Reward: 0.25 | Score: 1188.0\n",
            "2\t8\t4\t.\n",
            ".\t2\t128\t2\n",
            "4\t16\t64\t16\n",
            "2\t2\t8\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 126 | Action: 1 | Reward: 0.00 | Score: 1188.0\n",
            "2\t8\t4\t.\n",
            "2\t2\t128\t2\n",
            "4\t16\t64\t16\n",
            "2\t2\t8\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 127 | Action: 0 | Reward: 0.12 | Score: 1192.0\n",
            "4\t8\t4\t2\n",
            "4\t2\t128\t16\n",
            "2\t16\t64\t8\n",
            "2\t2\t8\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 128 | Action: 2 | Reward: 0.12 | Score: 1196.0\n",
            "4\t8\t4\t2\n",
            "4\t2\t128\t16\n",
            "2\t16\t64\t8\n",
            "4\t8\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 129 | Action: 0 | Reward: 0.25 | Score: 1204.0\n",
            "8\t8\t4\t2\n",
            "2\t2\t128\t16\n",
            "4\t16\t64\t8\n",
            "2\t8\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 130 | Action: 2 | Reward: 0.62 | Score: 1224.0\n",
            "16\t4\t2\t.\n",
            "4\t128\t16\t.\n",
            "4\t16\t64\t8\n",
            "2\t8\t2\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 131 | Action: 3 | Reward: 0.12 | Score: 1228.0\n",
            ".\t16\t4\t2\n",
            "2\t4\t128\t16\n",
            "4\t16\t64\t8\n",
            ".\t2\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 132 | Action: 2 | Reward: 0.00 | Score: 1228.0\n",
            "16\t4\t2\t2\n",
            "2\t4\t128\t16\n",
            "4\t16\t64\t8\n",
            "2\t8\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 133 | Action: 3 | Reward: 0.12 | Score: 1232.0\n",
            ".\t16\t4\t4\n",
            "2\t4\t128\t16\n",
            "4\t16\t64\t8\n",
            "2\t2\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 134 | Action: 2 | Reward: 0.38 | Score: 1244.0\n",
            "16\t8\t2\t.\n",
            "2\t4\t128\t16\n",
            "4\t16\t64\t8\n",
            "4\t8\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 135 | Action: 3 | Reward: 0.00 | Score: 1244.0\n",
            ".\t16\t8\t2\n",
            "2\t4\t128\t16\n",
            "4\t16\t64\t8\n",
            "2\t4\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 136 | Action: 0 | Reward: 0.00 | Score: 1244.0\n",
            "2\t16\t8\t2\n",
            "4\t4\t128\t16\n",
            "2\t16\t64\t8\n",
            "2\t4\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 137 | Action: 1 | Reward: 0.12 | Score: 1248.0\n",
            "2\t16\t8\t2\n",
            "2\t4\t128\t16\n",
            "4\t16\t64\t8\n",
            "4\t4\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 138 | Action: 0 | Reward: 0.38 | Score: 1260.0\n",
            "4\t16\t8\t2\n",
            "8\t4\t128\t16\n",
            ".\t16\t64\t8\n",
            "2\t4\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 139 | Action: 2 | Reward: 0.00 | Score: 1260.0\n",
            "4\t16\t8\t2\n",
            "8\t4\t128\t16\n",
            "16\t64\t8\t2\n",
            "2\t4\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 140 | Action: 1 | Reward: 0.50 | Score: 1276.0\n",
            "4\t16\t2\t2\n",
            "8\t4\t8\t16\n",
            "16\t64\t128\t2\n",
            "2\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 141 | Action: 3 | Reward: 0.12 | Score: 1280.0\n",
            "2\t4\t16\t4\n",
            "8\t4\t8\t16\n",
            "16\t64\t128\t2\n",
            "2\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 142 | Action: 1 | Reward: 0.25 | Score: 1288.0\n",
            "2\t2\t16\t4\n",
            "8\t8\t8\t16\n",
            "16\t64\t128\t2\n",
            "2\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 143 | Action: 3 | Reward: 0.62 | Score: 1308.0\n",
            "2\t4\t16\t4\n",
            ".\t8\t16\t16\n",
            "16\t64\t128\t2\n",
            "2\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 144 | Action: 2 | Reward: 1.00 | Score: 1340.0\n",
            "2\t4\t16\t4\n",
            "8\t32\t2\t.\n",
            "16\t64\t128\t2\n",
            "2\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 145 | Action: 3 | Reward: 0.00 | Score: 1340.0\n",
            "2\t4\t16\t4\n",
            "2\t8\t32\t2\n",
            "16\t64\t128\t2\n",
            "2\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 146 | Action: 1 | Reward: 0.25 | Score: 1348.0\n",
            "2\t4\t16\t.\n",
            "4\t8\t32\t4\n",
            "16\t64\t128\t4\n",
            "2\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 147 | Action: 2 | Reward: -1.00 | Score: 1348.0\n",
            "2\t4\t16\t.\n",
            "4\t8\t32\t4\n",
            "16\t64\t128\t4\n",
            "2\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 148 | Action: 3 | Reward: 0.00 | Score: 1348.0\n",
            "2\t2\t4\t16\n",
            "4\t8\t32\t4\n",
            "16\t64\t128\t4\n",
            "2\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 149 | Action: 2 | Reward: 0.12 | Score: 1352.0\n",
            "4\t4\t16\t2\n",
            "4\t8\t32\t4\n",
            "16\t64\t128\t4\n",
            "2\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 150 | Action: 0 | Reward: 0.50 | Score: 1368.0\n",
            "8\t4\t16\t2\n",
            "16\t8\t32\t8\n",
            "2\t64\t128\t4\n",
            "2\t4\t16\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 151 | Action: 1 | Reward: 0.12 | Score: 1372.0\n",
            ".\t4\t16\t2\n",
            "8\t8\t32\t2\n",
            "16\t64\t128\t8\n",
            "4\t4\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 152 | Action: 2 | Reward: 0.75 | Score: 1396.0\n",
            "4\t16\t2\t2\n",
            "16\t32\t2\t.\n",
            "16\t64\t128\t8\n",
            "8\t16\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 153 | Action: 3 | Reward: 0.12 | Score: 1400.0\n",
            ".\t4\t16\t4\n",
            ".\t16\t32\t2\n",
            "16\t64\t128\t8\n",
            "2\t8\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 154 | Action: 1 | Reward: -1.00 | Score: 1400.0\n",
            ".\t4\t16\t4\n",
            ".\t16\t32\t2\n",
            "16\t64\t128\t8\n",
            "2\t8\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 155 | Action: 2 | Reward: 0.00 | Score: 1400.0\n",
            "4\t16\t4\t.\n",
            "16\t32\t2\t2\n",
            "16\t64\t128\t8\n",
            "2\t8\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 156 | Action: 0 | Reward: 1.00 | Score: 1432.0\n",
            "4\t16\t4\t2\n",
            "32\t32\t2\t8\n",
            "2\t64\t128\t4\n",
            "2\t8\t16\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 157 | Action: 2 | Reward: 2.00 | Score: 1496.0\n",
            "4\t16\t4\t2\n",
            "64\t2\t8\t2\n",
            "2\t64\t128\t4\n",
            "2\t8\t16\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 158 | Action: 0 | Reward: 0.25 | Score: 1504.0\n",
            "4\t16\t4\t4\n",
            "64\t2\t8\t4\n",
            "4\t64\t128\t2\n",
            ".\t8\t16\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 159 | Action: 3 | Reward: 0.25 | Score: 1512.0\n",
            ".\t4\t16\t8\n",
            "64\t2\t8\t4\n",
            "4\t64\t128\t2\n",
            ".\t4\t8\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 160 | Action: 1 | Reward: 0.00 | Score: 1512.0\n",
            "2\t4\t16\t8\n",
            ".\t2\t8\t4\n",
            "64\t64\t128\t2\n",
            "4\t4\t8\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 161 | Action: 2 | Reward: 4.25 | Score: 1648.0\n",
            "2\t4\t16\t8\n",
            "2\t8\t4\t2\n",
            "128\t128\t2\t.\n",
            "8\t8\t16\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 162 | Action: 0 | Reward: 0.12 | Score: 1652.0\n",
            "4\t4\t16\t8\n",
            "128\t8\t4\t2\n",
            "8\t128\t2\t.\n",
            "2\t8\t16\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 163 | Action: 1 | Reward: 0.00 | Score: 1652.0\n",
            "4\t4\t16\t.\n",
            "128\t8\t4\t2\n",
            "8\t128\t2\t8\n",
            "2\t8\t16\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 164 | Action: 0 | Reward: 0.00 | Score: 1652.0\n",
            "4\t4\t16\t2\n",
            "128\t8\t4\t8\n",
            "8\t128\t2\t2\n",
            "2\t8\t16\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 165 | Action: 2 | Reward: 0.38 | Score: 1664.0\n",
            "8\t16\t2\t2\n",
            "128\t8\t4\t8\n",
            "8\t128\t4\t.\n",
            "2\t8\t16\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 166 | Action: 2 | Reward: 0.12 | Score: 1668.0\n",
            "8\t16\t4\t.\n",
            "128\t8\t4\t8\n",
            "8\t128\t4\t2\n",
            "2\t8\t16\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 167 | Action: 0 | Reward: 0.38 | Score: 1680.0\n",
            "8\t16\t8\t8\n",
            "128\t8\t4\t4\n",
            "8\t128\t16\t2\n",
            "2\t8\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 168 | Action: 2 | Reward: 0.75 | Score: 1704.0\n",
            "8\t16\t16\t2\n",
            "128\t8\t8\t.\n",
            "8\t128\t16\t2\n",
            "2\t8\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 169 | Action: 1 | Reward: 0.12 | Score: 1708.0\n",
            "8\t16\t.\t.\n",
            "128\t8\t16\t2\n",
            "8\t128\t8\t.\n",
            "2\t8\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 170 | Action: 3 | Reward: 0.00 | Score: 1708.0\n",
            "2\t.\t8\t16\n",
            "128\t8\t16\t2\n",
            ".\t8\t128\t8\n",
            "2\t8\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 171 | Action: 2 | Reward: 0.00 | Score: 1708.0\n",
            "2\t8\t16\t.\n",
            "128\t8\t16\t2\n",
            "8\t128\t8\t2\n",
            "2\t8\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 172 | Action: 2 | Reward: -1.00 | Score: 1708.0\n",
            "2\t8\t16\t.\n",
            "128\t8\t16\t2\n",
            "8\t128\t8\t2\n",
            "2\t8\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 173 | Action: 0 | Reward: 1.62 | Score: 1760.0\n",
            "2\t16\t32\t4\n",
            "128\t128\t8\t4\n",
            "8\t8\t16\t.\n",
            "2\t.\t.\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 174 | Action: 2 | Reward: 8.50 | Score: 2032.0\n",
            "2\t16\t32\t4\n",
            "256\t8\t4\t.\n",
            "16\t16\t.\t.\n",
            "2\t4\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 175 | Action: 1 | Reward: 0.00 | Score: 2032.0\n",
            "2\t16\t.\t2\n",
            "256\t8\t32\t.\n",
            "16\t16\t4\t.\n",
            "2\t4\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 176 | Action: 3 | Reward: 1.00 | Score: 2064.0\n",
            "2\t2\t16\t2\n",
            ".\t256\t8\t32\n",
            ".\t.\t32\t4\n",
            "2\t4\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 177 | Action: 1 | Reward: 0.38 | Score: 2076.0\n",
            ".\t.\t16\t.\n",
            "4\t2\t8\t2\n",
            ".\t256\t32\t32\n",
            "4\t4\t2\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 178 | Action: 1 | Reward: 0.25 | Score: 2084.0\n",
            "2\t.\t16\t.\n",
            ".\t2\t8\t2\n",
            ".\t256\t32\t32\n",
            "8\t4\t2\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 179 | Action: 2 | Reward: 2.00 | Score: 2148.0\n",
            "2\t16\t.\t2\n",
            "2\t8\t2\t.\n",
            "256\t64\t.\t.\n",
            "8\t4\t2\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 180 | Action: 0 | Reward: 0.25 | Score: 2156.0\n",
            "4\t16\t4\t2\n",
            "256\t8\t.\t8\n",
            "8\t64\t.\t.\n",
            ".\t4\t.\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 181 | Action: 3 | Reward: 0.75 | Score: 2180.0\n",
            "4\t16\t4\t2\n",
            ".\t.\t256\t16\n",
            ".\t2\t8\t64\n",
            ".\t.\t.\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 182 | Action: 1 | Reward: 0.00 | Score: 2180.0\n",
            ".\t.\t.\t2\n",
            ".\t.\t4\t16\n",
            "4\t16\t256\t64\n",
            "4\t2\t8\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 183 | Action: 3 | Reward: 0.50 | Score: 2196.0\n",
            ".\t.\t2\t2\n",
            ".\t.\t4\t16\n",
            "4\t16\t256\t64\n",
            ".\t4\t2\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 184 | Action: 2 | Reward: 0.12 | Score: 2200.0\n",
            "4\t2\t.\t.\n",
            "4\t16\t.\t.\n",
            "4\t16\t256\t64\n",
            "4\t2\t16\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 185 | Action: 0 | Reward: 1.50 | Score: 2248.0\n",
            "8\t2\t256\t64\n",
            "8\t32\t16\t.\n",
            ".\t2\t.\t.\n",
            ".\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 186 | Action: 2 | Reward: 0.00 | Score: 2248.0\n",
            "8\t2\t256\t64\n",
            "8\t32\t16\t.\n",
            "2\t.\t.\t.\n",
            "2\t.\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 187 | Action: 2 | Reward: 0.12 | Score: 2252.0\n",
            "8\t2\t256\t64\n",
            "8\t32\t16\t.\n",
            "2\t.\t.\t.\n",
            "4\t.\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 188 | Action: 0 | Reward: 0.50 | Score: 2268.0\n",
            "16\t2\t256\t64\n",
            "2\t32\t16\t.\n",
            "4\t.\t2\t.\n",
            ".\t.\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 189 | Action: 2 | Reward: 0.00 | Score: 2268.0\n",
            "16\t2\t256\t64\n",
            "2\t32\t16\t.\n",
            "4\t2\t2\t.\n",
            "2\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 190 | Action: 1 | Reward: 0.00 | Score: 2268.0\n",
            "16\t.\t.\t.\n",
            "2\t2\t256\t4\n",
            "4\t32\t16\t.\n",
            "2\t2\t2\t64\n",
            "----------------------------------------\n",
            "\n",
            "Step 191 | Action: 1 | Reward: 0.00 | Score: 2268.0\n",
            "16\t2\t.\t.\n",
            "2\t2\t256\t.\n",
            "4\t32\t16\t4\n",
            "2\t2\t2\t64\n",
            "----------------------------------------\n",
            "\n",
            "Step 192 | Action: 3 | Reward: 0.25 | Score: 2276.0\n",
            ".\t.\t16\t2\n",
            ".\t.\t4\t256\n",
            "4\t32\t16\t4\n",
            "2\t2\t4\t64\n",
            "----------------------------------------\n",
            "\n",
            "Step 193 | Action: 0 | Reward: 0.00 | Score: 2276.0\n",
            "4\t32\t16\t2\n",
            "2\t2\t4\t256\n",
            "4\t.\t16\t4\n",
            ".\t.\t4\t64\n",
            "----------------------------------------\n",
            "\n",
            "Step 194 | Action: 1 | Reward: 0.00 | Score: 2276.0\n",
            ".\t2\t16\t2\n",
            "4\t.\t4\t256\n",
            "2\t32\t16\t4\n",
            "4\t2\t4\t64\n",
            "----------------------------------------\n",
            "\n",
            "Step 195 | Action: 0 | Reward: 0.00 | Score: 2276.0\n",
            "4\t2\t16\t2\n",
            "2\t32\t4\t256\n",
            "4\t2\t16\t4\n",
            "2\t.\t4\t64\n",
            "----------------------------------------\n",
            "\n",
            "Step 196 | Action: 1 | Reward: 0.00 | Score: 2276.0\n",
            "4\t2\t16\t2\n",
            "2\t2\t4\t256\n",
            "4\t32\t16\t4\n",
            "2\t2\t4\t64\n",
            "----------------------------------------\n",
            "\n",
            "Step 197 | Action: 1 | Reward: 0.12 | Score: 2280.0\n",
            "4\t4\t16\t2\n",
            "2\t4\t4\t256\n",
            "4\t32\t16\t4\n",
            "2\t2\t4\t64\n",
            "----------------------------------------\n",
            "\n",
            "Step 198 | Action: 2 | Reward: 0.62 | Score: 2300.0\n",
            "8\t16\t2\t.\n",
            "2\t8\t256\t.\n",
            "4\t32\t16\t4\n",
            "4\t4\t64\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 199 | Action: 0 | Reward: 0.25 | Score: 2308.0\n",
            "8\t16\t2\t4\n",
            "2\t8\t256\t2\n",
            "8\t32\t16\t.\n",
            ".\t4\t64\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 200 | Action: 1 | Reward: 0.12 | Score: 2312.0\n",
            ".\t16\t2\t2\n",
            "8\t8\t256\t.\n",
            "2\t32\t16\t4\n",
            "8\t4\t64\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 201 | Action: 1 | Reward: 0.25 | Score: 2320.0\n",
            ".\t16\t2\t.\n",
            "8\t8\t256\t2\n",
            "2\t32\t16\t2\n",
            "8\t4\t64\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 202 | Action: 3 | Reward: 0.50 | Score: 2336.0\n",
            ".\t2\t16\t2\n",
            ".\t16\t256\t2\n",
            "2\t32\t16\t2\n",
            "8\t4\t64\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 203 | Action: 1 | Reward: 0.12 | Score: 2340.0\n",
            "2\t2\t16\t.\n",
            ".\t16\t256\t2\n",
            "2\t32\t16\t4\n",
            "8\t4\t64\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 204 | Action: 3 | Reward: 0.12 | Score: 2344.0\n",
            "2\t.\t4\t16\n",
            ".\t16\t256\t2\n",
            "2\t32\t16\t4\n",
            "8\t4\t64\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 205 | Action: 0 | Reward: 0.12 | Score: 2348.0\n",
            "4\t16\t4\t16\n",
            "8\t32\t256\t2\n",
            ".\t4\t16\t4\n",
            ".\t2\t64\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 206 | Action: 2 | Reward: 0.00 | Score: 2348.0\n",
            "4\t16\t4\t16\n",
            "8\t32\t256\t2\n",
            "4\t16\t4\t.\n",
            "2\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 207 | Action: 1 | Reward: 0.12 | Score: 2352.0\n",
            "4\t16\t4\t.\n",
            "8\t32\t256\t2\n",
            "4\t16\t4\t16\n",
            "2\t64\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 208 | Action: 1 | Reward: -1.00 | Score: 2352.0\n",
            "4\t16\t4\t.\n",
            "8\t32\t256\t2\n",
            "4\t16\t4\t16\n",
            "2\t64\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 209 | Action: 0 | Reward: 0.00 | Score: 2352.0\n",
            "4\t16\t4\t2\n",
            "8\t32\t256\t16\n",
            "4\t16\t4\t4\n",
            "2\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 210 | Action: 1 | Reward: -1.00 | Score: 2352.0\n",
            "4\t16\t4\t2\n",
            "8\t32\t256\t16\n",
            "4\t16\t4\t4\n",
            "2\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 211 | Action: 3 | Reward: 0.25 | Score: 2360.0\n",
            "4\t16\t4\t2\n",
            "8\t32\t256\t16\n",
            "2\t4\t16\t8\n",
            "2\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 212 | Action: 2 | Reward: -1.00 | Score: 2360.0\n",
            "4\t16\t4\t2\n",
            "8\t32\t256\t16\n",
            "2\t4\t16\t8\n",
            "2\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 213 | Action: 3 | Reward: -1.00 | Score: 2360.0\n",
            "4\t16\t4\t2\n",
            "8\t32\t256\t16\n",
            "2\t4\t16\t8\n",
            "2\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 214 | Action: 0 | Reward: 0.12 | Score: 2364.0\n",
            "4\t16\t4\t2\n",
            "8\t32\t256\t16\n",
            "4\t4\t16\t8\n",
            "2\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 215 | Action: 0 | Reward: -1.00 | Score: 2364.0\n",
            "4\t16\t4\t2\n",
            "8\t32\t256\t16\n",
            "4\t4\t16\t8\n",
            "2\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 216 | Action: 3 | Reward: 0.25 | Score: 2372.0\n",
            "4\t16\t4\t2\n",
            "8\t32\t256\t16\n",
            "2\t8\t16\t8\n",
            "2\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 217 | Action: 2 | Reward: -1.00 | Score: 2372.0\n",
            "4\t16\t4\t2\n",
            "8\t32\t256\t16\n",
            "2\t8\t16\t8\n",
            "2\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 218 | Action: 2 | Reward: -1.00 | Score: 2372.0\n",
            "4\t16\t4\t2\n",
            "8\t32\t256\t16\n",
            "2\t8\t16\t8\n",
            "2\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 219 | Action: 1 | Reward: 0.12 | Score: 2376.0\n",
            "4\t16\t4\t2\n",
            "4\t32\t256\t16\n",
            "8\t8\t16\t8\n",
            "4\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 220 | Action: 3 | Reward: 0.50 | Score: 2392.0\n",
            "4\t16\t4\t2\n",
            "4\t32\t256\t16\n",
            "2\t16\t16\t8\n",
            "4\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 221 | Action: 3 | Reward: 1.00 | Score: 2424.0\n",
            "4\t16\t4\t2\n",
            "4\t32\t256\t16\n",
            "2\t2\t32\t8\n",
            "4\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 222 | Action: 3 | Reward: 0.12 | Score: 2428.0\n",
            "4\t16\t4\t2\n",
            "4\t32\t256\t16\n",
            "2\t4\t32\t8\n",
            "4\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 223 | Action: 1 | Reward: 0.25 | Score: 2436.0\n",
            "2\t16\t4\t2\n",
            "8\t32\t256\t16\n",
            "2\t4\t32\t8\n",
            "4\t64\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "=== Episode Finished ===\n",
            "Final Score: 2436.0\n",
            "Max Tile: 256\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from stable_baselines3 import PPO\n",
        "\n",
        "NUM_EPISODES = 30\n",
        "\n",
        "# Load model\n",
        "ppo_model = PPO.load(\"ppo_2048_v3_big\")\n",
        "\n",
        "def run_episode(env, policy_fn):\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    total_reward = 0.0\n",
        "    final_score = 0.0\n",
        "    max_tile = 0\n",
        "\n",
        "    while not done:\n",
        "        action = policy_fn(obs, env)\n",
        "        obs, reward, terminated, truncated, info = env.step(action)\n",
        "        done = terminated or truncated\n",
        "\n",
        "        total_reward += reward\n",
        "        final_score = info[\"score\"]\n",
        "        max_tile = info[\"max_tile\"]\n",
        "\n",
        "    return total_reward, final_score, max_tile\n",
        "\n",
        "def random_policy(obs, env):\n",
        "    return env.action_space.sample()\n",
        "\n",
        "def ppo_policy(obs, env):\n",
        "    action, _ = ppo_model.predict(obs, deterministic=True)\n",
        "    return int(action)\n",
        "\n",
        "def evaluate(name, policy_fn, num_episodes=NUM_EPISODES):\n",
        "    rewards, scores, tiles = [], [], []\n",
        "    for _ in range(num_episodes):\n",
        "        env = Game2048EnvV3()\n",
        "        R, S, T = run_episode(env, policy_fn)\n",
        "        rewards.append(R)\n",
        "        scores.append(S)\n",
        "        tiles.append(T)\n",
        "    print(f\"\\n=== {name} over {num_episodes} episodes ===\")\n",
        "    print(f\"Avg total_reward: {np.mean(rewards):.2f} ¬± {np.std(rewards):.2f}\")\n",
        "    print(f\"Avg final score:  {np.mean(scores):.2f} ¬± {np.std(scores):.2f}\")\n",
        "    print(f\"Avg max tile:     {np.mean(tiles):.1f}\")\n",
        "    print(f\"Max of max tiles: {np.max(tiles)}\")\n",
        "    return rewards, scores, tiles\n",
        "\n",
        "rand_r, rand_s, rand_t = evaluate(\"Random\", random_policy)\n",
        "ppo_r,  ppo_s,  ppo_t  = evaluate(\"PPO\",    ppo_policy,num_episodes=2)\n",
        "\n",
        "print(\"\\nDone.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 460
        },
        "id": "J507PgsTVgf6",
        "outputId": "4bc69650-ce8b-4eda-fb1f-aa58ca784e8a"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "=== Random over 30 episodes ===\n",
            "Avg total_reward: 9.68 ¬± 10.65\n",
            "Avg final score:  962.40 ¬± 466.98\n",
            "Avg max tile:     89.6\n",
            "Max of max tiles: 128\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2184795140.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0mrand_r\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrand_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Random\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrandom_policy\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m \u001b[0mppo_r\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mppo_s\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0mppo_t\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"PPO\"\u001b[0m\u001b[0;34m,\u001b[0m    \u001b[0mppo_policy\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nDone.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2184795140.py\u001b[0m in \u001b[0;36mevaluate\u001b[0;34m(name, policy_fn, num_episodes)\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_episodes\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGame2048EnvV3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mS\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mT\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mR\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m         \u001b[0mscores\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mS\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-2184795140.py\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(env, policy_fn)\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpolicy_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m         \u001b[0mobs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mterminated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m         \u001b[0mdone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mterminated\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mtruncated\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3159462754.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, action)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# left\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mr\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m                 \u001b[0mnew_line\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slide_and_merge_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mboard\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnew_line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m                 \u001b[0mmerged_value\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mmv\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/tmp/ipython-input-3159462754.py\u001b[0m in \u001b[0;36m_slide_and_merge_line\u001b[0;34m(self, line)\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_slide_and_merge_line\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m         \u001b[0mnon_zero\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mline\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mline\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtolist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m         \u001b[0mnew\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mmerged_value\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium stable-baselines3 sb3-contrib numpy\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8pmf2RAxQwN9",
        "outputId": "ad1ee4dd-710c-475c-c57b-8be049584804"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Collecting sb3-contrib\n",
            "  Downloading sb3_contrib-2.7.0-py3-none-any.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.9.0+cu126)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.3)\n",
            "Downloading sb3_contrib-2.7.0-py3-none-any.whl (93 kB)\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m93.2/93.2 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sb3-contrib\n",
            "Successfully installed sb3-contrib-2.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "\n",
        "class Game2048EnvV4(gym.Env):\n",
        "    \"\"\"\n",
        "    2048 with action masking support.\n",
        "\n",
        "    - Obs: 4x4 grid, exponents / 15.0 in [0, 1]\n",
        "    - Action: 0=up,1=down,2=left,3=right\n",
        "    - Reward:\n",
        "        * (sum of merged tile values) / 32.0\n",
        "        * -1 for invalid move (but with masking we should never see those)\n",
        "    - get_action_mask(): True where action is valid (changes board)\n",
        "    \"\"\"\n",
        "\n",
        "    metadata = {\"render_modes\": [\"ansi\"], \"render_fps\": 60}\n",
        "\n",
        "    def __init__(self, render_mode=None, target_tile=2048):\n",
        "        super().__init__()\n",
        "        self.board_size = 4\n",
        "        self.target_tile = target_tile\n",
        "\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0.0,\n",
        "            high=1.0,\n",
        "            shape=(self.board_size, self.board_size),\n",
        "            dtype=np.float32,\n",
        "        )\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.board = np.zeros((self.board_size, self.board_size), dtype=np.int32)\n",
        "        self.score = 0.0\n",
        "        self.rng = np.random.default_rng()\n",
        "\n",
        "    # ---------- Helpers ----------\n",
        "    def _get_obs(self):\n",
        "        return self.board.astype(np.float32) / 15.0\n",
        "\n",
        "    def _slide_and_merge_line(self, line):\n",
        "        non_zero = line[line != 0].tolist()\n",
        "        new = []\n",
        "        merged_value = 0\n",
        "        i = 0\n",
        "        while i < len(non_zero):\n",
        "            if i + 1 < len(non_zero) and non_zero[i] == non_zero[i+1]:\n",
        "                exp = non_zero[i] + 1\n",
        "                new.append(exp)\n",
        "                merged_value += 2 ** exp\n",
        "                i += 2\n",
        "            else:\n",
        "                new.append(non_zero[i])\n",
        "                i += 1\n",
        "        new += [0] * (len(line) - len(new))\n",
        "        return np.array(new, dtype=np.int32), merged_value\n",
        "\n",
        "    def _add_random_tile(self):\n",
        "        empties = list(zip(*np.where(self.board == 0)))\n",
        "        if not empties:\n",
        "            return\n",
        "        r, c = empties[self.rng.integers(len(empties))]\n",
        "        self.board[r, c] = 1 if self.rng.random() < 0.9 else 2  # 2 or 4\n",
        "\n",
        "    def _can_move(self):\n",
        "        if np.any(self.board == 0):\n",
        "            return True\n",
        "        for r in range(self.board_size):\n",
        "            for c in range(self.board_size - 1):\n",
        "                if self.board[r, c] == self.board[r, c+1]:\n",
        "                    return True\n",
        "        for c in range(self.board_size):\n",
        "            for r in range(self.board_size - 1):\n",
        "                if self.board[r, c] == self.board[r+1, c]:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    def _get_max_tile(self):\n",
        "        exp = int(self.board.max())\n",
        "        return 0 if exp == 0 else 2 ** exp\n",
        "\n",
        "    # ---------- Simulation for masking ----------\n",
        "    def _simulate_move(self, action):\n",
        "        \"\"\"\n",
        "        Return the board that would result from taking `action`,\n",
        "        without changing the real env state and without adding a random tile.\n",
        "        \"\"\"\n",
        "        board = self.board\n",
        "        result = board.copy()\n",
        "\n",
        "        if action == 0:  # up\n",
        "            for c in range(self.board_size):\n",
        "                new_line, _ = self._slide_and_merge_line(board[:, c])\n",
        "                result[:, c] = new_line\n",
        "        elif action == 1:  # down\n",
        "            for c in range(self.board_size):\n",
        "                new_line, _ = self._slide_and_merge_line(board[:, c][::-1])\n",
        "                result[:, c] = new_line[::-1]\n",
        "        elif action == 2:  # left\n",
        "            for r in range(self.board_size):\n",
        "                new_line, _ = self._slide_and_merge_line(board[r])\n",
        "                result[r] = new_line\n",
        "        elif action == 3:  # right\n",
        "            for r in range(self.board_size):\n",
        "                new_line, _ = self._slide_and_merge_line(board[r][::-1])\n",
        "                result[r] = new_line[::-1]\n",
        "\n",
        "        return result\n",
        "\n",
        "    def get_action_mask(self):\n",
        "        \"\"\"\n",
        "        Returns a boolean mask of shape (4,):\n",
        "        True = action changes the board, False = action would do nothing.\n",
        "        \"\"\"\n",
        "        mask = np.zeros(self.action_space.n, dtype=bool)\n",
        "        for a in range(self.action_space.n):\n",
        "            new_board = self._simulate_move(a)\n",
        "            mask[a] = not np.array_equal(self.board, new_board)\n",
        "\n",
        "        # Safety: if somehow no action changes the board, allow all (should only happen at terminal).\n",
        "        if not mask.any():\n",
        "            mask[:] = True\n",
        "        return mask\n",
        "\n",
        "    # ---------- Gym API ----------\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "        if seed is not None:\n",
        "            self.rng = np.random.default_rng(seed)\n",
        "\n",
        "        self.board[:] = 0\n",
        "        self.score = 0.0\n",
        "\n",
        "        self._add_random_tile()\n",
        "        self._add_random_tile()\n",
        "\n",
        "        return self._get_obs(), {\"score\": self.score, \"max_tile\": self._get_max_tile()}\n",
        "\n",
        "    def step(self, action):\n",
        "        assert self.action_space.contains(action), \"Invalid action\"\n",
        "\n",
        "        old_board = self.board.copy()\n",
        "        merged_value = 0\n",
        "\n",
        "        if action == 0:  # up\n",
        "            for c in range(self.board_size):\n",
        "                new_line, mv = self._slide_and_merge_line(self.board[:, c])\n",
        "                self.board[:, c] = new_line\n",
        "                merged_value += mv\n",
        "        elif action == 1:  # down\n",
        "            for c in range(self.board_size):\n",
        "                new_line, mv = self._slide_and_merge_line(self.board[:, c][::-1])\n",
        "                self.board[:, c] = new_line[::-1]\n",
        "                merged_value += mv\n",
        "        elif action == 2:  # left\n",
        "            for r in range(self.board_size):\n",
        "                new_line, mv = self._slide_and_merge_line(self.board[r])\n",
        "                self.board[r] = new_line\n",
        "                merged_value += mv\n",
        "        elif action == 3:  # right\n",
        "            for r in range(self.board_size):\n",
        "                new_line, mv = self._slide_and_merge_line(self.board[r][::-1])\n",
        "                self.board[r] = new_line[::-1]\n",
        "                merged_value += mv\n",
        "\n",
        "        moved = not np.array_equal(old_board, self.board)\n",
        "\n",
        "        reward = 0.0\n",
        "        if moved:\n",
        "            self._add_random_tile()\n",
        "            reward += merged_value / 32.0\n",
        "        else:\n",
        "            # With masking this should basically never happen\n",
        "            reward -= 1.0\n",
        "\n",
        "        self.score += merged_value\n",
        "\n",
        "        max_tile = self._get_max_tile()\n",
        "        terminated = (not self._can_move()) or (max_tile >= self.target_tile)\n",
        "        truncated = False\n",
        "\n",
        "        obs = self._get_obs()\n",
        "        info = {\"score\": self.score, \"max_tile\": max_tile}\n",
        "\n",
        "        return obs, reward, terminated, truncated, info\n",
        "\n",
        "    def _board_to_string(self):\n",
        "        rows = []\n",
        "        for row in self.board:\n",
        "            r = []\n",
        "            for exp in row:\n",
        "                r.append(\".\" if exp == 0 else str(2 ** exp))\n",
        "            rows.append(\"\\t\".join(r))\n",
        "        return \"\\n\".join(rows)\n"
      ],
      "metadata": {
        "id": "wo5SUlxyQmtd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sb3_contrib import MaskablePPO\n",
        "from sb3_contrib.common.wrappers import ActionMasker\n",
        "\n",
        "# function that returns mask for ActionMasker\n",
        "def mask_fn(env: Game2048EnvV4):\n",
        "    # True = allowed, False = masked out\n",
        "    return env.get_action_mask()\n",
        "\n",
        "# create env + wrap with ActionMasker\n",
        "env = Game2048EnvV4()\n",
        "env = ActionMasker(env, mask_fn)\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    net_arch=[dict(pi=[256, 256, 256],\n",
        "                   vf=[256, 256, 256])]\n",
        ")\n",
        "\n",
        "model = MaskablePPO(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    policy_kwargs=policy_kwargs,\n",
        "    learning_rate=1e-4,\n",
        "    n_steps=4096,\n",
        "    batch_size=512,\n",
        "    n_epochs=20,\n",
        "    gamma=0.99,\n",
        "    clip_range=0.1,\n",
        "    ent_coef=0.1,\n",
        "    target_kl=0.02,\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "model.learn(total_timesteps=1_000_000)\n",
        "model.save(\"maskable_ppo_2048_v4\")\n",
        "print(\"Saved maskable_ppo_2048_v4.zip\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B5avtLFhQ3PD",
        "outputId": "8feaeb9e-9bba-42a7-9e7f-f81c04a92408"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/sb3_contrib/common/maskable/policies.py:78: UserWarning: As shared layers in the mlp_extractor are removed since SB3 v1.8.0, you should now pass directly a dictionary and not a list (net_arch=dict(pi=..., vf=...) instead of net_arch=[dict(pi=..., vf=...)])\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "---------------------------------\n",
            "| rollout/           |          |\n",
            "|    ep_len_mean     | 120      |\n",
            "|    ep_rew_mean     | 34.4     |\n",
            "| time/              |          |\n",
            "|    fps             | 374      |\n",
            "|    iterations      | 1        |\n",
            "|    time_elapsed    | 10       |\n",
            "|    total_timesteps | 4096     |\n",
            "---------------------------------\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 32.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 88           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014210669 |\n",
            "|    clip_fraction        | 0.0177       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.197        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.43         |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00168     |\n",
            "|    value_loss           | 15.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 34           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 9            |\n",
            "|    time_elapsed         | 99           |\n",
            "|    total_timesteps      | 36864        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015014465 |\n",
            "|    clip_fraction        | 0.0265       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.207        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.94         |\n",
            "|    n_updates            | 160          |\n",
            "|    policy_gradient_loss | -0.00222     |\n",
            "|    value_loss           | 17           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 110          |\n",
            "|    total_timesteps      | 40960        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010303346 |\n",
            "|    clip_fraction        | 0.00706      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.279        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.14         |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.00118     |\n",
            "|    value_loss           | 14.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 36.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 121          |\n",
            "|    total_timesteps      | 45056        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012185933 |\n",
            "|    clip_fraction        | 0.0136       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.276        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 9.05         |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.00182     |\n",
            "|    value_loss           | 15.9         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 123        |\n",
            "|    ep_rew_mean          | 36.2       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 369        |\n",
            "|    iterations           | 12         |\n",
            "|    time_elapsed         | 133        |\n",
            "|    total_timesteps      | 49152      |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00074244 |\n",
            "|    clip_fraction        | 0.000586   |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -1.24      |\n",
            "|    explained_variance   | 0.308      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 7.86       |\n",
            "|    n_updates            | 220        |\n",
            "|    policy_gradient_loss | -0.000826  |\n",
            "|    value_loss           | 16.3       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 13           |\n",
            "|    time_elapsed         | 144          |\n",
            "|    total_timesteps      | 53248        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011938323 |\n",
            "|    clip_fraction        | 0.00929      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 0.338        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 9.12         |\n",
            "|    n_updates            | 240          |\n",
            "|    policy_gradient_loss | -0.00153     |\n",
            "|    value_loss           | 16.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 155          |\n",
            "|    total_timesteps      | 57344        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013683506 |\n",
            "|    clip_fraction        | 0.0114       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 0.431        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.98         |\n",
            "|    n_updates            | 260          |\n",
            "|    policy_gradient_loss | -0.00183     |\n",
            "|    value_loss           | 14.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 166          |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008498293 |\n",
            "|    clip_fraction        | 0.0069       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.428        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.2          |\n",
            "|    n_updates            | 280          |\n",
            "|    policy_gradient_loss | -0.00118     |\n",
            "|    value_loss           | 15.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 32.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 16           |\n",
            "|    time_elapsed         | 177          |\n",
            "|    total_timesteps      | 65536        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014334938 |\n",
            "|    clip_fraction        | 0.00742      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 0.526        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.03         |\n",
            "|    n_updates            | 300          |\n",
            "|    policy_gradient_loss | -0.00115     |\n",
            "|    value_loss           | 12.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 188          |\n",
            "|    total_timesteps      | 69632        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015390243 |\n",
            "|    clip_fraction        | 0.0126       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 0.443        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.97         |\n",
            "|    n_updates            | 320          |\n",
            "|    policy_gradient_loss | -0.00172     |\n",
            "|    value_loss           | 15.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 18           |\n",
            "|    time_elapsed         | 199          |\n",
            "|    total_timesteps      | 73728        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013410173 |\n",
            "|    clip_fraction        | 0.0133       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.368        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 8.73         |\n",
            "|    n_updates            | 340          |\n",
            "|    policy_gradient_loss | -0.00197     |\n",
            "|    value_loss           | 16.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 38.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 210          |\n",
            "|    total_timesteps      | 77824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011242372 |\n",
            "|    clip_fraction        | 0.014        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.409        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.97         |\n",
            "|    n_updates            | 360          |\n",
            "|    policy_gradient_loss | -0.00196     |\n",
            "|    value_loss           | 15.7         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 125         |\n",
            "|    ep_rew_mean          | 37.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 20          |\n",
            "|    time_elapsed         | 221         |\n",
            "|    total_timesteps      | 81920       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000921871 |\n",
            "|    clip_fraction        | 0.0148      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.312       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 8.14        |\n",
            "|    n_updates            | 380         |\n",
            "|    policy_gradient_loss | -0.00188    |\n",
            "|    value_loss           | 17.2        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 21           |\n",
            "|    time_elapsed         | 233          |\n",
            "|    total_timesteps      | 86016        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016234468 |\n",
            "|    clip_fraction        | 0.0249       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.451        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.05         |\n",
            "|    n_updates            | 400          |\n",
            "|    policy_gradient_loss | -0.00248     |\n",
            "|    value_loss           | 13.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 112          |\n",
            "|    ep_rew_mean          | 31.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 22           |\n",
            "|    time_elapsed         | 244          |\n",
            "|    total_timesteps      | 90112        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015666168 |\n",
            "|    clip_fraction        | 0.0259       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.446        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.26         |\n",
            "|    n_updates            | 420          |\n",
            "|    policy_gradient_loss | -0.00244     |\n",
            "|    value_loss           | 15.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 32.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 255          |\n",
            "|    total_timesteps      | 94208        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011629435 |\n",
            "|    clip_fraction        | 0.0211       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 0.438        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.5          |\n",
            "|    n_updates            | 440          |\n",
            "|    policy_gradient_loss | -0.00199     |\n",
            "|    value_loss           | 15.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 32.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 266          |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015810552 |\n",
            "|    clip_fraction        | 0.0264       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.276        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 8.37         |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | -0.00304     |\n",
            "|    value_loss           | 17.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 35.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 25           |\n",
            "|    time_elapsed         | 277          |\n",
            "|    total_timesteps      | 102400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012076504 |\n",
            "|    clip_fraction        | 0.0276       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 0.406        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.85         |\n",
            "|    n_updates            | 480          |\n",
            "|    policy_gradient_loss | -0.00288     |\n",
            "|    value_loss           | 13           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 26           |\n",
            "|    time_elapsed         | 288          |\n",
            "|    total_timesteps      | 106496       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011995167 |\n",
            "|    clip_fraction        | 0.0182       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.358        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.75         |\n",
            "|    n_updates            | 500          |\n",
            "|    policy_gradient_loss | -0.00213     |\n",
            "|    value_loss           | 14.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 27           |\n",
            "|    time_elapsed         | 299          |\n",
            "|    total_timesteps      | 110592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014717176 |\n",
            "|    clip_fraction        | 0.0288       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 0.42         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.15         |\n",
            "|    n_updates            | 520          |\n",
            "|    policy_gradient_loss | -0.00288     |\n",
            "|    value_loss           | 13.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 28           |\n",
            "|    time_elapsed         | 310          |\n",
            "|    total_timesteps      | 114688       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013687124 |\n",
            "|    clip_fraction        | 0.0345       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.374        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.36         |\n",
            "|    n_updates            | 540          |\n",
            "|    policy_gradient_loss | -0.00227     |\n",
            "|    value_loss           | 15.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 321          |\n",
            "|    total_timesteps      | 118784       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010771687 |\n",
            "|    clip_fraction        | 0.00955      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.361        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.71         |\n",
            "|    n_updates            | 560          |\n",
            "|    policy_gradient_loss | -0.00178     |\n",
            "|    value_loss           | 15.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 332          |\n",
            "|    total_timesteps      | 122880       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014416007 |\n",
            "|    clip_fraction        | 0.0215       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.311        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.07         |\n",
            "|    n_updates            | 580          |\n",
            "|    policy_gradient_loss | -0.00238     |\n",
            "|    value_loss           | 15.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 36.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 31           |\n",
            "|    time_elapsed         | 343          |\n",
            "|    total_timesteps      | 126976       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011904803 |\n",
            "|    clip_fraction        | 0.0207       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.223        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 9.73         |\n",
            "|    n_updates            | 600          |\n",
            "|    policy_gradient_loss | -0.00236     |\n",
            "|    value_loss           | 17.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 355          |\n",
            "|    total_timesteps      | 131072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009737784 |\n",
            "|    clip_fraction        | 0.0125       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.407        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.92         |\n",
            "|    n_updates            | 620          |\n",
            "|    policy_gradient_loss | -0.00208     |\n",
            "|    value_loss           | 13.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 366          |\n",
            "|    total_timesteps      | 135168       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010180981 |\n",
            "|    clip_fraction        | 0.0139       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.424        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.04         |\n",
            "|    n_updates            | 640          |\n",
            "|    policy_gradient_loss | -0.00202     |\n",
            "|    value_loss           | 15.1         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 121        |\n",
            "|    ep_rew_mean          | 34.7       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 368        |\n",
            "|    iterations           | 34         |\n",
            "|    time_elapsed         | 377        |\n",
            "|    total_timesteps      | 139264     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00134876 |\n",
            "|    clip_fraction        | 0.0385     |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -1.22      |\n",
            "|    explained_variance   | 0.417      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 7.31       |\n",
            "|    n_updates            | 660        |\n",
            "|    policy_gradient_loss | -0.00339   |\n",
            "|    value_loss           | 14.9       |\n",
            "----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 117         |\n",
            "|    ep_rew_mean          | 33.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 35          |\n",
            "|    time_elapsed         | 388         |\n",
            "|    total_timesteps      | 143360      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001004508 |\n",
            "|    clip_fraction        | 0.0188      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.497       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 6.36        |\n",
            "|    n_updates            | 680         |\n",
            "|    policy_gradient_loss | -0.00281    |\n",
            "|    value_loss           | 12.9        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 31.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 36           |\n",
            "|    time_elapsed         | 400          |\n",
            "|    total_timesteps      | 147456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013876597 |\n",
            "|    clip_fraction        | 0.0274       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.411        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.31         |\n",
            "|    n_updates            | 700          |\n",
            "|    policy_gradient_loss | -0.00245     |\n",
            "|    value_loss           | 15.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 32.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 37           |\n",
            "|    time_elapsed         | 411          |\n",
            "|    total_timesteps      | 151552       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016091296 |\n",
            "|    clip_fraction        | 0.0491       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.546        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.14         |\n",
            "|    n_updates            | 720          |\n",
            "|    policy_gradient_loss | -0.00339     |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 33.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 38           |\n",
            "|    time_elapsed         | 422          |\n",
            "|    total_timesteps      | 155648       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016904734 |\n",
            "|    clip_fraction        | 0.0405       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.415        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.6          |\n",
            "|    n_updates            | 740          |\n",
            "|    policy_gradient_loss | -0.00299     |\n",
            "|    value_loss           | 14.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 433          |\n",
            "|    total_timesteps      | 159744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013883924 |\n",
            "|    clip_fraction        | 0.0247       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.337        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 8.29         |\n",
            "|    n_updates            | 760          |\n",
            "|    policy_gradient_loss | -0.00227     |\n",
            "|    value_loss           | 16.8         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 114           |\n",
            "|    ep_rew_mean          | 32.2          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 368           |\n",
            "|    iterations           | 40            |\n",
            "|    time_elapsed         | 444           |\n",
            "|    total_timesteps      | 163840        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00097103196 |\n",
            "|    clip_fraction        | 0.0187        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.417         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 5.85          |\n",
            "|    n_updates            | 780           |\n",
            "|    policy_gradient_loss | -0.00221      |\n",
            "|    value_loss           | 13.7          |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 32.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 456          |\n",
            "|    total_timesteps      | 167936       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012770146 |\n",
            "|    clip_fraction        | 0.025        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.456        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.63         |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | -0.00269     |\n",
            "|    value_loss           | 12.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 42           |\n",
            "|    time_elapsed         | 467          |\n",
            "|    total_timesteps      | 172032       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013310243 |\n",
            "|    clip_fraction        | 0.0267       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.407        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.35         |\n",
            "|    n_updates            | 820          |\n",
            "|    policy_gradient_loss | -0.00258     |\n",
            "|    value_loss           | 13.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 37.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 43           |\n",
            "|    time_elapsed         | 478          |\n",
            "|    total_timesteps      | 176128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015902962 |\n",
            "|    clip_fraction        | 0.0229       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.41         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.2          |\n",
            "|    n_updates            | 840          |\n",
            "|    policy_gradient_loss | -0.00268     |\n",
            "|    value_loss           | 14.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 37           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 44           |\n",
            "|    time_elapsed         | 489          |\n",
            "|    total_timesteps      | 180224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013218538 |\n",
            "|    clip_fraction        | 0.0173       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.351        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.71         |\n",
            "|    n_updates            | 860          |\n",
            "|    policy_gradient_loss | -0.0013      |\n",
            "|    value_loss           | 16.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 45           |\n",
            "|    time_elapsed         | 500          |\n",
            "|    total_timesteps      | 184320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011721316 |\n",
            "|    clip_fraction        | 0.0301       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.413        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.69         |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -0.00274     |\n",
            "|    value_loss           | 14.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 46           |\n",
            "|    time_elapsed         | 512          |\n",
            "|    total_timesteps      | 188416       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016811254 |\n",
            "|    clip_fraction        | 0.0493       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.424        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.47         |\n",
            "|    n_updates            | 900          |\n",
            "|    policy_gradient_loss | -0.00309     |\n",
            "|    value_loss           | 14.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 33.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 47           |\n",
            "|    time_elapsed         | 523          |\n",
            "|    total_timesteps      | 192512       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015463298 |\n",
            "|    clip_fraction        | 0.0421       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.46         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.69         |\n",
            "|    n_updates            | 920          |\n",
            "|    policy_gradient_loss | -0.00358     |\n",
            "|    value_loss           | 14.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 48           |\n",
            "|    time_elapsed         | 534          |\n",
            "|    total_timesteps      | 196608       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009607316 |\n",
            "|    clip_fraction        | 0.00994      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.468        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.99         |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | -0.00174     |\n",
            "|    value_loss           | 13.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 49           |\n",
            "|    time_elapsed         | 545          |\n",
            "|    total_timesteps      | 200704       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015263481 |\n",
            "|    clip_fraction        | 0.0296       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.418        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.77         |\n",
            "|    n_updates            | 960          |\n",
            "|    policy_gradient_loss | -0.00282     |\n",
            "|    value_loss           | 15.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 50           |\n",
            "|    time_elapsed         | 556          |\n",
            "|    total_timesteps      | 204800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014130477 |\n",
            "|    clip_fraction        | 0.0275       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.483        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.87         |\n",
            "|    n_updates            | 980          |\n",
            "|    policy_gradient_loss | -0.00259     |\n",
            "|    value_loss           | 13.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 51           |\n",
            "|    time_elapsed         | 567          |\n",
            "|    total_timesteps      | 208896       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011568388 |\n",
            "|    clip_fraction        | 0.0178       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.386        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 8.5          |\n",
            "|    n_updates            | 1000         |\n",
            "|    policy_gradient_loss | -0.00177     |\n",
            "|    value_loss           | 15.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 52           |\n",
            "|    time_elapsed         | 578          |\n",
            "|    total_timesteps      | 212992       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012169775 |\n",
            "|    clip_fraction        | 0.0224       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.291        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.8          |\n",
            "|    n_updates            | 1020         |\n",
            "|    policy_gradient_loss | -0.00184     |\n",
            "|    value_loss           | 16.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 35.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 53           |\n",
            "|    time_elapsed         | 589          |\n",
            "|    total_timesteps      | 217088       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010926656 |\n",
            "|    clip_fraction        | 0.017        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.427        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.55         |\n",
            "|    n_updates            | 1040         |\n",
            "|    policy_gradient_loss | -0.00187     |\n",
            "|    value_loss           | 12.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 54           |\n",
            "|    time_elapsed         | 600          |\n",
            "|    total_timesteps      | 221184       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012597372 |\n",
            "|    clip_fraction        | 0.0188       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.35         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.18         |\n",
            "|    n_updates            | 1060         |\n",
            "|    policy_gradient_loss | -0.00258     |\n",
            "|    value_loss           | 15.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 36.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 55           |\n",
            "|    time_elapsed         | 611          |\n",
            "|    total_timesteps      | 225280       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010651326 |\n",
            "|    clip_fraction        | 0.0195       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.418        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6            |\n",
            "|    n_updates            | 1080         |\n",
            "|    policy_gradient_loss | -0.00274     |\n",
            "|    value_loss           | 13.2         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 122         |\n",
            "|    ep_rew_mean          | 36.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 622         |\n",
            "|    total_timesteps      | 229376      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001560038 |\n",
            "|    clip_fraction        | 0.0361      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.231       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 8.48        |\n",
            "|    n_updates            | 1100        |\n",
            "|    policy_gradient_loss | -0.00284    |\n",
            "|    value_loss           | 17.9        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 35.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 57           |\n",
            "|    time_elapsed         | 633          |\n",
            "|    total_timesteps      | 233472       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017145637 |\n",
            "|    clip_fraction        | 0.0419       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.26         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 9.22         |\n",
            "|    n_updates            | 1120         |\n",
            "|    policy_gradient_loss | -0.00363     |\n",
            "|    value_loss           | 17.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 36.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 58           |\n",
            "|    time_elapsed         | 644          |\n",
            "|    total_timesteps      | 237568       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014024315 |\n",
            "|    clip_fraction        | 0.0255       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.364        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.03         |\n",
            "|    n_updates            | 1140         |\n",
            "|    policy_gradient_loss | -0.00274     |\n",
            "|    value_loss           | 13.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 35.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 59           |\n",
            "|    time_elapsed         | 655          |\n",
            "|    total_timesteps      | 241664       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012712495 |\n",
            "|    clip_fraction        | 0.0249       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.394        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.17         |\n",
            "|    n_updates            | 1160         |\n",
            "|    policy_gradient_loss | -0.00203     |\n",
            "|    value_loss           | 15.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 38.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 60           |\n",
            "|    time_elapsed         | 666          |\n",
            "|    total_timesteps      | 245760       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012949955 |\n",
            "|    clip_fraction        | 0.0138       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.348        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 9.07         |\n",
            "|    n_updates            | 1180         |\n",
            "|    policy_gradient_loss | -0.00205     |\n",
            "|    value_loss           | 17.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 37.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 678          |\n",
            "|    total_timesteps      | 249856       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012437605 |\n",
            "|    clip_fraction        | 0.0172       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.284        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.52         |\n",
            "|    n_updates            | 1200         |\n",
            "|    policy_gradient_loss | -0.00219     |\n",
            "|    value_loss           | 17.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 37.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 689          |\n",
            "|    total_timesteps      | 253952       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011693172 |\n",
            "|    clip_fraction        | 0.0138       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.344        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.85         |\n",
            "|    n_updates            | 1220         |\n",
            "|    policy_gradient_loss | -0.00281     |\n",
            "|    value_loss           | 16.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 34           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 63           |\n",
            "|    time_elapsed         | 700          |\n",
            "|    total_timesteps      | 258048       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012181425 |\n",
            "|    clip_fraction        | 0.0198       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.378        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 8.52         |\n",
            "|    n_updates            | 1240         |\n",
            "|    policy_gradient_loss | -0.00196     |\n",
            "|    value_loss           | 15.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 31.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 64           |\n",
            "|    time_elapsed         | 711          |\n",
            "|    total_timesteps      | 262144       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013552315 |\n",
            "|    clip_fraction        | 0.0228       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.363        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.24         |\n",
            "|    n_updates            | 1260         |\n",
            "|    policy_gradient_loss | -0.00302     |\n",
            "|    value_loss           | 15.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 65           |\n",
            "|    time_elapsed         | 722          |\n",
            "|    total_timesteps      | 266240       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013145884 |\n",
            "|    clip_fraction        | 0.0166       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.496        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.11         |\n",
            "|    n_updates            | 1280         |\n",
            "|    policy_gradient_loss | -0.00205     |\n",
            "|    value_loss           | 13.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 33.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 66           |\n",
            "|    time_elapsed         | 733          |\n",
            "|    total_timesteps      | 270336       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018552397 |\n",
            "|    clip_fraction        | 0.0441       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.474        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.68         |\n",
            "|    n_updates            | 1300         |\n",
            "|    policy_gradient_loss | -0.00351     |\n",
            "|    value_loss           | 13.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 67           |\n",
            "|    time_elapsed         | 744          |\n",
            "|    total_timesteps      | 274432       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012451687 |\n",
            "|    clip_fraction        | 0.0236       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.512        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.15         |\n",
            "|    n_updates            | 1320         |\n",
            "|    policy_gradient_loss | -0.00219     |\n",
            "|    value_loss           | 12           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 68           |\n",
            "|    time_elapsed         | 755          |\n",
            "|    total_timesteps      | 278528       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013389026 |\n",
            "|    clip_fraction        | 0.0327       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.466        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.08         |\n",
            "|    n_updates            | 1340         |\n",
            "|    policy_gradient_loss | -0.00292     |\n",
            "|    value_loss           | 13.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 69           |\n",
            "|    time_elapsed         | 766          |\n",
            "|    total_timesteps      | 282624       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012503627 |\n",
            "|    clip_fraction        | 0.0225       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.414        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.07         |\n",
            "|    n_updates            | 1360         |\n",
            "|    policy_gradient_loss | -0.00261     |\n",
            "|    value_loss           | 14.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 37.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 70           |\n",
            "|    time_elapsed         | 777          |\n",
            "|    total_timesteps      | 286720       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015005944 |\n",
            "|    clip_fraction        | 0.0437       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.387        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.24         |\n",
            "|    n_updates            | 1380         |\n",
            "|    policy_gradient_loss | -0.00353     |\n",
            "|    value_loss           | 13.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 38.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 788          |\n",
            "|    total_timesteps      | 290816       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015743909 |\n",
            "|    clip_fraction        | 0.0309       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.333        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.5          |\n",
            "|    n_updates            | 1400         |\n",
            "|    policy_gradient_loss | -0.00247     |\n",
            "|    value_loss           | 14.9         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 128         |\n",
            "|    ep_rew_mean          | 38.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 72          |\n",
            "|    time_elapsed         | 799         |\n",
            "|    total_timesteps      | 294912      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001131718 |\n",
            "|    clip_fraction        | 0.0172      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.368       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 6.56        |\n",
            "|    n_updates            | 1420        |\n",
            "|    policy_gradient_loss | -0.00292    |\n",
            "|    value_loss           | 14.7        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 36.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 73           |\n",
            "|    time_elapsed         | 810          |\n",
            "|    total_timesteps      | 299008       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016295384 |\n",
            "|    clip_fraction        | 0.044        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.296        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.79         |\n",
            "|    n_updates            | 1440         |\n",
            "|    policy_gradient_loss | -0.00318     |\n",
            "|    value_loss           | 16.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 36.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 74           |\n",
            "|    time_elapsed         | 821          |\n",
            "|    total_timesteps      | 303104       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015221937 |\n",
            "|    clip_fraction        | 0.0209       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.358        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.94         |\n",
            "|    n_updates            | 1460         |\n",
            "|    policy_gradient_loss | -0.00254     |\n",
            "|    value_loss           | 15.8         |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 122           |\n",
            "|    ep_rew_mean          | 35.7          |\n",
            "| time/                   |               |\n",
            "|    fps                  | 368           |\n",
            "|    iterations           | 75            |\n",
            "|    time_elapsed         | 832           |\n",
            "|    total_timesteps      | 307200        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00094246696 |\n",
            "|    clip_fraction        | 0.00885       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.472         |\n",
            "|    learning_rate        | 0.0001        |\n",
            "|    loss                 | 5.98          |\n",
            "|    n_updates            | 1480          |\n",
            "|    policy_gradient_loss | -0.00131      |\n",
            "|    value_loss           | 13            |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 36.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 76           |\n",
            "|    time_elapsed         | 843          |\n",
            "|    total_timesteps      | 311296       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015895645 |\n",
            "|    clip_fraction        | 0.0252       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.442        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.28         |\n",
            "|    n_updates            | 1500         |\n",
            "|    policy_gradient_loss | -0.00235     |\n",
            "|    value_loss           | 15.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 36.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 77           |\n",
            "|    time_elapsed         | 854          |\n",
            "|    total_timesteps      | 315392       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012914855 |\n",
            "|    clip_fraction        | 0.0241       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.334        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 8.06         |\n",
            "|    n_updates            | 1520         |\n",
            "|    policy_gradient_loss | -0.00266     |\n",
            "|    value_loss           | 17.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 32.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 78           |\n",
            "|    time_elapsed         | 865          |\n",
            "|    total_timesteps      | 319488       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016241859 |\n",
            "|    clip_fraction        | 0.0274       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.391        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.43         |\n",
            "|    n_updates            | 1540         |\n",
            "|    policy_gradient_loss | -0.00271     |\n",
            "|    value_loss           | 15.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 79           |\n",
            "|    time_elapsed         | 876          |\n",
            "|    total_timesteps      | 323584       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013450251 |\n",
            "|    clip_fraction        | 0.027        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 0.418        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 8.25         |\n",
            "|    n_updates            | 1560         |\n",
            "|    policy_gradient_loss | -0.00374     |\n",
            "|    value_loss           | 14.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 31.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 80           |\n",
            "|    time_elapsed         | 887          |\n",
            "|    total_timesteps      | 327680       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013469779 |\n",
            "|    clip_fraction        | 0.02         |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.373        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.29         |\n",
            "|    n_updates            | 1580         |\n",
            "|    policy_gradient_loss | -0.00178     |\n",
            "|    value_loss           | 15           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 81           |\n",
            "|    time_elapsed         | 898          |\n",
            "|    total_timesteps      | 331776       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013321834 |\n",
            "|    clip_fraction        | 0.0317       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.458        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.28         |\n",
            "|    n_updates            | 1600         |\n",
            "|    policy_gradient_loss | -0.00265     |\n",
            "|    value_loss           | 13.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 34.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 82           |\n",
            "|    time_elapsed         | 909          |\n",
            "|    total_timesteps      | 335872       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013064346 |\n",
            "|    clip_fraction        | 0.0341       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.394        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.45         |\n",
            "|    n_updates            | 1620         |\n",
            "|    policy_gradient_loss | -0.00419     |\n",
            "|    value_loss           | 14.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 37           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 83           |\n",
            "|    time_elapsed         | 920          |\n",
            "|    total_timesteps      | 339968       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016942447 |\n",
            "|    clip_fraction        | 0.0447       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.428        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.79         |\n",
            "|    n_updates            | 1640         |\n",
            "|    policy_gradient_loss | -0.00375     |\n",
            "|    value_loss           | 13.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 38           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 84           |\n",
            "|    time_elapsed         | 932          |\n",
            "|    total_timesteps      | 344064       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014457664 |\n",
            "|    clip_fraction        | 0.0238       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.4          |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.93         |\n",
            "|    n_updates            | 1660         |\n",
            "|    policy_gradient_loss | -0.00279     |\n",
            "|    value_loss           | 14.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 36.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 85           |\n",
            "|    time_elapsed         | 943          |\n",
            "|    total_timesteps      | 348160       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016211132 |\n",
            "|    clip_fraction        | 0.0282       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.396        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.75         |\n",
            "|    n_updates            | 1680         |\n",
            "|    policy_gradient_loss | -0.00233     |\n",
            "|    value_loss           | 15.1         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 121         |\n",
            "|    ep_rew_mean          | 35.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 86          |\n",
            "|    time_elapsed         | 954         |\n",
            "|    total_timesteps      | 352256      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001318348 |\n",
            "|    clip_fraction        | 0.0332      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.367       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 7.18        |\n",
            "|    n_updates            | 1700        |\n",
            "|    policy_gradient_loss | -0.00322    |\n",
            "|    value_loss           | 15.7        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 87           |\n",
            "|    time_elapsed         | 965          |\n",
            "|    total_timesteps      | 356352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012496498 |\n",
            "|    clip_fraction        | 0.0234       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.466        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.43         |\n",
            "|    n_updates            | 1720         |\n",
            "|    policy_gradient_loss | -0.00251     |\n",
            "|    value_loss           | 14           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 88           |\n",
            "|    time_elapsed         | 976          |\n",
            "|    total_timesteps      | 360448       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013347947 |\n",
            "|    clip_fraction        | 0.021        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.613        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.6          |\n",
            "|    n_updates            | 1740         |\n",
            "|    policy_gradient_loss | -0.00242     |\n",
            "|    value_loss           | 10.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 987          |\n",
            "|    total_timesteps      | 364544       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013742936 |\n",
            "|    clip_fraction        | 0.0336       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.523        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.8          |\n",
            "|    n_updates            | 1760         |\n",
            "|    policy_gradient_loss | -0.00273     |\n",
            "|    value_loss           | 13.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 37.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 998          |\n",
            "|    total_timesteps      | 368640       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016027752 |\n",
            "|    clip_fraction        | 0.0327       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.467        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.66         |\n",
            "|    n_updates            | 1780         |\n",
            "|    policy_gradient_loss | -0.00235     |\n",
            "|    value_loss           | 13.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 37.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 91           |\n",
            "|    time_elapsed         | 1009         |\n",
            "|    total_timesteps      | 372736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012762311 |\n",
            "|    clip_fraction        | 0.0207       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.409        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.33         |\n",
            "|    n_updates            | 1800         |\n",
            "|    policy_gradient_loss | -0.00208     |\n",
            "|    value_loss           | 13.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 36.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 92           |\n",
            "|    time_elapsed         | 1020         |\n",
            "|    total_timesteps      | 376832       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016086784 |\n",
            "|    clip_fraction        | 0.0398       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.389        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.04         |\n",
            "|    n_updates            | 1820         |\n",
            "|    policy_gradient_loss | -0.00397     |\n",
            "|    value_loss           | 14.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 36.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 93           |\n",
            "|    time_elapsed         | 1031         |\n",
            "|    total_timesteps      | 380928       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010458797 |\n",
            "|    clip_fraction        | 0.0167       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.275        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 8.64         |\n",
            "|    n_updates            | 1840         |\n",
            "|    policy_gradient_loss | -0.00196     |\n",
            "|    value_loss           | 17.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 38.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 94           |\n",
            "|    time_elapsed         | 1043         |\n",
            "|    total_timesteps      | 385024       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016089707 |\n",
            "|    clip_fraction        | 0.0392       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.311        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 8.11         |\n",
            "|    n_updates            | 1860         |\n",
            "|    policy_gradient_loss | -0.00377     |\n",
            "|    value_loss           | 16.6         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 121         |\n",
            "|    ep_rew_mean          | 35.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 1054        |\n",
            "|    total_timesteps      | 389120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001104421 |\n",
            "|    clip_fraction        | 0.0141      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.381       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 7.63        |\n",
            "|    n_updates            | 1880        |\n",
            "|    policy_gradient_loss | -0.00173    |\n",
            "|    value_loss           | 14.9        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 96           |\n",
            "|    time_elapsed         | 1065         |\n",
            "|    total_timesteps      | 393216       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014039588 |\n",
            "|    clip_fraction        | 0.0177       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.355        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.76         |\n",
            "|    n_updates            | 1900         |\n",
            "|    policy_gradient_loss | -0.00226     |\n",
            "|    value_loss           | 15.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 97           |\n",
            "|    time_elapsed         | 1076         |\n",
            "|    total_timesteps      | 397312       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012888298 |\n",
            "|    clip_fraction        | 0.0243       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.494        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.3          |\n",
            "|    n_updates            | 1920         |\n",
            "|    policy_gradient_loss | -0.00229     |\n",
            "|    value_loss           | 13.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 37.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 98           |\n",
            "|    time_elapsed         | 1087         |\n",
            "|    total_timesteps      | 401408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014715984 |\n",
            "|    clip_fraction        | 0.0269       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.447        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.44         |\n",
            "|    n_updates            | 1940         |\n",
            "|    policy_gradient_loss | -0.00191     |\n",
            "|    value_loss           | 14.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 37.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 99           |\n",
            "|    time_elapsed         | 1098         |\n",
            "|    total_timesteps      | 405504       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018762964 |\n",
            "|    clip_fraction        | 0.0383       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.368        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.94         |\n",
            "|    n_updates            | 1960         |\n",
            "|    policy_gradient_loss | -0.00314     |\n",
            "|    value_loss           | 16           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 36.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 100          |\n",
            "|    time_elapsed         | 1110         |\n",
            "|    total_timesteps      | 409600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015952461 |\n",
            "|    clip_fraction        | 0.0389       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.387        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.68         |\n",
            "|    n_updates            | 1980         |\n",
            "|    policy_gradient_loss | -0.00392     |\n",
            "|    value_loss           | 15.3         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 122         |\n",
            "|    ep_rew_mean          | 36          |\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 101         |\n",
            "|    time_elapsed         | 1121        |\n",
            "|    total_timesteps      | 413696      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001471692 |\n",
            "|    clip_fraction        | 0.0257      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0.462       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 6.25        |\n",
            "|    n_updates            | 2000        |\n",
            "|    policy_gradient_loss | -0.00253    |\n",
            "|    value_loss           | 12.8        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 102          |\n",
            "|    time_elapsed         | 1132         |\n",
            "|    total_timesteps      | 417792       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013578227 |\n",
            "|    clip_fraction        | 0.025        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.376        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.47         |\n",
            "|    n_updates            | 2020         |\n",
            "|    policy_gradient_loss | -0.00255     |\n",
            "|    value_loss           | 14.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 103          |\n",
            "|    time_elapsed         | 1143         |\n",
            "|    total_timesteps      | 421888       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012943398 |\n",
            "|    clip_fraction        | 0.0294       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.412        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.75         |\n",
            "|    n_updates            | 2040         |\n",
            "|    policy_gradient_loss | -0.0026      |\n",
            "|    value_loss           | 14.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 104          |\n",
            "|    time_elapsed         | 1154         |\n",
            "|    total_timesteps      | 425984       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015493494 |\n",
            "|    clip_fraction        | 0.0367       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.426        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.64         |\n",
            "|    n_updates            | 2060         |\n",
            "|    policy_gradient_loss | -0.00363     |\n",
            "|    value_loss           | 14.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 105          |\n",
            "|    time_elapsed         | 1165         |\n",
            "|    total_timesteps      | 430080       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014695767 |\n",
            "|    clip_fraction        | 0.0326       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.541        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.17         |\n",
            "|    n_updates            | 2080         |\n",
            "|    policy_gradient_loss | -0.00312     |\n",
            "|    value_loss           | 11.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 106          |\n",
            "|    time_elapsed         | 1176         |\n",
            "|    total_timesteps      | 434176       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015032657 |\n",
            "|    clip_fraction        | 0.0451       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.605        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.42         |\n",
            "|    n_updates            | 2100         |\n",
            "|    policy_gradient_loss | -0.00336     |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 35           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 107          |\n",
            "|    time_elapsed         | 1187         |\n",
            "|    total_timesteps      | 438272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012800291 |\n",
            "|    clip_fraction        | 0.0271       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.478        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.02         |\n",
            "|    n_updates            | 2120         |\n",
            "|    policy_gradient_loss | -0.0026      |\n",
            "|    value_loss           | 12.8         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 124         |\n",
            "|    ep_rew_mean          | 37.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 108         |\n",
            "|    time_elapsed         | 1198        |\n",
            "|    total_timesteps      | 442368      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001158362 |\n",
            "|    clip_fraction        | 0.0166      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0.411       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 7.34        |\n",
            "|    n_updates            | 2140        |\n",
            "|    policy_gradient_loss | -0.00216    |\n",
            "|    value_loss           | 15.2        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 35.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 109          |\n",
            "|    time_elapsed         | 1210         |\n",
            "|    total_timesteps      | 446464       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013982217 |\n",
            "|    clip_fraction        | 0.0228       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.46         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.53         |\n",
            "|    n_updates            | 2160         |\n",
            "|    policy_gradient_loss | -0.00232     |\n",
            "|    value_loss           | 13.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 110          |\n",
            "|    time_elapsed         | 1221         |\n",
            "|    total_timesteps      | 450560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010076517 |\n",
            "|    clip_fraction        | 0.0162       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.53         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.03         |\n",
            "|    n_updates            | 2180         |\n",
            "|    policy_gradient_loss | -0.00204     |\n",
            "|    value_loss           | 11           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 31.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 111          |\n",
            "|    time_elapsed         | 1232         |\n",
            "|    total_timesteps      | 454656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009398479 |\n",
            "|    clip_fraction        | 0.00977      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.52         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.24         |\n",
            "|    n_updates            | 2200         |\n",
            "|    policy_gradient_loss | -0.00227     |\n",
            "|    value_loss           | 12.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 109          |\n",
            "|    ep_rew_mean          | 30           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 112          |\n",
            "|    time_elapsed         | 1243         |\n",
            "|    total_timesteps      | 458752       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015080038 |\n",
            "|    clip_fraction        | 0.0302       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.564        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.29         |\n",
            "|    n_updates            | 2220         |\n",
            "|    policy_gradient_loss | -0.00292     |\n",
            "|    value_loss           | 11.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 109          |\n",
            "|    ep_rew_mean          | 30.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 113          |\n",
            "|    time_elapsed         | 1254         |\n",
            "|    total_timesteps      | 462848       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017152423 |\n",
            "|    clip_fraction        | 0.0544       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.464        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.82         |\n",
            "|    n_updates            | 2240         |\n",
            "|    policy_gradient_loss | -0.00421     |\n",
            "|    value_loss           | 13           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 32.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 114          |\n",
            "|    time_elapsed         | 1265         |\n",
            "|    total_timesteps      | 466944       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014510589 |\n",
            "|    clip_fraction        | 0.0315       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 0.542        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.94         |\n",
            "|    n_updates            | 2260         |\n",
            "|    policy_gradient_loss | -0.00333     |\n",
            "|    value_loss           | 10.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 115          |\n",
            "|    time_elapsed         | 1276         |\n",
            "|    total_timesteps      | 471040       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013975329 |\n",
            "|    clip_fraction        | 0.0231       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.509        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.14         |\n",
            "|    n_updates            | 2280         |\n",
            "|    policy_gradient_loss | -0.00331     |\n",
            "|    value_loss           | 11.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 36.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 116          |\n",
            "|    time_elapsed         | 1287         |\n",
            "|    total_timesteps      | 475136       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012499718 |\n",
            "|    clip_fraction        | 0.0317       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.482        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.57         |\n",
            "|    n_updates            | 2300         |\n",
            "|    policy_gradient_loss | -0.00366     |\n",
            "|    value_loss           | 11.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 37.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 117          |\n",
            "|    time_elapsed         | 1298         |\n",
            "|    total_timesteps      | 479232       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014727567 |\n",
            "|    clip_fraction        | 0.0278       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.51         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.61         |\n",
            "|    n_updates            | 2320         |\n",
            "|    policy_gradient_loss | -0.00307     |\n",
            "|    value_loss           | 11.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 36.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 118          |\n",
            "|    time_elapsed         | 1309         |\n",
            "|    total_timesteps      | 483328       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010242681 |\n",
            "|    clip_fraction        | 0.0151       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.327        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.47         |\n",
            "|    n_updates            | 2340         |\n",
            "|    policy_gradient_loss | -0.00151     |\n",
            "|    value_loss           | 14.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 34.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 119          |\n",
            "|    time_elapsed         | 1320         |\n",
            "|    total_timesteps      | 487424       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015773473 |\n",
            "|    clip_fraction        | 0.0329       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.339        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.39         |\n",
            "|    n_updates            | 2360         |\n",
            "|    policy_gradient_loss | -0.00369     |\n",
            "|    value_loss           | 16           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 120          |\n",
            "|    time_elapsed         | 1331         |\n",
            "|    total_timesteps      | 491520       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013347282 |\n",
            "|    clip_fraction        | 0.0198       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.33         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.72         |\n",
            "|    n_updates            | 2380         |\n",
            "|    policy_gradient_loss | -0.00245     |\n",
            "|    value_loss           | 14.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 121          |\n",
            "|    time_elapsed         | 1342         |\n",
            "|    total_timesteps      | 495616       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015925879 |\n",
            "|    clip_fraction        | 0.0317       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.458        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.07         |\n",
            "|    n_updates            | 2400         |\n",
            "|    policy_gradient_loss | -0.00369     |\n",
            "|    value_loss           | 12.6         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 124         |\n",
            "|    ep_rew_mean          | 36.6        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 122         |\n",
            "|    time_elapsed         | 1353        |\n",
            "|    total_timesteps      | 499712      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001336908 |\n",
            "|    clip_fraction        | 0.0239      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.402       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 6.39        |\n",
            "|    n_updates            | 2420        |\n",
            "|    policy_gradient_loss | -0.00279    |\n",
            "|    value_loss           | 13.4        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 127          |\n",
            "|    ep_rew_mean          | 38.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 123          |\n",
            "|    time_elapsed         | 1365         |\n",
            "|    total_timesteps      | 503808       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012461366 |\n",
            "|    clip_fraction        | 0.0154       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.371        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.03         |\n",
            "|    n_updates            | 2440         |\n",
            "|    policy_gradient_loss | -0.00213     |\n",
            "|    value_loss           | 14.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 36.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 124          |\n",
            "|    time_elapsed         | 1376         |\n",
            "|    total_timesteps      | 507904       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013568087 |\n",
            "|    clip_fraction        | 0.0196       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.328        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.72         |\n",
            "|    n_updates            | 2460         |\n",
            "|    policy_gradient_loss | -0.00256     |\n",
            "|    value_loss           | 16.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 35           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 125          |\n",
            "|    time_elapsed         | 1387         |\n",
            "|    total_timesteps      | 512000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017142387 |\n",
            "|    clip_fraction        | 0.0403       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.437        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.98         |\n",
            "|    n_updates            | 2480         |\n",
            "|    policy_gradient_loss | -0.00345     |\n",
            "|    value_loss           | 13.8         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 121         |\n",
            "|    ep_rew_mean          | 35.4        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 369         |\n",
            "|    iterations           | 126         |\n",
            "|    time_elapsed         | 1398        |\n",
            "|    total_timesteps      | 516096      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001707616 |\n",
            "|    clip_fraction        | 0.0452      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.45        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 5.87        |\n",
            "|    n_updates            | 2500        |\n",
            "|    policy_gradient_loss | -0.00354    |\n",
            "|    value_loss           | 13.5        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 36.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 127          |\n",
            "|    time_elapsed         | 1409         |\n",
            "|    total_timesteps      | 520192       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014950121 |\n",
            "|    clip_fraction        | 0.0381       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.441        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.62         |\n",
            "|    n_updates            | 2520         |\n",
            "|    policy_gradient_loss | -0.00324     |\n",
            "|    value_loss           | 13.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 37           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 128          |\n",
            "|    time_elapsed         | 1420         |\n",
            "|    total_timesteps      | 524288       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014824208 |\n",
            "|    clip_fraction        | 0.0252       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.405        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.63         |\n",
            "|    n_updates            | 2540         |\n",
            "|    policy_gradient_loss | -0.00257     |\n",
            "|    value_loss           | 14           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 36.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 129          |\n",
            "|    time_elapsed         | 1431         |\n",
            "|    total_timesteps      | 528384       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015272098 |\n",
            "|    clip_fraction        | 0.0287       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.481        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.2          |\n",
            "|    n_updates            | 2560         |\n",
            "|    policy_gradient_loss | -0.00277     |\n",
            "|    value_loss           | 12.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 33.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 130          |\n",
            "|    time_elapsed         | 1442         |\n",
            "|    total_timesteps      | 532480       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017732792 |\n",
            "|    clip_fraction        | 0.0637       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.576        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.07         |\n",
            "|    n_updates            | 2580         |\n",
            "|    policy_gradient_loss | -0.00422     |\n",
            "|    value_loss           | 11.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 32.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 131          |\n",
            "|    time_elapsed         | 1453         |\n",
            "|    total_timesteps      | 536576       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014037124 |\n",
            "|    clip_fraction        | 0.0265       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.553        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.9          |\n",
            "|    n_updates            | 2600         |\n",
            "|    policy_gradient_loss | -0.00235     |\n",
            "|    value_loss           | 12.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 32.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 369          |\n",
            "|    iterations           | 132          |\n",
            "|    time_elapsed         | 1465         |\n",
            "|    total_timesteps      | 540672       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016983382 |\n",
            "|    clip_fraction        | 0.0322       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.566        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.21         |\n",
            "|    n_updates            | 2620         |\n",
            "|    policy_gradient_loss | -0.00266     |\n",
            "|    value_loss           | 10.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 133          |\n",
            "|    time_elapsed         | 1476         |\n",
            "|    total_timesteps      | 544768       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012714628 |\n",
            "|    clip_fraction        | 0.0153       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.471        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.51         |\n",
            "|    n_updates            | 2640         |\n",
            "|    policy_gradient_loss | -0.00282     |\n",
            "|    value_loss           | 12.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 134          |\n",
            "|    time_elapsed         | 1487         |\n",
            "|    total_timesteps      | 548864       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015815346 |\n",
            "|    clip_fraction        | 0.0351       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.454        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.26         |\n",
            "|    n_updates            | 2660         |\n",
            "|    policy_gradient_loss | -0.00328     |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 114         |\n",
            "|    ep_rew_mean          | 32.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 135         |\n",
            "|    time_elapsed         | 1498        |\n",
            "|    total_timesteps      | 552960      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001522042 |\n",
            "|    clip_fraction        | 0.0343      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0.424       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 6.79        |\n",
            "|    n_updates            | 2680        |\n",
            "|    policy_gradient_loss | -0.0029     |\n",
            "|    value_loss           | 13.8        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 32.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 136          |\n",
            "|    time_elapsed         | 1510         |\n",
            "|    total_timesteps      | 557056       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011902298 |\n",
            "|    clip_fraction        | 0.026        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.471        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.68         |\n",
            "|    n_updates            | 2700         |\n",
            "|    policy_gradient_loss | -0.00243     |\n",
            "|    value_loss           | 12.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 32.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 137          |\n",
            "|    time_elapsed         | 1521         |\n",
            "|    total_timesteps      | 561152       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012243411 |\n",
            "|    clip_fraction        | 0.0207       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.458        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.29         |\n",
            "|    n_updates            | 2720         |\n",
            "|    policy_gradient_loss | -0.00243     |\n",
            "|    value_loss           | 12.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 36           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 138          |\n",
            "|    time_elapsed         | 1532         |\n",
            "|    total_timesteps      | 565248       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012373328 |\n",
            "|    clip_fraction        | 0.0221       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.511        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.76         |\n",
            "|    n_updates            | 2740         |\n",
            "|    policy_gradient_loss | -0.00193     |\n",
            "|    value_loss           | 10.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 139          |\n",
            "|    time_elapsed         | 1544         |\n",
            "|    total_timesteps      | 569344       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015441736 |\n",
            "|    clip_fraction        | 0.0355       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.465        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.14         |\n",
            "|    n_updates            | 2760         |\n",
            "|    policy_gradient_loss | -0.00328     |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 140          |\n",
            "|    time_elapsed         | 1555         |\n",
            "|    total_timesteps      | 573440       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014881344 |\n",
            "|    clip_fraction        | 0.0328       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.355        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.47         |\n",
            "|    n_updates            | 2780         |\n",
            "|    policy_gradient_loss | -0.00299     |\n",
            "|    value_loss           | 14.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 32.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 141          |\n",
            "|    time_elapsed         | 1566         |\n",
            "|    total_timesteps      | 577536       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013721617 |\n",
            "|    clip_fraction        | 0.0357       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.529        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.08         |\n",
            "|    n_updates            | 2800         |\n",
            "|    policy_gradient_loss | -0.00311     |\n",
            "|    value_loss           | 10.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 142          |\n",
            "|    time_elapsed         | 1577         |\n",
            "|    total_timesteps      | 581632       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019236747 |\n",
            "|    clip_fraction        | 0.0459       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.434        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.26         |\n",
            "|    n_updates            | 2820         |\n",
            "|    policy_gradient_loss | -0.00335     |\n",
            "|    value_loss           | 13.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 34.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 143          |\n",
            "|    time_elapsed         | 1588         |\n",
            "|    total_timesteps      | 585728       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016407354 |\n",
            "|    clip_fraction        | 0.0432       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.433        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.27         |\n",
            "|    n_updates            | 2840         |\n",
            "|    policy_gradient_loss | -0.00314     |\n",
            "|    value_loss           | 12.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 144          |\n",
            "|    time_elapsed         | 1599         |\n",
            "|    total_timesteps      | 589824       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013706591 |\n",
            "|    clip_fraction        | 0.0278       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.372        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.19         |\n",
            "|    n_updates            | 2860         |\n",
            "|    policy_gradient_loss | -0.00239     |\n",
            "|    value_loss           | 13.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 36.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 145          |\n",
            "|    time_elapsed         | 1611         |\n",
            "|    total_timesteps      | 593920       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014498157 |\n",
            "|    clip_fraction        | 0.0381       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.321        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.35         |\n",
            "|    n_updates            | 2880         |\n",
            "|    policy_gradient_loss | -0.0036      |\n",
            "|    value_loss           | 14.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 36.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 146          |\n",
            "|    time_elapsed         | 1622         |\n",
            "|    total_timesteps      | 598016       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012706041 |\n",
            "|    clip_fraction        | 0.0231       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.252        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 9.15         |\n",
            "|    n_updates            | 2900         |\n",
            "|    policy_gradient_loss | -0.00254     |\n",
            "|    value_loss           | 17.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 34           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 147          |\n",
            "|    time_elapsed         | 1633         |\n",
            "|    total_timesteps      | 602112       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015290573 |\n",
            "|    clip_fraction        | 0.0323       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.488        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.53         |\n",
            "|    n_updates            | 2920         |\n",
            "|    policy_gradient_loss | -0.00355     |\n",
            "|    value_loss           | 10.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 33.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 148          |\n",
            "|    time_elapsed         | 1644         |\n",
            "|    total_timesteps      | 606208       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014252518 |\n",
            "|    clip_fraction        | 0.0254       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.462        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.79         |\n",
            "|    n_updates            | 2940         |\n",
            "|    policy_gradient_loss | -0.00265     |\n",
            "|    value_loss           | 13.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 149          |\n",
            "|    time_elapsed         | 1655         |\n",
            "|    total_timesteps      | 610304       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010498944 |\n",
            "|    clip_fraction        | 0.00798      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.518        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.9          |\n",
            "|    n_updates            | 2960         |\n",
            "|    policy_gradient_loss | -0.00146     |\n",
            "|    value_loss           | 12.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 150          |\n",
            "|    time_elapsed         | 1667         |\n",
            "|    total_timesteps      | 614400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010643187 |\n",
            "|    clip_fraction        | 0.0177       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.399        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.28         |\n",
            "|    n_updates            | 2980         |\n",
            "|    policy_gradient_loss | -0.00219     |\n",
            "|    value_loss           | 16.1         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 122         |\n",
            "|    ep_rew_mean          | 35.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 151         |\n",
            "|    time_elapsed         | 1678        |\n",
            "|    total_timesteps      | 618496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001242259 |\n",
            "|    clip_fraction        | 0.0255      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.452       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 6.52        |\n",
            "|    n_updates            | 3000        |\n",
            "|    policy_gradient_loss | -0.00259    |\n",
            "|    value_loss           | 13.4        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 152          |\n",
            "|    time_elapsed         | 1689         |\n",
            "|    total_timesteps      | 622592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014843722 |\n",
            "|    clip_fraction        | 0.0428       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.2         |\n",
            "|    explained_variance   | 0.439        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.21         |\n",
            "|    n_updates            | 3020         |\n",
            "|    policy_gradient_loss | -0.00358     |\n",
            "|    value_loss           | 13.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 37           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 153          |\n",
            "|    time_elapsed         | 1700         |\n",
            "|    total_timesteps      | 626688       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012960974 |\n",
            "|    clip_fraction        | 0.0275       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.334        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.43         |\n",
            "|    n_updates            | 3040         |\n",
            "|    policy_gradient_loss | -0.00289     |\n",
            "|    value_loss           | 14.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 154          |\n",
            "|    time_elapsed         | 1711         |\n",
            "|    total_timesteps      | 630784       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016504677 |\n",
            "|    clip_fraction        | 0.0428       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.416        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.36         |\n",
            "|    n_updates            | 3060         |\n",
            "|    policy_gradient_loss | -0.00278     |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 127         |\n",
            "|    ep_rew_mean          | 38.1        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 155         |\n",
            "|    time_elapsed         | 1723        |\n",
            "|    total_timesteps      | 634880      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001342641 |\n",
            "|    clip_fraction        | 0.0274      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.463       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 5.85        |\n",
            "|    n_updates            | 3080        |\n",
            "|    policy_gradient_loss | -0.00271    |\n",
            "|    value_loss           | 12.1        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 35.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 156          |\n",
            "|    time_elapsed         | 1734         |\n",
            "|    total_timesteps      | 638976       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012034089 |\n",
            "|    clip_fraction        | 0.0169       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.267        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.05         |\n",
            "|    n_updates            | 3100         |\n",
            "|    policy_gradient_loss | -0.00183     |\n",
            "|    value_loss           | 16.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 157          |\n",
            "|    time_elapsed         | 1745         |\n",
            "|    total_timesteps      | 643072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016142498 |\n",
            "|    clip_fraction        | 0.0364       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.333        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.14         |\n",
            "|    n_updates            | 3120         |\n",
            "|    policy_gradient_loss | -0.00307     |\n",
            "|    value_loss           | 15.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 32.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 158          |\n",
            "|    time_elapsed         | 1756         |\n",
            "|    total_timesteps      | 647168       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013369389 |\n",
            "|    clip_fraction        | 0.019        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.43         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.3          |\n",
            "|    n_updates            | 3140         |\n",
            "|    policy_gradient_loss | -0.00206     |\n",
            "|    value_loss           | 13.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 32.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 159          |\n",
            "|    time_elapsed         | 1767         |\n",
            "|    total_timesteps      | 651264       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015285042 |\n",
            "|    clip_fraction        | 0.0442       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.487        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.79         |\n",
            "|    n_updates            | 3160         |\n",
            "|    policy_gradient_loss | -0.00388     |\n",
            "|    value_loss           | 12           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 31.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 160          |\n",
            "|    time_elapsed         | 1778         |\n",
            "|    total_timesteps      | 655360       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013019457 |\n",
            "|    clip_fraction        | 0.0409       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.445        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.13         |\n",
            "|    n_updates            | 3180         |\n",
            "|    policy_gradient_loss | -0.00343     |\n",
            "|    value_loss           | 12.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 161          |\n",
            "|    time_elapsed         | 1789         |\n",
            "|    total_timesteps      | 659456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015049191 |\n",
            "|    clip_fraction        | 0.0375       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.517        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.3          |\n",
            "|    n_updates            | 3200         |\n",
            "|    policy_gradient_loss | -0.00337     |\n",
            "|    value_loss           | 11.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 162          |\n",
            "|    time_elapsed         | 1801         |\n",
            "|    total_timesteps      | 663552       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016666712 |\n",
            "|    clip_fraction        | 0.0549       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.455        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.47         |\n",
            "|    n_updates            | 3220         |\n",
            "|    policy_gradient_loss | -0.00426     |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 130          |\n",
            "|    ep_rew_mean          | 39.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 163          |\n",
            "|    time_elapsed         | 1812         |\n",
            "|    total_timesteps      | 667648       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014898714 |\n",
            "|    clip_fraction        | 0.0272       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.438        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.54         |\n",
            "|    n_updates            | 3240         |\n",
            "|    policy_gradient_loss | -0.00296     |\n",
            "|    value_loss           | 12.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 36.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 164          |\n",
            "|    time_elapsed         | 1823         |\n",
            "|    total_timesteps      | 671744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017422528 |\n",
            "|    clip_fraction        | 0.0444       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.2         |\n",
            "|    explained_variance   | 0.265        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.09         |\n",
            "|    n_updates            | 3260         |\n",
            "|    policy_gradient_loss | -0.00353     |\n",
            "|    value_loss           | 14.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 165          |\n",
            "|    time_elapsed         | 1834         |\n",
            "|    total_timesteps      | 675840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016273181 |\n",
            "|    clip_fraction        | 0.0457       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.433        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.63         |\n",
            "|    n_updates            | 3280         |\n",
            "|    policy_gradient_loss | -0.00363     |\n",
            "|    value_loss           | 11.9         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 115         |\n",
            "|    ep_rew_mean          | 32.3        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 368         |\n",
            "|    iterations           | 166         |\n",
            "|    time_elapsed         | 1845        |\n",
            "|    total_timesteps      | 679936      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001752675 |\n",
            "|    clip_fraction        | 0.0567      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | 0.637       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 4.17        |\n",
            "|    n_updates            | 3300        |\n",
            "|    policy_gradient_loss | -0.00361    |\n",
            "|    value_loss           | 9.3         |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 35.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 167          |\n",
            "|    time_elapsed         | 1857         |\n",
            "|    total_timesteps      | 684032       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014878223 |\n",
            "|    clip_fraction        | 0.0305       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.569        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.36         |\n",
            "|    n_updates            | 3320         |\n",
            "|    policy_gradient_loss | -0.00295     |\n",
            "|    value_loss           | 11           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 168          |\n",
            "|    time_elapsed         | 1868         |\n",
            "|    total_timesteps      | 688128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013976117 |\n",
            "|    clip_fraction        | 0.0327       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.531        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.85         |\n",
            "|    n_updates            | 3340         |\n",
            "|    policy_gradient_loss | -0.00319     |\n",
            "|    value_loss           | 12.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 112          |\n",
            "|    ep_rew_mean          | 31.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 169          |\n",
            "|    time_elapsed         | 1879         |\n",
            "|    total_timesteps      | 692224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015070562 |\n",
            "|    clip_fraction        | 0.0362       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.516        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.61         |\n",
            "|    n_updates            | 3360         |\n",
            "|    policy_gradient_loss | -0.0028      |\n",
            "|    value_loss           | 11.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 109          |\n",
            "|    ep_rew_mean          | 30           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 170          |\n",
            "|    time_elapsed         | 1891         |\n",
            "|    total_timesteps      | 696320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018639013 |\n",
            "|    clip_fraction        | 0.0364       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.517        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.06         |\n",
            "|    n_updates            | 3380         |\n",
            "|    policy_gradient_loss | -0.00287     |\n",
            "|    value_loss           | 12.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 32.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 171          |\n",
            "|    time_elapsed         | 1902         |\n",
            "|    total_timesteps      | 700416       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015833259 |\n",
            "|    clip_fraction        | 0.0366       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.601        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 3.71         |\n",
            "|    n_updates            | 3400         |\n",
            "|    policy_gradient_loss | -0.00313     |\n",
            "|    value_loss           | 9.05         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 33.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 172          |\n",
            "|    time_elapsed         | 1914         |\n",
            "|    total_timesteps      | 704512       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015548898 |\n",
            "|    clip_fraction        | 0.0417       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.534        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.7          |\n",
            "|    n_updates            | 3420         |\n",
            "|    policy_gradient_loss | -0.00309     |\n",
            "|    value_loss           | 9.73         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 32.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 368          |\n",
            "|    iterations           | 173          |\n",
            "|    time_elapsed         | 1925         |\n",
            "|    total_timesteps      | 708608       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013467046 |\n",
            "|    clip_fraction        | 0.0278       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.515        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.83         |\n",
            "|    n_updates            | 3440         |\n",
            "|    policy_gradient_loss | -0.00293     |\n",
            "|    value_loss           | 10.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 32.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 174          |\n",
            "|    time_elapsed         | 1936         |\n",
            "|    total_timesteps      | 712704       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014279998 |\n",
            "|    clip_fraction        | 0.0348       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.52         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.16         |\n",
            "|    n_updates            | 3460         |\n",
            "|    policy_gradient_loss | -0.00369     |\n",
            "|    value_loss           | 11.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 175          |\n",
            "|    time_elapsed         | 1948         |\n",
            "|    total_timesteps      | 716800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015327854 |\n",
            "|    clip_fraction        | 0.032        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.438        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.84         |\n",
            "|    n_updates            | 3480         |\n",
            "|    policy_gradient_loss | -0.00361     |\n",
            "|    value_loss           | 12.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 176          |\n",
            "|    time_elapsed         | 1959         |\n",
            "|    total_timesteps      | 720896       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015020324 |\n",
            "|    clip_fraction        | 0.033        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.294        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.87         |\n",
            "|    n_updates            | 3500         |\n",
            "|    policy_gradient_loss | -0.00324     |\n",
            "|    value_loss           | 13.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 177          |\n",
            "|    time_elapsed         | 1970         |\n",
            "|    total_timesteps      | 724992       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010922684 |\n",
            "|    clip_fraction        | 0.0189       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.37         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.63         |\n",
            "|    n_updates            | 3520         |\n",
            "|    policy_gradient_loss | -0.00248     |\n",
            "|    value_loss           | 12           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 33.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 178          |\n",
            "|    time_elapsed         | 1981         |\n",
            "|    total_timesteps      | 729088       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012094222 |\n",
            "|    clip_fraction        | 0.0235       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.452        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.55         |\n",
            "|    n_updates            | 3540         |\n",
            "|    policy_gradient_loss | -0.00239     |\n",
            "|    value_loss           | 10.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 32.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 179          |\n",
            "|    time_elapsed         | 1993         |\n",
            "|    total_timesteps      | 733184       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013290674 |\n",
            "|    clip_fraction        | 0.0284       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.465        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.92         |\n",
            "|    n_updates            | 3560         |\n",
            "|    policy_gradient_loss | -0.00268     |\n",
            "|    value_loss           | 12.6         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 117         |\n",
            "|    ep_rew_mean          | 33.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 180         |\n",
            "|    time_elapsed         | 2004        |\n",
            "|    total_timesteps      | 737280      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001194949 |\n",
            "|    clip_fraction        | 0.0162      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | 0.581       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 3.97        |\n",
            "|    n_updates            | 3580        |\n",
            "|    policy_gradient_loss | -0.00248    |\n",
            "|    value_loss           | 9.36        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 34.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 181          |\n",
            "|    time_elapsed         | 2015         |\n",
            "|    total_timesteps      | 741376       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015708262 |\n",
            "|    clip_fraction        | 0.0282       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.531        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.8          |\n",
            "|    n_updates            | 3600         |\n",
            "|    policy_gradient_loss | -0.00275     |\n",
            "|    value_loss           | 12           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 182          |\n",
            "|    time_elapsed         | 2027         |\n",
            "|    total_timesteps      | 745472       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011761547 |\n",
            "|    clip_fraction        | 0.0295       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.37         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.17         |\n",
            "|    n_updates            | 3620         |\n",
            "|    policy_gradient_loss | -0.00273     |\n",
            "|    value_loss           | 13.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 183          |\n",
            "|    time_elapsed         | 2038         |\n",
            "|    total_timesteps      | 749568       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015383615 |\n",
            "|    clip_fraction        | 0.0296       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.456        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.36         |\n",
            "|    n_updates            | 3640         |\n",
            "|    policy_gradient_loss | -0.00281     |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 184          |\n",
            "|    time_elapsed         | 2049         |\n",
            "|    total_timesteps      | 753664       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013533558 |\n",
            "|    clip_fraction        | 0.029        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.473        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.37         |\n",
            "|    n_updates            | 3660         |\n",
            "|    policy_gradient_loss | -0.0032      |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 185          |\n",
            "|    time_elapsed         | 2060         |\n",
            "|    total_timesteps      | 757760       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015553082 |\n",
            "|    clip_fraction        | 0.0305       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.439        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.11         |\n",
            "|    n_updates            | 3680         |\n",
            "|    policy_gradient_loss | -0.00329     |\n",
            "|    value_loss           | 13.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 32.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 186          |\n",
            "|    time_elapsed         | 2072         |\n",
            "|    total_timesteps      | 761856       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012628831 |\n",
            "|    clip_fraction        | 0.0264       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.506        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.09         |\n",
            "|    n_updates            | 3700         |\n",
            "|    policy_gradient_loss | -0.00244     |\n",
            "|    value_loss           | 11.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 187          |\n",
            "|    time_elapsed         | 2083         |\n",
            "|    total_timesteps      | 765952       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014965665 |\n",
            "|    clip_fraction        | 0.0354       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.505        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.65         |\n",
            "|    n_updates            | 3720         |\n",
            "|    policy_gradient_loss | -0.00283     |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 36.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 188          |\n",
            "|    time_elapsed         | 2094         |\n",
            "|    total_timesteps      | 770048       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013822913 |\n",
            "|    clip_fraction        | 0.0203       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.486        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.39         |\n",
            "|    n_updates            | 3740         |\n",
            "|    policy_gradient_loss | -0.00239     |\n",
            "|    value_loss           | 11.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 189          |\n",
            "|    time_elapsed         | 2106         |\n",
            "|    total_timesteps      | 774144       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011385856 |\n",
            "|    clip_fraction        | 0.0119       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.405        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.09         |\n",
            "|    n_updates            | 3760         |\n",
            "|    policy_gradient_loss | -0.00193     |\n",
            "|    value_loss           | 14.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 34.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 190          |\n",
            "|    time_elapsed         | 2117         |\n",
            "|    total_timesteps      | 778240       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013512276 |\n",
            "|    clip_fraction        | 0.0278       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.439        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.9          |\n",
            "|    n_updates            | 3780         |\n",
            "|    policy_gradient_loss | -0.00267     |\n",
            "|    value_loss           | 13           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 32.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 191          |\n",
            "|    time_elapsed         | 2128         |\n",
            "|    total_timesteps      | 782336       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016095517 |\n",
            "|    clip_fraction        | 0.0443       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.496        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.88         |\n",
            "|    n_updates            | 3800         |\n",
            "|    policy_gradient_loss | -0.00376     |\n",
            "|    value_loss           | 12.3         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 114         |\n",
            "|    ep_rew_mean          | 31.9        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 192         |\n",
            "|    time_elapsed         | 2139        |\n",
            "|    total_timesteps      | 786432      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001719816 |\n",
            "|    clip_fraction        | 0.0449      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0.49        |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 5.2         |\n",
            "|    n_updates            | 3820        |\n",
            "|    policy_gradient_loss | -0.00338    |\n",
            "|    value_loss           | 12.3        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 193          |\n",
            "|    time_elapsed         | 2150         |\n",
            "|    total_timesteps      | 790528       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013983015 |\n",
            "|    clip_fraction        | 0.0311       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.626        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.22         |\n",
            "|    n_updates            | 3840         |\n",
            "|    policy_gradient_loss | -0.00278     |\n",
            "|    value_loss           | 9.27         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 194          |\n",
            "|    time_elapsed         | 2162         |\n",
            "|    total_timesteps      | 794624       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018304014 |\n",
            "|    clip_fraction        | 0.0587       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.473        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.1          |\n",
            "|    n_updates            | 3860         |\n",
            "|    policy_gradient_loss | -0.004       |\n",
            "|    value_loss           | 12.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 195          |\n",
            "|    time_elapsed         | 2173         |\n",
            "|    total_timesteps      | 798720       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016873138 |\n",
            "|    clip_fraction        | 0.0467       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.408        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.93         |\n",
            "|    n_updates            | 3880         |\n",
            "|    policy_gradient_loss | -0.00387     |\n",
            "|    value_loss           | 13.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 32.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 196          |\n",
            "|    time_elapsed         | 2184         |\n",
            "|    total_timesteps      | 802816       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017561791 |\n",
            "|    clip_fraction        | 0.0425       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.338        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.47         |\n",
            "|    n_updates            | 3900         |\n",
            "|    policy_gradient_loss | -0.00311     |\n",
            "|    value_loss           | 14.2         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 115         |\n",
            "|    ep_rew_mean          | 33.2        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 197         |\n",
            "|    time_elapsed         | 2195        |\n",
            "|    total_timesteps      | 806912      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001380605 |\n",
            "|    clip_fraction        | 0.0266      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0.477       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 5.39        |\n",
            "|    n_updates            | 3920        |\n",
            "|    policy_gradient_loss | -0.00303    |\n",
            "|    value_loss           | 10.9        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 198          |\n",
            "|    time_elapsed         | 2206         |\n",
            "|    total_timesteps      | 811008       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015459815 |\n",
            "|    clip_fraction        | 0.0292       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.405        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.68         |\n",
            "|    n_updates            | 3940         |\n",
            "|    policy_gradient_loss | -0.00262     |\n",
            "|    value_loss           | 13.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 36.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 199          |\n",
            "|    time_elapsed         | 2217         |\n",
            "|    total_timesteps      | 815104       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014783343 |\n",
            "|    clip_fraction        | 0.0389       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.424        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.85         |\n",
            "|    n_updates            | 3960         |\n",
            "|    policy_gradient_loss | -0.00354     |\n",
            "|    value_loss           | 12.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 34.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 200          |\n",
            "|    time_elapsed         | 2228         |\n",
            "|    total_timesteps      | 819200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015826221 |\n",
            "|    clip_fraction        | 0.0388       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.409        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.48         |\n",
            "|    n_updates            | 3980         |\n",
            "|    policy_gradient_loss | -0.0027      |\n",
            "|    value_loss           | 14.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 201          |\n",
            "|    time_elapsed         | 2239         |\n",
            "|    total_timesteps      | 823296       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013538948 |\n",
            "|    clip_fraction        | 0.0252       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.523        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.1          |\n",
            "|    n_updates            | 4000         |\n",
            "|    policy_gradient_loss | -0.0023      |\n",
            "|    value_loss           | 11.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 110          |\n",
            "|    ep_rew_mean          | 31.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 202          |\n",
            "|    time_elapsed         | 2251         |\n",
            "|    total_timesteps      | 827392       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015820997 |\n",
            "|    clip_fraction        | 0.0394       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.555        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.96         |\n",
            "|    n_updates            | 4020         |\n",
            "|    policy_gradient_loss | -0.00409     |\n",
            "|    value_loss           | 11.4         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 112         |\n",
            "|    ep_rew_mean          | 31.7        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 203         |\n",
            "|    time_elapsed         | 2262        |\n",
            "|    total_timesteps      | 831488      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001265477 |\n",
            "|    clip_fraction        | 0.0253      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0.438       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 6.83        |\n",
            "|    n_updates            | 4040        |\n",
            "|    policy_gradient_loss | -0.00245    |\n",
            "|    value_loss           | 13.6        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 32.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 204          |\n",
            "|    time_elapsed         | 2273         |\n",
            "|    total_timesteps      | 835584       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014853636 |\n",
            "|    clip_fraction        | 0.0323       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.507        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.35         |\n",
            "|    n_updates            | 4060         |\n",
            "|    policy_gradient_loss | -0.00294     |\n",
            "|    value_loss           | 11           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 32.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 205          |\n",
            "|    time_elapsed         | 2284         |\n",
            "|    total_timesteps      | 839680       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013972479 |\n",
            "|    clip_fraction        | 0.0293       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.427        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.24         |\n",
            "|    n_updates            | 4080         |\n",
            "|    policy_gradient_loss | -0.00274     |\n",
            "|    value_loss           | 12.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 32.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 206          |\n",
            "|    time_elapsed         | 2295         |\n",
            "|    total_timesteps      | 843776       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012588352 |\n",
            "|    clip_fraction        | 0.0244       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.455        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.55         |\n",
            "|    n_updates            | 4100         |\n",
            "|    policy_gradient_loss | -0.00284     |\n",
            "|    value_loss           | 11.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 207          |\n",
            "|    time_elapsed         | 2306         |\n",
            "|    total_timesteps      | 847872       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012813553 |\n",
            "|    clip_fraction        | 0.0299       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.467        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.81         |\n",
            "|    n_updates            | 4120         |\n",
            "|    policy_gradient_loss | -0.00311     |\n",
            "|    value_loss           | 10.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 36           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 208          |\n",
            "|    time_elapsed         | 2318         |\n",
            "|    total_timesteps      | 851968       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010716312 |\n",
            "|    clip_fraction        | 0.0174       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.332        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.52         |\n",
            "|    n_updates            | 4140         |\n",
            "|    policy_gradient_loss | -0.0019      |\n",
            "|    value_loss           | 14.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 36.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 209          |\n",
            "|    time_elapsed         | 2329         |\n",
            "|    total_timesteps      | 856064       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013209104 |\n",
            "|    clip_fraction        | 0.0264       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.356        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.06         |\n",
            "|    n_updates            | 4160         |\n",
            "|    policy_gradient_loss | -0.00309     |\n",
            "|    value_loss           | 14           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 31.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 210          |\n",
            "|    time_elapsed         | 2340         |\n",
            "|    total_timesteps      | 860160       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012565189 |\n",
            "|    clip_fraction        | 0.0245       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.403        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.39         |\n",
            "|    n_updates            | 4180         |\n",
            "|    policy_gradient_loss | -0.00241     |\n",
            "|    value_loss           | 13.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 34           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 211          |\n",
            "|    time_elapsed         | 2352         |\n",
            "|    total_timesteps      | 864256       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011532007 |\n",
            "|    clip_fraction        | 0.0216       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.428        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.72         |\n",
            "|    n_updates            | 4200         |\n",
            "|    policy_gradient_loss | -0.0022      |\n",
            "|    value_loss           | 12.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 212          |\n",
            "|    time_elapsed         | 2363         |\n",
            "|    total_timesteps      | 868352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014747856 |\n",
            "|    clip_fraction        | 0.027        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.37         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.72         |\n",
            "|    n_updates            | 4220         |\n",
            "|    policy_gradient_loss | -0.00271     |\n",
            "|    value_loss           | 15           |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 117        |\n",
            "|    ep_rew_mean          | 33.3       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 367        |\n",
            "|    iterations           | 213        |\n",
            "|    time_elapsed         | 2374       |\n",
            "|    total_timesteps      | 872448     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00117555 |\n",
            "|    clip_fraction        | 0.015      |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -1.22      |\n",
            "|    explained_variance   | 0.486      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 4.59       |\n",
            "|    n_updates            | 4240       |\n",
            "|    policy_gradient_loss | -0.00173   |\n",
            "|    value_loss           | 10.5       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 214          |\n",
            "|    time_elapsed         | 2386         |\n",
            "|    total_timesteps      | 876544       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013772228 |\n",
            "|    clip_fraction        | 0.0285       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.442        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.76         |\n",
            "|    n_updates            | 4260         |\n",
            "|    policy_gradient_loss | -0.00314     |\n",
            "|    value_loss           | 13.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 215          |\n",
            "|    time_elapsed         | 2397         |\n",
            "|    total_timesteps      | 880640       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017139704 |\n",
            "|    clip_fraction        | 0.0501       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.527        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.93         |\n",
            "|    n_updates            | 4280         |\n",
            "|    policy_gradient_loss | -0.00347     |\n",
            "|    value_loss           | 10.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 36.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 216          |\n",
            "|    time_elapsed         | 2408         |\n",
            "|    total_timesteps      | 884736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013750475 |\n",
            "|    clip_fraction        | 0.031        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.531        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.39         |\n",
            "|    n_updates            | 4300         |\n",
            "|    policy_gradient_loss | -0.00274     |\n",
            "|    value_loss           | 10.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 217          |\n",
            "|    time_elapsed         | 2420         |\n",
            "|    total_timesteps      | 888832       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012161386 |\n",
            "|    clip_fraction        | 0.0176       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.421        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.37         |\n",
            "|    n_updates            | 4320         |\n",
            "|    policy_gradient_loss | -0.00188     |\n",
            "|    value_loss           | 14.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 33.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 218          |\n",
            "|    time_elapsed         | 2431         |\n",
            "|    total_timesteps      | 892928       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010680328 |\n",
            "|    clip_fraction        | 0.00892      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.464        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.16         |\n",
            "|    n_updates            | 4340         |\n",
            "|    policy_gradient_loss | -0.00196     |\n",
            "|    value_loss           | 12.5         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 34           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 219          |\n",
            "|    time_elapsed         | 2443         |\n",
            "|    total_timesteps      | 897024       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013067491 |\n",
            "|    clip_fraction        | 0.0238       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.348        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 8.62         |\n",
            "|    n_updates            | 4360         |\n",
            "|    policy_gradient_loss | -0.00259     |\n",
            "|    value_loss           | 16.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 37           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 220          |\n",
            "|    time_elapsed         | 2454         |\n",
            "|    total_timesteps      | 901120       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012774694 |\n",
            "|    clip_fraction        | 0.0187       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.411        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.87         |\n",
            "|    n_updates            | 4380         |\n",
            "|    policy_gradient_loss | -0.00288     |\n",
            "|    value_loss           | 13.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 221          |\n",
            "|    time_elapsed         | 2465         |\n",
            "|    total_timesteps      | 905216       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015793608 |\n",
            "|    clip_fraction        | 0.0462       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.505        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.8          |\n",
            "|    n_updates            | 4400         |\n",
            "|    policy_gradient_loss | -0.00357     |\n",
            "|    value_loss           | 11.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 222          |\n",
            "|    time_elapsed         | 2477         |\n",
            "|    total_timesteps      | 909312       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015743297 |\n",
            "|    clip_fraction        | 0.0269       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.494        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.97         |\n",
            "|    n_updates            | 4420         |\n",
            "|    policy_gradient_loss | -0.00331     |\n",
            "|    value_loss           | 12.2         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 367          |\n",
            "|    iterations           | 223          |\n",
            "|    time_elapsed         | 2488         |\n",
            "|    total_timesteps      | 913408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013565875 |\n",
            "|    clip_fraction        | 0.0275       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.52         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.09         |\n",
            "|    n_updates            | 4440         |\n",
            "|    policy_gradient_loss | -0.00259     |\n",
            "|    value_loss           | 11.1         |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 120         |\n",
            "|    ep_rew_mean          | 34.8        |\n",
            "| time/                   |             |\n",
            "|    fps                  | 367         |\n",
            "|    iterations           | 224         |\n",
            "|    time_elapsed         | 2499        |\n",
            "|    total_timesteps      | 917504      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001640406 |\n",
            "|    clip_fraction        | 0.0388      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0.519       |\n",
            "|    learning_rate        | 0.0001      |\n",
            "|    loss                 | 5.48        |\n",
            "|    n_updates            | 4460        |\n",
            "|    policy_gradient_loss | -0.00315    |\n",
            "|    value_loss           | 11.4        |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 36.3         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 225          |\n",
            "|    time_elapsed         | 2511         |\n",
            "|    total_timesteps      | 921600       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012167357 |\n",
            "|    clip_fraction        | 0.0146       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.427        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.96         |\n",
            "|    n_updates            | 4480         |\n",
            "|    policy_gradient_loss | -0.00255     |\n",
            "|    value_loss           | 13.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 226          |\n",
            "|    time_elapsed         | 2522         |\n",
            "|    total_timesteps      | 925696       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015905446 |\n",
            "|    clip_fraction        | 0.0462       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.481        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.14         |\n",
            "|    n_updates            | 4500         |\n",
            "|    policy_gradient_loss | -0.00332     |\n",
            "|    value_loss           | 12.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 36.1         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 227          |\n",
            "|    time_elapsed         | 2533         |\n",
            "|    total_timesteps      | 929792       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011411834 |\n",
            "|    clip_fraction        | 0.0264       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.41         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 7.62         |\n",
            "|    n_updates            | 4520         |\n",
            "|    policy_gradient_loss | -0.00367     |\n",
            "|    value_loss           | 14.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 228          |\n",
            "|    time_elapsed         | 2545         |\n",
            "|    total_timesteps      | 933888       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019638755 |\n",
            "|    clip_fraction        | 0.0573       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.43         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.84         |\n",
            "|    n_updates            | 4540         |\n",
            "|    policy_gradient_loss | -0.00397     |\n",
            "|    value_loss           | 12.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 35.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 229          |\n",
            "|    time_elapsed         | 2556         |\n",
            "|    total_timesteps      | 937984       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012122882 |\n",
            "|    clip_fraction        | 0.0135       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.475        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.64         |\n",
            "|    n_updates            | 4560         |\n",
            "|    policy_gradient_loss | -0.00217     |\n",
            "|    value_loss           | 11.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 230          |\n",
            "|    time_elapsed         | 2567         |\n",
            "|    total_timesteps      | 942080       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015342357 |\n",
            "|    clip_fraction        | 0.0301       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.587        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.64         |\n",
            "|    n_updates            | 4580         |\n",
            "|    policy_gradient_loss | -0.00408     |\n",
            "|    value_loss           | 10.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 36.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 231          |\n",
            "|    time_elapsed         | 2579         |\n",
            "|    total_timesteps      | 946176       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013518997 |\n",
            "|    clip_fraction        | 0.0216       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.531        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.13         |\n",
            "|    n_updates            | 4600         |\n",
            "|    policy_gradient_loss | -0.00229     |\n",
            "|    value_loss           | 12.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 35.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 232          |\n",
            "|    time_elapsed         | 2590         |\n",
            "|    total_timesteps      | 950272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014912621 |\n",
            "|    clip_fraction        | 0.0497       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0.536        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.81         |\n",
            "|    n_updates            | 4620         |\n",
            "|    policy_gradient_loss | -0.00415     |\n",
            "|    value_loss           | 11.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 233          |\n",
            "|    time_elapsed         | 2601         |\n",
            "|    total_timesteps      | 954368       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016731824 |\n",
            "|    clip_fraction        | 0.0401       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.506        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.81         |\n",
            "|    n_updates            | 4640         |\n",
            "|    policy_gradient_loss | -0.00359     |\n",
            "|    value_loss           | 13.1         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34.4         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 234          |\n",
            "|    time_elapsed         | 2612         |\n",
            "|    total_timesteps      | 958464       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019441128 |\n",
            "|    clip_fraction        | 0.0542       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.49         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.15         |\n",
            "|    n_updates            | 4660         |\n",
            "|    policy_gradient_loss | -0.00378     |\n",
            "|    value_loss           | 12.4         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 33.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 235          |\n",
            "|    time_elapsed         | 2623         |\n",
            "|    total_timesteps      | 962560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013593165 |\n",
            "|    clip_fraction        | 0.0368       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.417        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 6.06         |\n",
            "|    n_updates            | 4680         |\n",
            "|    policy_gradient_loss | -0.00329     |\n",
            "|    value_loss           | 13.6         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 236          |\n",
            "|    time_elapsed         | 2634         |\n",
            "|    total_timesteps      | 966656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013517011 |\n",
            "|    clip_fraction        | 0.0262       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.482        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.27         |\n",
            "|    n_updates            | 4700         |\n",
            "|    policy_gradient_loss | -0.00256     |\n",
            "|    value_loss           | 12.3         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 33.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 237          |\n",
            "|    time_elapsed         | 2645         |\n",
            "|    total_timesteps      | 970752       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014829428 |\n",
            "|    clip_fraction        | 0.0363       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.499        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.49         |\n",
            "|    n_updates            | 4720         |\n",
            "|    policy_gradient_loss | -0.00312     |\n",
            "|    value_loss           | 11.7         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 34           |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 238          |\n",
            "|    time_elapsed         | 2657         |\n",
            "|    total_timesteps      | 974848       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014087246 |\n",
            "|    clip_fraction        | 0.0349       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.457        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.83         |\n",
            "|    n_updates            | 4740         |\n",
            "|    policy_gradient_loss | -0.00389     |\n",
            "|    value_loss           | 13           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 33.8         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 239          |\n",
            "|    time_elapsed         | 2668         |\n",
            "|    total_timesteps      | 978944       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015100905 |\n",
            "|    clip_fraction        | 0.0307       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.433        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.35         |\n",
            "|    n_updates            | 4760         |\n",
            "|    policy_gradient_loss | -0.00247     |\n",
            "|    value_loss           | 13           |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 35.5         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 240          |\n",
            "|    time_elapsed         | 2679         |\n",
            "|    total_timesteps      | 983040       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016210862 |\n",
            "|    clip_fraction        | 0.0444       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.49         |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.24         |\n",
            "|    n_updates            | 4780         |\n",
            "|    policy_gradient_loss | -0.00332     |\n",
            "|    value_loss           | 10.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 35.7         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 241          |\n",
            "|    time_elapsed         | 2690         |\n",
            "|    total_timesteps      | 987136       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012900187 |\n",
            "|    clip_fraction        | 0.0337       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.487        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.64         |\n",
            "|    n_updates            | 4800         |\n",
            "|    policy_gradient_loss | -0.00296     |\n",
            "|    value_loss           | 12.1         |\n",
            "------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 117        |\n",
            "|    ep_rew_mean          | 33.2       |\n",
            "| time/                   |            |\n",
            "|    fps                  | 366        |\n",
            "|    iterations           | 242        |\n",
            "|    time_elapsed         | 2702       |\n",
            "|    total_timesteps      | 991232     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00145088 |\n",
            "|    clip_fraction        | 0.0299     |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -1.23      |\n",
            "|    explained_variance   | 0.454      |\n",
            "|    learning_rate        | 0.0001     |\n",
            "|    loss                 | 6.13       |\n",
            "|    n_updates            | 4820       |\n",
            "|    policy_gradient_loss | -0.00259   |\n",
            "|    value_loss           | 12.7       |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 33.9         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 243          |\n",
            "|    time_elapsed         | 2713         |\n",
            "|    total_timesteps      | 995328       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012999342 |\n",
            "|    clip_fraction        | 0.0194       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.495        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.28         |\n",
            "|    n_updates            | 4840         |\n",
            "|    policy_gradient_loss | -0.00231     |\n",
            "|    value_loss           | 11.8         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 34.6         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 244          |\n",
            "|    time_elapsed         | 2724         |\n",
            "|    total_timesteps      | 999424       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015577531 |\n",
            "|    clip_fraction        | 0.0261       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.468        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 4.52         |\n",
            "|    n_updates            | 4860         |\n",
            "|    policy_gradient_loss | -0.0027      |\n",
            "|    value_loss           | 12.9         |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 35.2         |\n",
            "| time/                   |              |\n",
            "|    fps                  | 366          |\n",
            "|    iterations           | 245          |\n",
            "|    time_elapsed         | 2736         |\n",
            "|    total_timesteps      | 1003520      |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015914395 |\n",
            "|    clip_fraction        | 0.054        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.463        |\n",
            "|    learning_rate        | 0.0001       |\n",
            "|    loss                 | 5.16         |\n",
            "|    n_updates            | 4880         |\n",
            "|    policy_gradient_loss | -0.00388     |\n",
            "|    value_loss           | 12.5         |\n",
            "------------------------------------------\n",
            "Saved maskable_ppo_2048_v4.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "env = Game2048EnvV4()\n",
        "from sb3_contrib.common.wrappers import ActionMasker\n",
        "\n",
        "def mask_fn(e): return e.get_action_mask()\n",
        "env_m = ActionMasker(env, mask_fn)\n",
        "\n",
        "obs, info = env_m.reset()\n",
        "for i in range(10):\n",
        "    mask = env.get_action_mask()\n",
        "    print(\"Mask:\", mask)\n",
        "    # take only valid actions\n",
        "    valid = np.where(mask)[0]\n",
        "    a = int(np.random.choice(valid))\n",
        "    old_board = env.board.copy()\n",
        "    obs, r, term, trunc, info = env_m.step(a)\n",
        "    moved = not np.array_equal(env.board, old_board)\n",
        "    print(\"  Took action\", a, \"| Moved:\", moved)\n",
        "    if term or trunc: break\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gCQcBKdwfmxq",
        "outputId": "e2fe3b31-db53-47f8-fdfc-9e60386f8670"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mask: [ True  True  True False]\n",
            "  Took action 0 | Moved: True\n",
            "Mask: [ True  True  True  True]\n",
            "  Took action 3 | Moved: True\n",
            "Mask: [ True  True  True False]\n",
            "  Took action 0 | Moved: True\n",
            "Mask: [ True  True  True  True]\n",
            "  Took action 2 | Moved: True\n",
            "Mask: [ True  True False  True]\n",
            "  Took action 3 | Moved: True\n",
            "Mask: [ True  True  True  True]\n",
            "  Took action 1 | Moved: True\n",
            "Mask: [ True  True  True  True]\n",
            "  Took action 0 | Moved: True\n",
            "Mask: [ True  True  True False]\n",
            "  Took action 1 | Moved: True\n",
            "Mask: [ True  True  True  True]\n",
            "  Took action 2 | Moved: True\n",
            "Mask: [ True  True  True  True]\n",
            "  Took action 2 | Moved: True\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "from sb3_contrib import MaskablePPO\n",
        "from sb3_contrib.common.wrappers import ActionMasker\n",
        "\n",
        "# rebuild masked env for eval\n",
        "def mask_fn(env: Game2048EnvV4):\n",
        "    return env.get_action_mask()\n",
        "\n",
        "eval_env = Game2048EnvV4(render_mode=\"ansi\")\n",
        "eval_env = ActionMasker(eval_env, mask_fn)\n",
        "\n",
        "model = MaskablePPO.load(\"maskable_ppo_2048_v4\")\n",
        "\n",
        "obs, info = eval_env.reset()\n",
        "done = False\n",
        "step = 0\n",
        "\n",
        "# unwrap to get underlying raw env (to print board)\n",
        "raw_env = eval_env.env\n",
        "\n",
        "print(\"Initial Board:\")\n",
        "print(raw_env._board_to_string())\n",
        "\n",
        "while not done:\n",
        "    action, _ = model.predict(obs, deterministic=False)\n",
        "    obs, reward, terminated, truncated, info = eval_env.step(int(action))\n",
        "    done = terminated or truncated\n",
        "\n",
        "    print(f\"\\nStep {step} | Action: {int(action)} | Reward: {reward:.2f} | Score: {info['score']:.1f}\")\n",
        "    print(raw_env._board_to_string())\n",
        "    print(\"-\" * 40)\n",
        "\n",
        "    step += 1\n",
        "    time.sleep(0.15)\n",
        "\n",
        "print(\"\\n=== Episode Finished ===\")\n",
        "print(f\"Final Score: {info['score']}\")\n",
        "print(f\"Max Tile: {info['max_tile']}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8Sfw2quQQ9c0",
        "outputId": "5c161cdc-504b-47c9-fcf7-e074b35c0d37"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Board:\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            ".\t2\t.\t.\n",
            "\n",
            "Step 0 | Action: 2 | Reward: 0.00 | Score: 0.0\n",
            ".\t.\t2\t.\n",
            ".\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 1 | Action: 1 | Reward: 0.12 | Score: 4.0\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t4\n",
            "4\t.\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 2 | Action: 2 | Reward: 0.00 | Score: 4.0\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "4\t.\t.\t2\n",
            "4\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 3 | Action: 1 | Reward: 0.25 | Score: 12.0\n",
            ".\t.\t.\t.\n",
            ".\t2\t.\t.\n",
            ".\t.\t.\t.\n",
            "8\t2\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 4 | Action: 2 | Reward: 0.12 | Score: 16.0\n",
            ".\t.\t.\t2\n",
            "2\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "8\t4\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 5 | Action: 1 | Reward: 0.00 | Score: 16.0\n",
            ".\t2\t.\t.\n",
            ".\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "8\t4\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 6 | Action: 2 | Reward: 0.00 | Score: 16.0\n",
            "2\t.\t2\t.\n",
            ".\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "8\t4\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 7 | Action: 1 | Reward: 0.25 | Score: 24.0\n",
            ".\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "4\t.\t.\t.\n",
            "8\t4\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 8 | Action: 3 | Reward: 0.25 | Score: 32.0\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t2\n",
            "2\t.\t.\t4\n",
            ".\t.\t8\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 9 | Action: 3 | Reward: 0.50 | Score: 48.0\n",
            ".\t.\t2\t.\n",
            ".\t.\t.\t2\n",
            ".\t.\t2\t4\n",
            ".\t.\t.\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 10 | Action: 2 | Reward: 0.00 | Score: 48.0\n",
            "2\t.\t.\t.\n",
            "2\t.\t.\t.\n",
            "2\t4\t.\t2\n",
            "16\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 11 | Action: 3 | Reward: 0.00 | Score: 48.0\n",
            ".\t.\t.\t2\n",
            ".\t.\t.\t2\n",
            ".\t2\t4\t2\n",
            ".\t.\t4\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 12 | Action: 3 | Reward: -1.00 | Score: 48.0\n",
            ".\t.\t.\t2\n",
            ".\t.\t.\t2\n",
            ".\t2\t4\t2\n",
            ".\t.\t4\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 13 | Action: 0 | Reward: 0.38 | Score: 60.0\n",
            ".\t2\t8\t4\n",
            "2\t.\t.\t2\n",
            ".\t.\t.\t16\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 14 | Action: 1 | Reward: 0.00 | Score: 60.0\n",
            ".\t.\t.\t.\n",
            ".\t.\t2\t4\n",
            ".\t.\t.\t2\n",
            "2\t2\t8\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 15 | Action: 2 | Reward: 0.12 | Score: 64.0\n",
            ".\t2\t.\t.\n",
            "2\t4\t.\t.\n",
            "2\t.\t.\t.\n",
            "4\t8\t16\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 16 | Action: 2 | Reward: 0.00 | Score: 64.0\n",
            "2\t.\t.\t.\n",
            "2\t4\t.\t.\n",
            "2\t.\t.\t2\n",
            "4\t8\t16\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 17 | Action: 3 | Reward: 0.12 | Score: 68.0\n",
            "2\t.\t.\t2\n",
            ".\t.\t2\t4\n",
            ".\t.\t.\t4\n",
            ".\t4\t8\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 18 | Action: 0 | Reward: 0.25 | Score: 76.0\n",
            "2\t4\t2\t2\n",
            "2\t.\t8\t8\n",
            ".\t.\t.\t16\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 19 | Action: 1 | Reward: 0.12 | Score: 80.0\n",
            ".\t.\t.\t.\n",
            ".\t2\t.\t2\n",
            ".\t.\t2\t8\n",
            "4\t4\t8\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 20 | Action: 3 | Reward: 0.38 | Score: 92.0\n",
            ".\t.\t.\t.\n",
            ".\t.\t.\t4\n",
            "2\t.\t2\t8\n",
            ".\t8\t8\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 21 | Action: 2 | Reward: 0.62 | Score: 112.0\n",
            "2\t.\t.\t.\n",
            "4\t.\t.\t.\n",
            "4\t8\t.\t.\n",
            "16\t16\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 22 | Action: 0 | Reward: 0.25 | Score: 120.0\n",
            "2\t8\t.\t2\n",
            "8\t16\t.\t.\n",
            "16\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 23 | Action: 0 | Reward: -1.00 | Score: 120.0\n",
            "2\t8\t.\t2\n",
            "8\t16\t.\t.\n",
            "16\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 24 | Action: 2 | Reward: 0.00 | Score: 120.0\n",
            "2\t8\t2\t.\n",
            "8\t16\t2\t.\n",
            "16\t.\t.\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 25 | Action: 0 | Reward: 0.12 | Score: 124.0\n",
            "2\t8\t4\t.\n",
            "8\t16\t.\t.\n",
            "16\t.\t.\t.\n",
            ".\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 26 | Action: 2 | Reward: 0.00 | Score: 124.0\n",
            "2\t8\t4\t.\n",
            "8\t16\t.\t.\n",
            "16\t2\t.\t.\n",
            "2\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 27 | Action: 1 | Reward: 0.00 | Score: 124.0\n",
            "2\t.\t.\t.\n",
            "8\t8\t.\t.\n",
            "16\t16\t.\t.\n",
            "2\t2\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 28 | Action: 3 | Reward: 1.62 | Score: 176.0\n",
            ".\t.\t.\t2\n",
            ".\t.\t.\t16\n",
            "2\t.\t.\t32\n",
            ".\t4\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 29 | Action: 1 | Reward: 0.00 | Score: 176.0\n",
            ".\t.\t2\t2\n",
            ".\t.\t.\t16\n",
            ".\t.\t.\t32\n",
            "2\t4\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 30 | Action: 3 | Reward: 0.38 | Score: 188.0\n",
            ".\t.\t.\t4\n",
            ".\t2\t.\t16\n",
            ".\t.\t.\t32\n",
            ".\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 31 | Action: 3 | Reward: 0.00 | Score: 188.0\n",
            ".\t.\t.\t4\n",
            ".\t.\t2\t16\n",
            ".\t2\t.\t32\n",
            ".\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 32 | Action: 2 | Reward: 0.00 | Score: 188.0\n",
            "4\t.\t.\t.\n",
            "2\t16\t.\t.\n",
            "2\t32\t2\t.\n",
            "2\t8\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 33 | Action: 0 | Reward: 0.25 | Score: 196.0\n",
            "4\t16\t4\t.\n",
            "4\t32\t.\t2\n",
            "2\t8\t.\t.\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 34 | Action: 2 | Reward: 0.00 | Score: 196.0\n",
            "4\t16\t4\t.\n",
            "4\t32\t2\t.\n",
            "2\t8\t.\t.\n",
            ".\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 35 | Action: 0 | Reward: 0.25 | Score: 204.0\n",
            "8\t16\t4\t2\n",
            "2\t32\t2\t.\n",
            ".\t8\t.\t4\n",
            ".\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 36 | Action: 1 | Reward: 0.00 | Score: 204.0\n",
            ".\t.\t.\t.\n",
            ".\t16\t2\t.\n",
            "8\t32\t4\t2\n",
            "2\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 37 | Action: 0 | Reward: 0.00 | Score: 204.0\n",
            "8\t16\t2\t2\n",
            "2\t32\t4\t4\n",
            ".\t8\t2\t.\n",
            ".\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 38 | Action: 2 | Reward: 0.38 | Score: 216.0\n",
            "8\t16\t4\t.\n",
            "2\t32\t8\t.\n",
            "8\t2\t.\t.\n",
            "2\t.\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 39 | Action: 1 | Reward: 0.00 | Score: 216.0\n",
            "8\t.\t.\t.\n",
            "2\t16\t4\t.\n",
            "8\t32\t8\t.\n",
            "2\t2\t2\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 40 | Action: 0 | Reward: 0.00 | Score: 216.0\n",
            "8\t16\t4\t2\n",
            "2\t32\t8\t.\n",
            "8\t2\t2\t4\n",
            "2\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 41 | Action: 2 | Reward: 0.12 | Score: 220.0\n",
            "8\t16\t4\t2\n",
            "2\t32\t8\t.\n",
            "8\t4\t4\t.\n",
            "2\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 42 | Action: 0 | Reward: 0.12 | Score: 224.0\n",
            "8\t16\t4\t4\n",
            "2\t32\t8\t2\n",
            "8\t4\t4\t.\n",
            "2\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 43 | Action: 2 | Reward: 0.50 | Score: 240.0\n",
            "8\t16\t8\t.\n",
            "2\t32\t8\t2\n",
            "8\t8\t.\t.\n",
            "2\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 44 | Action: 2 | Reward: 0.62 | Score: 260.0\n",
            "8\t16\t8\t.\n",
            "2\t32\t8\t2\n",
            "16\t.\t.\t.\n",
            "4\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 45 | Action: 2 | Reward: 0.00 | Score: 260.0\n",
            "8\t16\t8\t.\n",
            "2\t32\t8\t2\n",
            "16\t.\t.\t4\n",
            "4\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 46 | Action: 2 | Reward: 0.00 | Score: 260.0\n",
            "8\t16\t8\t.\n",
            "2\t32\t8\t2\n",
            "16\t4\t.\t.\n",
            "4\t2\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 47 | Action: 2 | Reward: 0.12 | Score: 264.0\n",
            "8\t16\t8\t.\n",
            "2\t32\t8\t2\n",
            "16\t4\t.\t.\n",
            "4\t4\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 48 | Action: 2 | Reward: 0.25 | Score: 272.0\n",
            "8\t16\t8\t.\n",
            "2\t32\t8\t2\n",
            "16\t4\t.\t.\n",
            "8\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 49 | Action: 2 | Reward: -1.00 | Score: 272.0\n",
            "8\t16\t8\t.\n",
            "2\t32\t8\t2\n",
            "16\t4\t.\t.\n",
            "8\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 50 | Action: 2 | Reward: -1.00 | Score: 272.0\n",
            "8\t16\t8\t.\n",
            "2\t32\t8\t2\n",
            "16\t4\t.\t.\n",
            "8\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 51 | Action: 2 | Reward: -1.00 | Score: 272.0\n",
            "8\t16\t8\t.\n",
            "2\t32\t8\t2\n",
            "16\t4\t.\t.\n",
            "8\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 52 | Action: 2 | Reward: -1.00 | Score: 272.0\n",
            "8\t16\t8\t.\n",
            "2\t32\t8\t2\n",
            "16\t4\t.\t.\n",
            "8\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 53 | Action: 3 | Reward: 0.00 | Score: 272.0\n",
            ".\t8\t16\t8\n",
            "2\t32\t8\t2\n",
            ".\t.\t16\t4\n",
            "2\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 54 | Action: 2 | Reward: 0.00 | Score: 272.0\n",
            "8\t16\t8\t.\n",
            "2\t32\t8\t2\n",
            "16\t4\t.\t2\n",
            "2\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 55 | Action: 1 | Reward: 0.62 | Score: 292.0\n",
            "8\t16\t.\t.\n",
            "2\t32\t.\t2\n",
            "16\t4\t16\t4\n",
            "2\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 56 | Action: 0 | Reward: 0.25 | Score: 300.0\n",
            "8\t16\t16\t2\n",
            "2\t32\t2\t8\n",
            "16\t4\t4\t.\n",
            "2\t8\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 57 | Action: 1 | Reward: 0.00 | Score: 300.0\n",
            "8\t16\t.\t.\n",
            "2\t32\t16\t2\n",
            "16\t4\t2\t2\n",
            "2\t8\t4\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 58 | Action: 0 | Reward: 0.12 | Score: 304.0\n",
            "8\t16\t16\t4\n",
            "2\t32\t2\t8\n",
            "16\t4\t4\t.\n",
            "2\t8\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 59 | Action: 3 | Reward: 1.25 | Score: 344.0\n",
            ".\t8\t32\t4\n",
            "2\t32\t2\t8\n",
            ".\t.\t16\t8\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 60 | Action: 2 | Reward: 0.00 | Score: 344.0\n",
            "8\t32\t4\t.\n",
            "2\t32\t2\t8\n",
            "16\t8\t.\t2\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 61 | Action: 3 | Reward: 0.00 | Score: 344.0\n",
            "2\t8\t32\t4\n",
            "2\t32\t2\t8\n",
            ".\t16\t8\t2\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 62 | Action: 3 | Reward: -1.00 | Score: 344.0\n",
            "2\t8\t32\t4\n",
            "2\t32\t2\t8\n",
            ".\t16\t8\t2\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 63 | Action: 3 | Reward: -1.00 | Score: 344.0\n",
            "2\t8\t32\t4\n",
            "2\t32\t2\t8\n",
            ".\t16\t8\t2\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 64 | Action: 2 | Reward: 0.00 | Score: 344.0\n",
            "2\t8\t32\t4\n",
            "2\t32\t2\t8\n",
            "16\t8\t2\t2\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 65 | Action: 1 | Reward: 0.38 | Score: 356.0\n",
            "2\t8\t.\t.\n",
            "4\t32\t32\t4\n",
            "16\t8\t4\t8\n",
            "4\t2\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 66 | Action: 0 | Reward: 0.00 | Score: 356.0\n",
            "2\t8\t32\t4\n",
            "4\t32\t4\t8\n",
            "16\t8\t8\t4\n",
            "4\t2\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 67 | Action: 1 | Reward: 0.00 | Score: 356.0\n",
            "2\t8\t2\t4\n",
            "4\t32\t32\t8\n",
            "16\t8\t4\t4\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 68 | Action: 0 | Reward: -1.00 | Score: 356.0\n",
            "2\t8\t2\t4\n",
            "4\t32\t32\t8\n",
            "16\t8\t4\t4\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 69 | Action: 1 | Reward: -1.00 | Score: 356.0\n",
            "2\t8\t2\t4\n",
            "4\t32\t32\t8\n",
            "16\t8\t4\t4\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 70 | Action: 1 | Reward: -1.00 | Score: 356.0\n",
            "2\t8\t2\t4\n",
            "4\t32\t32\t8\n",
            "16\t8\t4\t4\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 71 | Action: 3 | Reward: 2.25 | Score: 428.0\n",
            "2\t8\t2\t4\n",
            ".\t4\t64\t8\n",
            "2\t16\t8\t8\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 72 | Action: 3 | Reward: 0.50 | Score: 444.0\n",
            "2\t8\t2\t4\n",
            ".\t4\t64\t8\n",
            "2\t2\t16\t16\n",
            "4\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 73 | Action: 1 | Reward: 0.25 | Score: 452.0\n",
            "2\t.\t2\t4\n",
            ".\t8\t64\t8\n",
            "4\t4\t16\t16\n",
            "4\t4\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 74 | Action: 3 | Reward: 1.62 | Score: 504.0\n",
            ".\t.\t4\t4\n",
            "2\t8\t64\t8\n",
            ".\t.\t8\t32\n",
            ".\t8\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 75 | Action: 1 | Reward: 1.00 | Score: 536.0\n",
            ".\t.\t.\t4\n",
            ".\t2\t4\t8\n",
            ".\t.\t64\t32\n",
            "2\t16\t16\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 76 | Action: 3 | Reward: 1.00 | Score: 568.0\n",
            ".\t.\t.\t4\n",
            ".\t2\t4\t8\n",
            "2\t.\t64\t32\n",
            ".\t2\t32\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 77 | Action: 3 | Reward: 0.00 | Score: 568.0\n",
            ".\t2\t.\t4\n",
            ".\t2\t4\t8\n",
            ".\t2\t64\t32\n",
            ".\t2\t32\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 78 | Action: 3 | Reward: 0.00 | Score: 568.0\n",
            ".\t2\t2\t4\n",
            ".\t2\t4\t8\n",
            ".\t2\t64\t32\n",
            ".\t2\t32\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 79 | Action: 0 | Reward: 0.25 | Score: 576.0\n",
            ".\t4\t2\t4\n",
            "4\t4\t4\t8\n",
            ".\t.\t64\t32\n",
            ".\t.\t32\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 80 | Action: 2 | Reward: 0.25 | Score: 584.0\n",
            "4\t2\t4\t2\n",
            "8\t4\t8\t.\n",
            "64\t32\t.\t.\n",
            "32\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 81 | Action: 1 | Reward: 0.00 | Score: 584.0\n",
            "4\t2\t.\t.\n",
            "8\t4\t.\t2\n",
            "64\t32\t4\t.\n",
            "32\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 82 | Action: 2 | Reward: 0.00 | Score: 584.0\n",
            "4\t2\t.\t.\n",
            "8\t4\t2\t.\n",
            "64\t32\t4\t2\n",
            "32\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 83 | Action: 2 | Reward: -1.00 | Score: 584.0\n",
            "4\t2\t.\t.\n",
            "8\t4\t2\t.\n",
            "64\t32\t4\t2\n",
            "32\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 84 | Action: 2 | Reward: -1.00 | Score: 584.0\n",
            "4\t2\t.\t.\n",
            "8\t4\t2\t.\n",
            "64\t32\t4\t2\n",
            "32\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 85 | Action: 0 | Reward: 0.12 | Score: 588.0\n",
            "4\t2\t2\t4\n",
            "8\t4\t4\t.\n",
            "64\t32\t8\t.\n",
            "32\t2\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 86 | Action: 2 | Reward: 0.50 | Score: 604.0\n",
            "4\t4\t4\t.\n",
            "8\t8\t.\t.\n",
            "64\t32\t8\t4\n",
            "32\t4\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 87 | Action: 0 | Reward: 0.00 | Score: 604.0\n",
            "4\t4\t4\t4\n",
            "8\t8\t8\t.\n",
            "64\t32\t.\t.\n",
            "32\t4\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 88 | Action: 0 | Reward: 0.00 | Score: 604.0\n",
            "4\t4\t4\t4\n",
            "8\t8\t8\t2\n",
            "64\t32\t.\t.\n",
            "32\t4\t.\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 89 | Action: 2 | Reward: 1.25 | Score: 644.0\n",
            "8\t8\t.\t.\n",
            "16\t8\t2\t.\n",
            "64\t32\t.\t4\n",
            "32\t8\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 90 | Action: 1 | Reward: 0.50 | Score: 660.0\n",
            "8\t2\t.\t.\n",
            "16\t16\t.\t.\n",
            "64\t32\t.\t.\n",
            "32\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 91 | Action: 1 | Reward: -1.00 | Score: 660.0\n",
            "8\t2\t.\t.\n",
            "16\t16\t.\t.\n",
            "64\t32\t.\t.\n",
            "32\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 92 | Action: 3 | Reward: 1.00 | Score: 692.0\n",
            ".\t.\t8\t2\n",
            ".\t2\t.\t32\n",
            ".\t.\t64\t32\n",
            "32\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 93 | Action: 2 | Reward: 0.00 | Score: 692.0\n",
            "8\t2\t.\t.\n",
            "2\t32\t.\t.\n",
            "64\t32\t.\t2\n",
            "32\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 94 | Action: 0 | Reward: 2.00 | Score: 756.0\n",
            "8\t2\t2\t2\n",
            "2\t64\t.\t4\n",
            "64\t8\t.\t.\n",
            "32\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 95 | Action: 1 | Reward: 0.00 | Score: 756.0\n",
            "8\t2\t.\t.\n",
            "2\t2\t.\t2\n",
            "64\t64\t.\t4\n",
            "32\t8\t2\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 96 | Action: 3 | Reward: 4.25 | Score: 892.0\n",
            ".\t.\t8\t2\n",
            ".\t2\t2\t4\n",
            ".\t.\t128\t4\n",
            ".\t32\t8\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 97 | Action: 1 | Reward: 0.25 | Score: 900.0\n",
            ".\t.\t8\t.\n",
            ".\t2\t2\t2\n",
            ".\t2\t128\t4\n",
            ".\t32\t8\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 98 | Action: 3 | Reward: 0.62 | Score: 920.0\n",
            ".\t.\t.\t8\n",
            ".\t.\t2\t4\n",
            "4\t2\t128\t4\n",
            ".\t.\t32\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 99 | Action: 3 | Reward: -1.00 | Score: 920.0\n",
            ".\t.\t.\t8\n",
            ".\t.\t2\t4\n",
            "4\t2\t128\t4\n",
            ".\t.\t32\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 100 | Action: 0 | Reward: 0.25 | Score: 928.0\n",
            "4\t2\t2\t8\n",
            ".\t.\t128\t8\n",
            ".\t.\t32\t16\n",
            ".\t.\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 101 | Action: 1 | Reward: 0.50 | Score: 944.0\n",
            ".\t.\t2\t2\n",
            ".\t.\t128\t.\n",
            ".\t.\t32\t16\n",
            "4\t2\t2\t16\n",
            "----------------------------------------\n",
            "\n",
            "Step 102 | Action: 2 | Reward: 0.25 | Score: 952.0\n",
            "4\t.\t.\t.\n",
            "128\t.\t4\t.\n",
            "32\t16\t.\t.\n",
            "4\t4\t16\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 103 | Action: 0 | Reward: 0.00 | Score: 952.0\n",
            "4\t16\t4\t.\n",
            "128\t4\t16\t.\n",
            "32\t.\t.\t.\n",
            "4\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 104 | Action: 2 | Reward: 0.00 | Score: 952.0\n",
            "4\t16\t4\t.\n",
            "128\t4\t16\t.\n",
            "32\t.\t.\t.\n",
            "4\t2\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 105 | Action: 0 | Reward: 0.00 | Score: 952.0\n",
            "4\t16\t4\t2\n",
            "128\t4\t16\t.\n",
            "32\t2\t.\t2\n",
            "4\t.\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 106 | Action: 1 | Reward: 0.12 | Score: 956.0\n",
            "4\t.\t.\t.\n",
            "128\t16\t.\t.\n",
            "32\t4\t4\t2\n",
            "4\t2\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 107 | Action: 2 | Reward: 0.25 | Score: 964.0\n",
            "4\t.\t.\t2\n",
            "128\t16\t.\t.\n",
            "32\t8\t2\t.\n",
            "4\t2\t16\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 108 | Action: 0 | Reward: 0.00 | Score: 964.0\n",
            "4\t16\t2\t2\n",
            "128\t8\t16\t4\n",
            "32\t2\t.\t.\n",
            "4\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 109 | Action: 0 | Reward: 0.00 | Score: 964.0\n",
            "4\t16\t2\t2\n",
            "128\t8\t16\t4\n",
            "32\t2\t.\t2\n",
            "4\t.\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 110 | Action: 0 | Reward: 0.12 | Score: 968.0\n",
            "4\t16\t2\t2\n",
            "128\t8\t16\t4\n",
            "32\t2\t.\t4\n",
            "4\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 111 | Action: 0 | Reward: 0.38 | Score: 980.0\n",
            "4\t16\t2\t2\n",
            "128\t8\t16\t8\n",
            "32\t4\t.\t.\n",
            "4\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 112 | Action: 0 | Reward: -1.00 | Score: 980.0\n",
            "4\t16\t2\t2\n",
            "128\t8\t16\t8\n",
            "32\t4\t.\t.\n",
            "4\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 113 | Action: 1 | Reward: 0.00 | Score: 980.0\n",
            "4\t16\t.\t2\n",
            "128\t8\t.\t.\n",
            "32\t4\t2\t2\n",
            "4\t2\t16\t8\n",
            "----------------------------------------\n",
            "\n",
            "Step 114 | Action: 0 | Reward: 0.12 | Score: 984.0\n",
            "4\t16\t2\t4\n",
            "128\t8\t16\t8\n",
            "32\t4\t.\t.\n",
            "4\t2\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 115 | Action: 0 | Reward: 0.00 | Score: 984.0\n",
            "4\t16\t2\t4\n",
            "128\t8\t16\t8\n",
            "32\t4\t2\t2\n",
            "4\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 116 | Action: 0 | Reward: -1.00 | Score: 984.0\n",
            "4\t16\t2\t4\n",
            "128\t8\t16\t8\n",
            "32\t4\t2\t2\n",
            "4\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 117 | Action: 0 | Reward: -1.00 | Score: 984.0\n",
            "4\t16\t2\t4\n",
            "128\t8\t16\t8\n",
            "32\t4\t2\t2\n",
            "4\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 118 | Action: 0 | Reward: -1.00 | Score: 984.0\n",
            "4\t16\t2\t4\n",
            "128\t8\t16\t8\n",
            "32\t4\t2\t2\n",
            "4\t2\t.\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 119 | Action: 3 | Reward: 0.12 | Score: 988.0\n",
            "4\t16\t2\t4\n",
            "128\t8\t16\t8\n",
            ".\t32\t4\t4\n",
            ".\t2\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 120 | Action: 2 | Reward: 0.25 | Score: 996.0\n",
            "4\t16\t2\t4\n",
            "128\t8\t16\t8\n",
            "32\t8\t2\t.\n",
            "2\t4\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 121 | Action: 0 | Reward: 0.62 | Score: 1016.0\n",
            "4\t16\t2\t4\n",
            "128\t16\t16\t8\n",
            "32\t4\t4\t.\n",
            "2\t.\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 122 | Action: 0 | Reward: 1.00 | Score: 1048.0\n",
            "4\t32\t2\t4\n",
            "128\t4\t16\t8\n",
            "32\t.\t4\t.\n",
            "2\t.\t2\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 123 | Action: 2 | Reward: 0.12 | Score: 1052.0\n",
            "4\t32\t2\t4\n",
            "128\t4\t16\t8\n",
            "32\t4\t.\t.\n",
            "4\t2\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 124 | Action: 3 | Reward: 0.12 | Score: 1056.0\n",
            "4\t32\t2\t4\n",
            "128\t4\t16\t8\n",
            ".\t.\t32\t4\n",
            ".\t4\t4\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 125 | Action: 0 | Reward: 0.50 | Score: 1072.0\n",
            "4\t32\t2\t4\n",
            "128\t8\t16\t8\n",
            ".\t.\t32\t8\n",
            ".\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 126 | Action: 0 | Reward: 0.50 | Score: 1088.0\n",
            "4\t32\t2\t4\n",
            "128\t8\t16\t16\n",
            ".\t2\t32\t2\n",
            ".\t.\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 127 | Action: 1 | Reward: 0.00 | Score: 1088.0\n",
            ".\t.\t2\t.\n",
            "2\t32\t16\t4\n",
            "4\t8\t32\t16\n",
            "128\t2\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 128 | Action: 1 | Reward: -1.00 | Score: 1088.0\n",
            ".\t.\t2\t.\n",
            "2\t32\t16\t4\n",
            "4\t8\t32\t16\n",
            "128\t2\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 129 | Action: 1 | Reward: -1.00 | Score: 1088.0\n",
            ".\t.\t2\t.\n",
            "2\t32\t16\t4\n",
            "4\t8\t32\t16\n",
            "128\t2\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 130 | Action: 2 | Reward: 0.00 | Score: 1088.0\n",
            "2\t.\t2\t.\n",
            "2\t32\t16\t4\n",
            "4\t8\t32\t16\n",
            "128\t2\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 131 | Action: 1 | Reward: 0.12 | Score: 1092.0\n",
            ".\t.\t2\t2\n",
            "4\t32\t16\t4\n",
            "4\t8\t32\t16\n",
            "128\t2\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 132 | Action: 0 | Reward: 0.25 | Score: 1100.0\n",
            "8\t32\t2\t2\n",
            "128\t8\t16\t4\n",
            ".\t2\t32\t16\n",
            "2\t.\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 133 | Action: 3 | Reward: 0.12 | Score: 1104.0\n",
            ".\t8\t32\t4\n",
            "128\t8\t16\t4\n",
            ".\t2\t32\t16\n",
            "2\t2\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 134 | Action: 3 | Reward: 0.12 | Score: 1108.0\n",
            "2\t8\t32\t4\n",
            "128\t8\t16\t4\n",
            ".\t2\t32\t16\n",
            ".\t4\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 135 | Action: 1 | Reward: 0.75 | Score: 1132.0\n",
            "2\t.\t32\t.\n",
            ".\t16\t16\t8\n",
            "2\t2\t32\t16\n",
            "128\t4\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 136 | Action: 0 | Reward: 0.12 | Score: 1136.0\n",
            "4\t16\t32\t8\n",
            "128\t2\t16\t16\n",
            ".\t4\t32\t2\n",
            ".\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 137 | Action: 0 | Reward: -1.00 | Score: 1136.0\n",
            "4\t16\t32\t8\n",
            "128\t2\t16\t16\n",
            ".\t4\t32\t2\n",
            ".\t2\t4\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 138 | Action: 3 | Reward: 1.00 | Score: 1168.0\n",
            "4\t16\t32\t8\n",
            "2\t128\t2\t32\n",
            ".\t4\t32\t2\n",
            ".\t.\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 139 | Action: 2 | Reward: 0.00 | Score: 1168.0\n",
            "4\t16\t32\t8\n",
            "2\t128\t2\t32\n",
            "4\t32\t2\t.\n",
            "2\t4\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 140 | Action: 3 | Reward: 0.00 | Score: 1168.0\n",
            "4\t16\t32\t8\n",
            "2\t128\t2\t32\n",
            ".\t4\t32\t2\n",
            "2\t2\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 141 | Action: 3 | Reward: 0.12 | Score: 1172.0\n",
            "4\t16\t32\t8\n",
            "2\t128\t2\t32\n",
            "2\t4\t32\t2\n",
            ".\t4\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 142 | Action: 2 | Reward: 0.25 | Score: 1180.0\n",
            "4\t16\t32\t8\n",
            "2\t128\t2\t32\n",
            "2\t4\t32\t2\n",
            "8\t2\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 143 | Action: 2 | Reward: 0.12 | Score: 1184.0\n",
            "4\t16\t32\t8\n",
            "2\t128\t2\t32\n",
            "2\t4\t32\t2\n",
            "8\t4\t.\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 144 | Action: 0 | Reward: 0.50 | Score: 1200.0\n",
            "4\t16\t32\t8\n",
            "4\t128\t2\t32\n",
            "8\t8\t32\t4\n",
            ".\t.\t2\t.\n",
            "----------------------------------------\n",
            "\n",
            "Step 145 | Action: 1 | Reward: 0.25 | Score: 1208.0\n",
            "2\t.\t32\t.\n",
            ".\t16\t2\t8\n",
            "8\t128\t32\t32\n",
            "8\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 146 | Action: 1 | Reward: 0.50 | Score: 1224.0\n",
            ".\t2\t32\t.\n",
            ".\t16\t2\t8\n",
            "2\t128\t32\t32\n",
            "16\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 147 | Action: 1 | Reward: -1.00 | Score: 1224.0\n",
            ".\t2\t32\t.\n",
            ".\t16\t2\t8\n",
            "2\t128\t32\t32\n",
            "16\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 148 | Action: 2 | Reward: 2.00 | Score: 1288.0\n",
            "2\t32\t2\t.\n",
            "16\t2\t8\t.\n",
            "2\t128\t64\t.\n",
            "16\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 149 | Action: 3 | Reward: 0.00 | Score: 1288.0\n",
            "2\t2\t32\t2\n",
            ".\t16\t2\t8\n",
            ".\t2\t128\t64\n",
            "16\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 150 | Action: 1 | Reward: 0.00 | Score: 1288.0\n",
            ".\t2\t32\t2\n",
            "2\t16\t2\t8\n",
            "2\t2\t128\t64\n",
            "16\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 151 | Action: 0 | Reward: 0.12 | Score: 1292.0\n",
            "4\t2\t32\t2\n",
            "16\t16\t2\t8\n",
            "2\t2\t128\t64\n",
            ".\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 152 | Action: 3 | Reward: 1.12 | Score: 1328.0\n",
            "4\t2\t32\t2\n",
            ".\t32\t2\t8\n",
            ".\t4\t128\t64\n",
            "2\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 153 | Action: 0 | Reward: 0.00 | Score: 1328.0\n",
            "4\t2\t32\t2\n",
            "2\t32\t2\t8\n",
            "2\t4\t128\t64\n",
            ".\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 154 | Action: 3 | Reward: -1.00 | Score: 1328.0\n",
            "4\t2\t32\t2\n",
            "2\t32\t2\t8\n",
            "2\t4\t128\t64\n",
            ".\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 155 | Action: 0 | Reward: 0.12 | Score: 1332.0\n",
            "4\t2\t32\t2\n",
            "4\t32\t2\t8\n",
            ".\t4\t128\t64\n",
            "2\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 156 | Action: 0 | Reward: 0.25 | Score: 1340.0\n",
            "8\t2\t32\t2\n",
            "2\t32\t2\t8\n",
            "2\t4\t128\t64\n",
            ".\t8\t2\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 157 | Action: 2 | Reward: 0.00 | Score: 1340.0\n",
            "8\t2\t32\t2\n",
            "2\t32\t2\t8\n",
            "2\t4\t128\t64\n",
            "8\t2\t4\t4\n",
            "----------------------------------------\n",
            "\n",
            "Step 158 | Action: 2 | Reward: 0.25 | Score: 1348.0\n",
            "8\t2\t32\t2\n",
            "2\t32\t2\t8\n",
            "2\t4\t128\t64\n",
            "8\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 159 | Action: 0 | Reward: 0.12 | Score: 1352.0\n",
            "8\t2\t32\t2\n",
            "4\t32\t2\t8\n",
            "8\t4\t128\t64\n",
            "2\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 160 | Action: 1 | Reward: -1.00 | Score: 1352.0\n",
            "8\t2\t32\t2\n",
            "4\t32\t2\t8\n",
            "8\t4\t128\t64\n",
            "2\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 161 | Action: 1 | Reward: -1.00 | Score: 1352.0\n",
            "8\t2\t32\t2\n",
            "4\t32\t2\t8\n",
            "8\t4\t128\t64\n",
            "2\t2\t8\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 162 | Action: 2 | Reward: 0.12 | Score: 1356.0\n",
            "8\t2\t32\t2\n",
            "4\t32\t2\t8\n",
            "8\t4\t128\t64\n",
            "4\t8\t2\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 163 | Action: 0 | Reward: -1.00 | Score: 1356.0\n",
            "8\t2\t32\t2\n",
            "4\t32\t2\t8\n",
            "8\t4\t128\t64\n",
            "4\t8\t2\t2\n",
            "----------------------------------------\n",
            "\n",
            "Step 164 | Action: 2 | Reward: 0.12 | Score: 1360.0\n",
            "8\t2\t32\t2\n",
            "4\t32\t2\t8\n",
            "8\t4\t128\t64\n",
            "4\t8\t4\t2\n",
            "----------------------------------------\n",
            "\n",
            "=== Episode Finished ===\n",
            "Final Score: 1360.0\n",
            "Max Tile: 128\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "from gymnasium import spaces\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "class TwentyFortyEightEnv(gym.Env):\n",
        "    metadata = {\"render_modes\": [\"human\"], \"render_fps\": 4}\n",
        "\n",
        "    def __init__(self, render_mode=None):\n",
        "        super().__init__()\n",
        "        self.board_size = 4\n",
        "\n",
        "        # Actions: 0 = up, 1 = down, 2 = left, 3 = right\n",
        "        self.action_space = spaces.Discrete(4)\n",
        "\n",
        "        # Board is 4x4 integers; observation is a flattened vector of size 16\n",
        "        self.observation_space = spaces.Box(\n",
        "            low=0,\n",
        "            high=2**16,\n",
        "            shape=(16,),\n",
        "            dtype=np.int32\n",
        "        )\n",
        "\n",
        "        self.render_mode = render_mode\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self, seed=None, options=None):\n",
        "        super().reset(seed=seed)\n",
        "\n",
        "        self.board = np.zeros((self.board_size, self.board_size), dtype=np.int32)\n",
        "        self.score = 0\n",
        "\n",
        "        self._add_tile()\n",
        "        self._add_tile()\n",
        "\n",
        "        return self._get_obs(), {}\n",
        "\n",
        "    def step(self, action):\n",
        "        old_board = self.board[:][:]\n",
        "\n",
        "        if action == 0:\n",
        "            reward = self._move_up()\n",
        "        elif action == 1:\n",
        "            reward = self._move_down()\n",
        "        elif action == 2:\n",
        "            reward = self._move_left()\n",
        "        elif action == 3:\n",
        "            reward = self._move_right()\n",
        "\n",
        "        # Invalid move (board unchanged)\n",
        "        if np.array_equal(self.board, old_board):\n",
        "            reward = -2  # small penalty for useless actions\n",
        "        else:\n",
        "            # Only add a tile after a valid move\n",
        "            self._add_tile()\n",
        "\n",
        "        done = not self._moves_available()\n",
        "\n",
        "        return self._get_obs(), reward, done, False, {\"score\": self.score}\n",
        "\n",
        "    # -------- Rendering -------- #\n",
        "\n",
        "    def render(self):\n",
        "        print(\"\\nScore:\", self.score)\n",
        "        print(\"-\" * 25)\n",
        "        for row in self.board:\n",
        "            print(\"|\" + \"|\".join(f\"{num:^5}\" if num != 0 else \"     \" for num in row) + \"|\")\n",
        "            print(\"-\" * 25)\n",
        "\n",
        "    # -------- Helper Methods -------- #\n",
        "\n",
        "    def _get_obs(self):\n",
        "        return self.board.flatten()\n",
        "\n",
        "    def _add_tile(self):\n",
        "        empty = list(zip(*np.where(self.board == 0)))\n",
        "        if not empty:\n",
        "            return\n",
        "        i, j = random.choice(empty)\n",
        "        self.board[i, j] = 4 if random.random() < 0.1 else 2\n",
        "\n",
        "    def _moves_available(self):\n",
        "        if np.any(self.board == 0):\n",
        "            return True\n",
        "        for i in range(4):\n",
        "            for j in range(4):\n",
        "                if j < 3 and self.board[i, j] == self.board[i, j + 1]:\n",
        "                    return True\n",
        "                if i < 3 and self.board[i, j] == self.board[i + 1, j]:\n",
        "                    return True\n",
        "        return False\n",
        "\n",
        "    # -------- Movement Logic -------- #\n",
        "\n",
        "    def _compress(self, row):\n",
        "        new = row[row != 0]\n",
        "        return np.concatenate([new, np.zeros(4 - len(new), dtype=np.int32)])\n",
        "\n",
        "    def _merge(self, row):\n",
        "        score_gain = 0\n",
        "        for i in range(3):\n",
        "            if row[i] != 0 and row[i] == row[i + 1]:\n",
        "                row[i] *= 2\n",
        "                row[i + 1] = 0\n",
        "                score_gain += row[i]\n",
        "        return row, score_gain\n",
        "\n",
        "    def _move_left(self):\n",
        "        total_gain = 0\n",
        "        new_board = np.zeros((self.board_size, self.board_size), dtype=np.int32)\n",
        "\n",
        "        for i in range(4):\n",
        "            row = self._compress(self.board[i])\n",
        "            row, gain = self._merge(row)\n",
        "            row = self._compress(row)\n",
        "\n",
        "            new_board[i] = row\n",
        "            total_gain += gain\n",
        "\n",
        "        self.board = new_board\n",
        "        self.score += total_gain\n",
        "        return float(total_gain)\n",
        "\n",
        "    def _move_right(self):\n",
        "        self.board = np.fliplr(self.board)\n",
        "        reward = self._move_left()\n",
        "        self.board = np.fliplr(self.board)\n",
        "        return reward\n",
        "\n",
        "    def _move_up(self):\n",
        "        self.board = self.board.T\n",
        "        reward = self._move_left()\n",
        "        self.board = self.board.T\n",
        "        return reward\n",
        "\n",
        "    def _move_down(self):\n",
        "        self.board = self.board.T\n",
        "        reward = self._move_right()\n",
        "        self.board = self.board.T\n",
        "        return reward\n"
      ],
      "metadata": {
        "id": "2AmJ-I7sidR6"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class TwentyFortyEightEnvMasked(TwentyFortyEightEnv):\n",
        "    \"\"\"\n",
        "    Same behavior as TwentyFortyEightEnv, but with get_action_mask()\n",
        "    that uses the EXACT same move logic to test if an action changes the board.\n",
        "    \"\"\"\n",
        "\n",
        "    def _simulate_action_changes_board(self, action: int) -> bool:\n",
        "        \"\"\"\n",
        "        Return True if taking `action` would change the board (ignoring tile spawn),\n",
        "        using the real _move_* methods, without permanently changing env state.\n",
        "        \"\"\"\n",
        "        # Backup current state\n",
        "        old_board = self.board.copy()\n",
        "        old_score = self.score\n",
        "\n",
        "        # Apply the move using real env logic (no tile added here)\n",
        "        if action == 0:\n",
        "            self._move_up()\n",
        "        elif action == 1:\n",
        "            self._move_down()\n",
        "        elif action == 2:\n",
        "            self._move_left()\n",
        "        elif action == 3:\n",
        "            self._move_right()\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid action {action}\")\n",
        "\n",
        "        new_board = self.board.copy()\n",
        "\n",
        "        # Restore original state\n",
        "        self.board = old_board\n",
        "        self.score = old_score\n",
        "\n",
        "        # If board changed, action is valid\n",
        "        return not np.array_equal(new_board, old_board)\n",
        "\n",
        "    def get_action_mask(self) -> np.ndarray:\n",
        "        \"\"\"\n",
        "        Returns a boolean mask of shape (4,):\n",
        "        True = action changes the board (valid),\n",
        "        False = action leaves the board unchanged (no-op).\n",
        "        Order: 0=up, 1=down, 2=left, 3=right.\n",
        "        \"\"\"\n",
        "        mask = np.zeros(self.action_space.n, dtype=bool)\n",
        "\n",
        "        for a in range(self.action_space.n):\n",
        "            mask[a] = self._simulate_action_changes_board(a)\n",
        "\n",
        "        # If ALL actions appear invalid:\n",
        "        if not mask.any():\n",
        "            # This should only happen when there are truly no moves left\n",
        "            if self._moves_available():\n",
        "                # If this triggers, something is deeply inconsistent\n",
        "                raise RuntimeError(\"Mask is all False but moves_available() is True!\")\n",
        "            # Game is actually over: allow all actions to keep MaskablePPO happy.\n",
        "            mask[:] = True\n",
        "\n",
        "        return mask\n"
      ],
      "metadata": {
        "id": "-69xnyuJiW4v"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install sb3-contrib stable-baselines3 gymnasium numpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RvuxRMEaitNH",
        "outputId": "bba4696d-621a-43ae-c30f-db7ffe6aed6c"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sb3-contrib in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.12/dist-packages (2.7.0)\n",
            "Requirement already satisfied: gymnasium in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.9.0+cu126)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.1.2)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.12/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.20.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.5.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.3.3)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (4.60.1)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (1.4.9)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (25.0)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (11.3.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (3.2.5)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.12/dist-packages (from matplotlib->stable-baselines3) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas->stable-baselines3) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from sb3_contrib import MaskablePPO\n",
        "from sb3_contrib.common.wrappers import ActionMasker\n",
        "\n",
        "def mask_fn(env: TwentyFortyEightEnvMasked):\n",
        "    return env.get_action_mask()\n",
        "\n",
        "env = TwentyFortyEightEnvMasked()\n",
        "env = ActionMasker(env, mask_fn)\n",
        "\n",
        "policy_kwargs = dict(\n",
        "    net_arch=[dict(pi=[256, 256, 256], vf=[256, 256, 256])]\n",
        ")\n",
        "\n",
        "# model = MaskablePPO(\n",
        "#   \"MlpPolicy\",\n",
        "#   env,\n",
        "#   policy_kwargs=policy_kwargs,\n",
        "#   learning_rate=1e-4,\n",
        "#   n_steps=4096,\n",
        "#   batch_size=512,\n",
        "#   n_epochs=20,\n",
        "#   gamma=0.99,\n",
        "#   clip_range=0.1,\n",
        "#   ent_coef=0.1,\n",
        "#   target_kl=0.02,\n",
        "#   verbose=1,\n",
        "# )\n",
        "model = MaskablePPO(\n",
        "    \"MlpPolicy\",\n",
        "    env,\n",
        "    learning_rate = 5e-5,\n",
        "    n_steps       = 4096,\n",
        "    batch_size    = 256,\n",
        "    n_epochs      = 20,\n",
        "    gamma         = 0.995,\n",
        "    gae_lambda    = 0.95,\n",
        "    clip_range    = 0.1,\n",
        "    ent_coef      = 0.1,\n",
        "    vf_coef       = 0.5,\n",
        "    target_kl     = 0.01,\n",
        "    policy_kwargs = dict(\n",
        "        net_arch=[dict(pi=[256,256,256], vf=[256,256,256])]\n",
        "    ),\n",
        "    verbose=1,\n",
        ")\n",
        "\n",
        "model.learn(total_timesteps=1000000)\n",
        "model.save(\"maskable_ppo_2048_twentyfortyeightenv\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Xip1u8MihtU",
        "outputId": "49bdbe57-36c6-45c7-e45d-df0532499088"
      },
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "|    value_loss           | 2.72e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 330          |\n",
            "|    iterations           | 8            |\n",
            "|    time_elapsed         | 99           |\n",
            "|    total_timesteps      | 32768        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009521903 |\n",
            "|    clip_fraction        | 0.000635     |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.00017      |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.38e+04     |\n",
            "|    n_updates            | 140          |\n",
            "|    policy_gradient_loss | -0.00182     |\n",
            "|    value_loss           | 2.93e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 122           |\n",
            "|    ep_rew_mean          | 1.15e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 330           |\n",
            "|    iterations           | 9             |\n",
            "|    time_elapsed         | 111           |\n",
            "|    total_timesteps      | 36864         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00090784766 |\n",
            "|    clip_fraction        | 0.00033       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.24         |\n",
            "|    explained_variance   | 0.000102      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.44e+04      |\n",
            "|    n_updates            | 160           |\n",
            "|    policy_gradient_loss | -0.00164      |\n",
            "|    value_loss           | 2.82e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 329          |\n",
            "|    iterations           | 10           |\n",
            "|    time_elapsed         | 124          |\n",
            "|    total_timesteps      | 40960        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003020214 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 8.2e-05      |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.43e+04     |\n",
            "|    n_updates            | 180          |\n",
            "|    policy_gradient_loss | -0.00156     |\n",
            "|    value_loss           | 3.04e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 329          |\n",
            "|    iterations           | 11           |\n",
            "|    time_elapsed         | 136          |\n",
            "|    total_timesteps      | 45056        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009618946 |\n",
            "|    clip_fraction        | 0.00302      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 7.43e-05     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.21e+04     |\n",
            "|    n_updates            | 200          |\n",
            "|    policy_gradient_loss | -0.00223     |\n",
            "|    value_loss           | 2.38e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 111          |\n",
            "|    ep_rew_mean          | 994          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 329          |\n",
            "|    iterations           | 12           |\n",
            "|    time_elapsed         | 149          |\n",
            "|    total_timesteps      | 49152        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012694064 |\n",
            "|    clip_fraction        | 0.0142       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 5.02e-05     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.04e+04     |\n",
            "|    n_updates            | 220          |\n",
            "|    policy_gradient_loss | -0.00294     |\n",
            "|    value_loss           | 2.4e+04      |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 112           |\n",
            "|    ep_rew_mean          | 1.02e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 328           |\n",
            "|    iterations           | 13            |\n",
            "|    time_elapsed         | 161           |\n",
            "|    total_timesteps      | 53248         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00035154875 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.25         |\n",
            "|    explained_variance   | 3.91e-05      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.11e+04      |\n",
            "|    n_updates            | 240           |\n",
            "|    policy_gradient_loss | -0.00122      |\n",
            "|    value_loss           | 2.39e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 328          |\n",
            "|    iterations           | 14           |\n",
            "|    time_elapsed         | 174          |\n",
            "|    total_timesteps      | 57344        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0003045468 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 3.24e-05     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.42e+04     |\n",
            "|    n_updates            | 260          |\n",
            "|    policy_gradient_loss | -0.00147     |\n",
            "|    value_loss           | 2.86e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 328          |\n",
            "|    iterations           | 15           |\n",
            "|    time_elapsed         | 187          |\n",
            "|    total_timesteps      | 61440        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007561712 |\n",
            "|    clip_fraction        | 0.000232     |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 2.99e-05     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.2e+04      |\n",
            "|    n_updates            | 280          |\n",
            "|    policy_gradient_loss | -0.0017      |\n",
            "|    value_loss           | 2.73e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 119         |\n",
            "|    ep_rew_mean          | 1.11e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 328         |\n",
            "|    iterations           | 16          |\n",
            "|    time_elapsed         | 199         |\n",
            "|    total_timesteps      | 65536       |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000920724 |\n",
            "|    clip_fraction        | 0.00527     |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.24       |\n",
            "|    explained_variance   | 2.08e-05    |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.12e+04    |\n",
            "|    n_updates            | 300         |\n",
            "|    policy_gradient_loss | -0.0022     |\n",
            "|    value_loss           | 2.35e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 328          |\n",
            "|    iterations           | 17           |\n",
            "|    time_elapsed         | 212          |\n",
            "|    total_timesteps      | 69632        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005285592 |\n",
            "|    clip_fraction        | 0.000134     |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 1.67e-05     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.33e+04     |\n",
            "|    n_updates            | 320          |\n",
            "|    policy_gradient_loss | -0.00213     |\n",
            "|    value_loss           | 2.76e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 120           |\n",
            "|    ep_rew_mean          | 1.12e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 327           |\n",
            "|    iterations           | 18            |\n",
            "|    time_elapsed         | 224           |\n",
            "|    total_timesteps      | 73728         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00056621095 |\n",
            "|    clip_fraction        | 1.22e-05      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.24         |\n",
            "|    explained_variance   | 1.6e-05       |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.07e+04      |\n",
            "|    n_updates            | 340           |\n",
            "|    policy_gradient_loss | -0.00156      |\n",
            "|    value_loss           | 2.38e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 327          |\n",
            "|    iterations           | 19           |\n",
            "|    time_elapsed         | 237          |\n",
            "|    total_timesteps      | 77824        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004931472 |\n",
            "|    clip_fraction        | 1.22e-05     |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 1.47e-05     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.33e+04     |\n",
            "|    n_updates            | 360          |\n",
            "|    policy_gradient_loss | -0.00139     |\n",
            "|    value_loss           | 2.88e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 120           |\n",
            "|    ep_rew_mean          | 1.12e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 327           |\n",
            "|    iterations           | 20            |\n",
            "|    time_elapsed         | 250           |\n",
            "|    total_timesteps      | 81920         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00075411896 |\n",
            "|    clip_fraction        | 0.00629       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 8.58e-06      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.34e+04      |\n",
            "|    n_updates            | 380           |\n",
            "|    policy_gradient_loss | -0.0021       |\n",
            "|    value_loss           | 2.53e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 120           |\n",
            "|    ep_rew_mean          | 1.11e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 327           |\n",
            "|    iterations           | 21            |\n",
            "|    time_elapsed         | 262           |\n",
            "|    total_timesteps      | 86016         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00084466086 |\n",
            "|    clip_fraction        | 0.0017        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 8.05e-06      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.13e+04      |\n",
            "|    n_updates            | 400           |\n",
            "|    policy_gradient_loss | -0.00201      |\n",
            "|    value_loss           | 2.46e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 116           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 327           |\n",
            "|    iterations           | 22            |\n",
            "|    time_elapsed         | 275           |\n",
            "|    total_timesteps      | 90112         |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00056944886 |\n",
            "|    clip_fraction        | 3.66e-05      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 7.03e-06      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.24e+04      |\n",
            "|    n_updates            | 420           |\n",
            "|    policy_gradient_loss | -0.00162      |\n",
            "|    value_loss           | 2.67e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 23           |\n",
            "|    time_elapsed         | 288          |\n",
            "|    total_timesteps      | 94208        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013413623 |\n",
            "|    clip_fraction        | 0.00693      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 5.66e-06     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.14e+04     |\n",
            "|    n_updates            | 440          |\n",
            "|    policy_gradient_loss | -0.00269     |\n",
            "|    value_loss           | 2.33e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 24           |\n",
            "|    time_elapsed         | 300          |\n",
            "|    total_timesteps      | 98304        |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013108775 |\n",
            "|    clip_fraction        | 0.0208       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 5.01e-06     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.18e+04     |\n",
            "|    n_updates            | 460          |\n",
            "|    policy_gradient_loss | -0.00324     |\n",
            "|    value_loss           | 2.29e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 25           |\n",
            "|    time_elapsed         | 313          |\n",
            "|    total_timesteps      | 102400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007621784 |\n",
            "|    clip_fraction        | 0.000183     |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 4.11e-06     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.45e+04     |\n",
            "|    n_updates            | 480          |\n",
            "|    policy_gradient_loss | -0.00172     |\n",
            "|    value_loss           | 2.8e+04      |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 125           |\n",
            "|    ep_rew_mean          | 1.2e+03       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 26            |\n",
            "|    time_elapsed         | 326           |\n",
            "|    total_timesteps      | 106496        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00053668243 |\n",
            "|    clip_fraction        | 0.00011       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 4.29e-06      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.17e+04      |\n",
            "|    n_updates            | 500           |\n",
            "|    policy_gradient_loss | -0.0016       |\n",
            "|    value_loss           | 2.5e+04       |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 1.18e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 27           |\n",
            "|    time_elapsed         | 338          |\n",
            "|    total_timesteps      | 110592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011925148 |\n",
            "|    clip_fraction        | 0.0146       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 3.64e-06     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.49e+04     |\n",
            "|    n_updates            | 520          |\n",
            "|    policy_gradient_loss | -0.00314     |\n",
            "|    value_loss           | 2.92e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 123           |\n",
            "|    ep_rew_mean          | 1.17e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 28            |\n",
            "|    time_elapsed         | 351           |\n",
            "|    total_timesteps      | 114688        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00067052623 |\n",
            "|    clip_fraction        | 0.00129       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 2.92e-06      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.23e+04      |\n",
            "|    n_updates            | 540           |\n",
            "|    policy_gradient_loss | -0.00208      |\n",
            "|    value_loss           | 2.54e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.14e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 29           |\n",
            "|    time_elapsed         | 363          |\n",
            "|    total_timesteps      | 118784       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005006421 |\n",
            "|    clip_fraction        | 1.22e-05     |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 2.8e-06      |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.1e+04      |\n",
            "|    n_updates            | 560          |\n",
            "|    policy_gradient_loss | -0.0018      |\n",
            "|    value_loss           | 2.46e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 1.06e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 30           |\n",
            "|    time_elapsed         | 376          |\n",
            "|    total_timesteps      | 122880       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009581669 |\n",
            "|    clip_fraction        | 0.00135      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 2.62e-06     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.24e+04     |\n",
            "|    n_updates            | 580          |\n",
            "|    policy_gradient_loss | -0.00211     |\n",
            "|    value_loss           | 2.51e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 31           |\n",
            "|    time_elapsed         | 388          |\n",
            "|    total_timesteps      | 126976       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010361858 |\n",
            "|    clip_fraction        | 0.00874      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 2.21e-06     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 8.96e+03     |\n",
            "|    n_updates            | 600          |\n",
            "|    policy_gradient_loss | -0.00225     |\n",
            "|    value_loss           | 2.03e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 32           |\n",
            "|    time_elapsed         | 401          |\n",
            "|    total_timesteps      | 131072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0004948107 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 1.73e-06     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.2e+04      |\n",
            "|    n_updates            | 620          |\n",
            "|    policy_gradient_loss | -0.00189     |\n",
            "|    value_loss           | 2.5e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 33           |\n",
            "|    time_elapsed         | 413          |\n",
            "|    total_timesteps      | 135168       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008785551 |\n",
            "|    clip_fraction        | 0.00508      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 1.85e-06     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.37e+04     |\n",
            "|    n_updates            | 640          |\n",
            "|    policy_gradient_loss | -0.00259     |\n",
            "|    value_loss           | 2.72e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 34           |\n",
            "|    time_elapsed         | 426          |\n",
            "|    total_timesteps      | 139264       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012850959 |\n",
            "|    clip_fraction        | 0.0113       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 1.73e-06     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.01e+04     |\n",
            "|    n_updates            | 660          |\n",
            "|    policy_gradient_loss | -0.00303     |\n",
            "|    value_loss           | 2.11e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 35           |\n",
            "|    time_elapsed         | 439          |\n",
            "|    total_timesteps      | 143360       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011710231 |\n",
            "|    clip_fraction        | 0.0109       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 1.01e-06     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.01e+04     |\n",
            "|    n_updates            | 680          |\n",
            "|    policy_gradient_loss | -0.00335     |\n",
            "|    value_loss           | 2.23e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 119           |\n",
            "|    ep_rew_mean          | 1.1e+03       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 36            |\n",
            "|    time_elapsed         | 451           |\n",
            "|    total_timesteps      | 147456        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00090709515 |\n",
            "|    clip_fraction        | 0.00547       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 1.01e-06      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.08e+04      |\n",
            "|    n_updates            | 700           |\n",
            "|    policy_gradient_loss | -0.00296      |\n",
            "|    value_loss           | 2.21e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 37           |\n",
            "|    time_elapsed         | 464          |\n",
            "|    total_timesteps      | 151552       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012423557 |\n",
            "|    clip_fraction        | 0.00349      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 1.13e-06     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.22e+04     |\n",
            "|    n_updates            | 720          |\n",
            "|    policy_gradient_loss | -0.00243     |\n",
            "|    value_loss           | 2.47e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 124           |\n",
            "|    ep_rew_mean          | 1.16e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 38            |\n",
            "|    time_elapsed         | 476           |\n",
            "|    total_timesteps      | 155648        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00078633113 |\n",
            "|    clip_fraction        | 0.000842      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 1.01e-06      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.1e+04       |\n",
            "|    n_updates            | 740           |\n",
            "|    policy_gradient_loss | -0.00187      |\n",
            "|    value_loss           | 2.3e+04       |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 39           |\n",
            "|    time_elapsed         | 489          |\n",
            "|    total_timesteps      | 159744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011181756 |\n",
            "|    clip_fraction        | 0.00593      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 9.54e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.13e+04     |\n",
            "|    n_updates            | 760          |\n",
            "|    policy_gradient_loss | -0.00259     |\n",
            "|    value_loss           | 2.47e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 119         |\n",
            "|    ep_rew_mean          | 1.1e+03     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 326         |\n",
            "|    iterations           | 40          |\n",
            "|    time_elapsed         | 501         |\n",
            "|    total_timesteps      | 163840      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000850216 |\n",
            "|    clip_fraction        | 0.00106     |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 7.15e-07    |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.04e+04    |\n",
            "|    n_updates            | 780         |\n",
            "|    policy_gradient_loss | -0.00197    |\n",
            "|    value_loss           | 2.28e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 1.06e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 41           |\n",
            "|    time_elapsed         | 514          |\n",
            "|    total_timesteps      | 167936       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009764708 |\n",
            "|    clip_fraction        | 0.00094      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 7.75e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.1e+04      |\n",
            "|    n_updates            | 800          |\n",
            "|    policy_gradient_loss | -0.00196     |\n",
            "|    value_loss           | 2.15e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 115           |\n",
            "|    ep_rew_mean          | 1.05e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 42            |\n",
            "|    time_elapsed         | 526           |\n",
            "|    total_timesteps      | 172032        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00086895097 |\n",
            "|    clip_fraction        | 0.000427      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 5.96e-07      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.12e+04      |\n",
            "|    n_updates            | 820           |\n",
            "|    policy_gradient_loss | -0.00256      |\n",
            "|    value_loss           | 2.43e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 43           |\n",
            "|    time_elapsed         | 539          |\n",
            "|    total_timesteps      | 176128       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011891336 |\n",
            "|    clip_fraction        | 0.0066       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 5.36e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.09e+04     |\n",
            "|    n_updates            | 840          |\n",
            "|    policy_gradient_loss | -0.00317     |\n",
            "|    value_loss           | 2.22e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 122           |\n",
            "|    ep_rew_mean          | 1.14e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 44            |\n",
            "|    time_elapsed         | 552           |\n",
            "|    total_timesteps      | 180224        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00070107833 |\n",
            "|    clip_fraction        | 0.000696      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 5.96e-07      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.1e+04       |\n",
            "|    n_updates            | 860           |\n",
            "|    policy_gradient_loss | -0.002        |\n",
            "|    value_loss           | 2.17e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.13e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 45           |\n",
            "|    time_elapsed         | 564          |\n",
            "|    total_timesteps      | 184320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015583008 |\n",
            "|    clip_fraction        | 0.015        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 7.75e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.32e+04     |\n",
            "|    n_updates            | 880          |\n",
            "|    policy_gradient_loss | -0.0031      |\n",
            "|    value_loss           | 2.72e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 124           |\n",
            "|    ep_rew_mean          | 1.17e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 46            |\n",
            "|    time_elapsed         | 577           |\n",
            "|    total_timesteps      | 188416        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00084942137 |\n",
            "|    clip_fraction        | 0.00116       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 3.58e-07      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 9.9e+03       |\n",
            "|    n_updates            | 900           |\n",
            "|    policy_gradient_loss | -0.00192      |\n",
            "|    value_loss           | 2.15e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 120           |\n",
            "|    ep_rew_mean          | 1.11e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 47            |\n",
            "|    time_elapsed         | 589           |\n",
            "|    total_timesteps      | 192512        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00058511656 |\n",
            "|    clip_fraction        | 8.54e-05      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 6.56e-07      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.25e+04      |\n",
            "|    n_updates            | 920           |\n",
            "|    policy_gradient_loss | -0.00183      |\n",
            "|    value_loss           | 2.57e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 48           |\n",
            "|    time_elapsed         | 602          |\n",
            "|    total_timesteps      | 196608       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010930309 |\n",
            "|    clip_fraction        | 0.00389      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 4.77e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.33e+04     |\n",
            "|    n_updates            | 940          |\n",
            "|    policy_gradient_loss | -0.00279     |\n",
            "|    value_loss           | 2.31e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 49           |\n",
            "|    time_elapsed         | 615          |\n",
            "|    total_timesteps      | 200704       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015449759 |\n",
            "|    clip_fraction        | 0.0118       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 2.98e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.07e+04     |\n",
            "|    n_updates            | 960          |\n",
            "|    policy_gradient_loss | -0.00294     |\n",
            "|    value_loss           | 2.27e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 1.06e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 50           |\n",
            "|    time_elapsed         | 627          |\n",
            "|    total_timesteps      | 204800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018013498 |\n",
            "|    clip_fraction        | 0.0161       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 3.58e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.04e+04     |\n",
            "|    n_updates            | 980          |\n",
            "|    policy_gradient_loss | -0.00368     |\n",
            "|    value_loss           | 2.18e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 112          |\n",
            "|    ep_rew_mean          | 1e+03        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 51           |\n",
            "|    time_elapsed         | 640          |\n",
            "|    total_timesteps      | 208896       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007587269 |\n",
            "|    clip_fraction        | 0.000171     |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 3.58e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.03e+04     |\n",
            "|    n_updates            | 1000         |\n",
            "|    policy_gradient_loss | -0.00213     |\n",
            "|    value_loss           | 2.06e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 115         |\n",
            "|    ep_rew_mean          | 1.05e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 326         |\n",
            "|    iterations           | 52          |\n",
            "|    time_elapsed         | 652         |\n",
            "|    total_timesteps      | 212992      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001049285 |\n",
            "|    clip_fraction        | 0.00703     |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 1.79e-07    |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 8.05e+03    |\n",
            "|    n_updates            | 1020        |\n",
            "|    policy_gradient_loss | -0.00246    |\n",
            "|    value_loss           | 1.76e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 118           |\n",
            "|    ep_rew_mean          | 1.09e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 53            |\n",
            "|    time_elapsed         | 665           |\n",
            "|    total_timesteps      | 217088        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00021037574 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.21         |\n",
            "|    explained_variance   | 3.58e-07      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.19e+04      |\n",
            "|    n_updates            | 1040          |\n",
            "|    policy_gradient_loss | -0.00172      |\n",
            "|    value_loss           | 2.71e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 125           |\n",
            "|    ep_rew_mean          | 1.19e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 54            |\n",
            "|    time_elapsed         | 677           |\n",
            "|    total_timesteps      | 221184        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00089861016 |\n",
            "|    clip_fraction        | 0.00773       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 2.98e-07      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.11e+04      |\n",
            "|    n_updates            | 1060          |\n",
            "|    policy_gradient_loss | -0.00252      |\n",
            "|    value_loss           | 2.26e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.15e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 55           |\n",
            "|    time_elapsed         | 690          |\n",
            "|    total_timesteps      | 225280       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011981155 |\n",
            "|    clip_fraction        | 0.00963      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.56e+04     |\n",
            "|    n_updates            | 1080         |\n",
            "|    policy_gradient_loss | -0.003       |\n",
            "|    value_loss           | 2.74e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 122         |\n",
            "|    ep_rew_mean          | 1.14e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 326         |\n",
            "|    iterations           | 56          |\n",
            "|    time_elapsed         | 703         |\n",
            "|    total_timesteps      | 229376      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001372455 |\n",
            "|    clip_fraction        | 0.0184      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 2.98e-07    |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.07e+04    |\n",
            "|    n_updates            | 1100        |\n",
            "|    policy_gradient_loss | -0.00342    |\n",
            "|    value_loss           | 2.44e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 57           |\n",
            "|    time_elapsed         | 715          |\n",
            "|    total_timesteps      | 233472       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016189506 |\n",
            "|    clip_fraction        | 0.0135       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 2.98e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.1e+04      |\n",
            "|    n_updates            | 1120         |\n",
            "|    policy_gradient_loss | -0.00353     |\n",
            "|    value_loss           | 2.24e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 58           |\n",
            "|    time_elapsed         | 728          |\n",
            "|    total_timesteps      | 237568       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007665198 |\n",
            "|    clip_fraction        | 0.00134      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 1.79e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 9.75e+03     |\n",
            "|    n_updates            | 1140         |\n",
            "|    policy_gradient_loss | -0.00217     |\n",
            "|    value_loss           | 2.1e+04      |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 115           |\n",
            "|    ep_rew_mean          | 1.05e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 59            |\n",
            "|    time_elapsed         | 740           |\n",
            "|    total_timesteps      | 241664        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00093850587 |\n",
            "|    clip_fraction        | 0.00457       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 2.38e-07      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.01e+04      |\n",
            "|    n_updates            | 1160          |\n",
            "|    policy_gradient_loss | -0.0023       |\n",
            "|    value_loss           | 2.17e+04      |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 115         |\n",
            "|    ep_rew_mean          | 1.06e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 326         |\n",
            "|    iterations           | 60          |\n",
            "|    time_elapsed         | 753         |\n",
            "|    total_timesteps      | 245760      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001131892 |\n",
            "|    clip_fraction        | 0.0067      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.1e+04     |\n",
            "|    n_updates            | 1180        |\n",
            "|    policy_gradient_loss | -0.00291    |\n",
            "|    value_loss           | 2.25e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 1.13e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 61           |\n",
            "|    time_elapsed         | 766          |\n",
            "|    total_timesteps      | 249856       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012927909 |\n",
            "|    clip_fraction        | 0.00922      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.13e+04     |\n",
            "|    n_updates            | 1200         |\n",
            "|    policy_gradient_loss | -0.00329     |\n",
            "|    value_loss           | 2.21e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 1.19e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 62           |\n",
            "|    time_elapsed         | 778          |\n",
            "|    total_timesteps      | 253952       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010123298 |\n",
            "|    clip_fraction        | 0.00967      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.35e+04     |\n",
            "|    n_updates            | 1220         |\n",
            "|    policy_gradient_loss | -0.00324     |\n",
            "|    value_loss           | 2.54e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 128          |\n",
            "|    ep_rew_mean          | 1.22e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 63           |\n",
            "|    time_elapsed         | 791          |\n",
            "|    total_timesteps      | 258048       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008033449 |\n",
            "|    clip_fraction        | 0.00144      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 2.98e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.2e+04      |\n",
            "|    n_updates            | 1240         |\n",
            "|    policy_gradient_loss | -0.00201     |\n",
            "|    value_loss           | 2.3e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 1.2e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 64           |\n",
            "|    time_elapsed         | 803          |\n",
            "|    total_timesteps      | 262144       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011204828 |\n",
            "|    clip_fraction        | 0.00293      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.19e+04     |\n",
            "|    n_updates            | 1260         |\n",
            "|    policy_gradient_loss | -0.00234     |\n",
            "|    value_loss           | 2.43e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 1.19e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 65           |\n",
            "|    time_elapsed         | 816          |\n",
            "|    total_timesteps      | 266240       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010395953 |\n",
            "|    clip_fraction        | 0.0072       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.25e+04     |\n",
            "|    n_updates            | 1280         |\n",
            "|    policy_gradient_loss | -0.00318     |\n",
            "|    value_loss           | 2.47e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 66           |\n",
            "|    time_elapsed         | 828          |\n",
            "|    total_timesteps      | 270336       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009402396 |\n",
            "|    clip_fraction        | 0.0063       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.2e+04      |\n",
            "|    n_updates            | 1300         |\n",
            "|    policy_gradient_loss | -0.00313     |\n",
            "|    value_loss           | 2.27e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 115           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 67            |\n",
            "|    time_elapsed         | 841           |\n",
            "|    total_timesteps      | 274432        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00091112306 |\n",
            "|    clip_fraction        | 0.000891      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.08e+04      |\n",
            "|    n_updates            | 1320          |\n",
            "|    policy_gradient_loss | -0.00257      |\n",
            "|    value_loss           | 2.16e+04      |\n",
            "-------------------------------------------\n",
            "----------------------------------------\n",
            "| rollout/                |            |\n",
            "|    ep_len_mean          | 113        |\n",
            "|    ep_rew_mean          | 1.02e+03   |\n",
            "| time/                   |            |\n",
            "|    fps                  | 326        |\n",
            "|    iterations           | 68         |\n",
            "|    time_elapsed         | 854        |\n",
            "|    total_timesteps      | 278528     |\n",
            "| train/                  |            |\n",
            "|    approx_kl            | 0.00111782 |\n",
            "|    clip_fraction        | 0.0122     |\n",
            "|    clip_range           | 0.1        |\n",
            "|    entropy_loss         | -1.23      |\n",
            "|    explained_variance   | 5.96e-08   |\n",
            "|    learning_rate        | 5e-05      |\n",
            "|    loss                 | 1.18e+04   |\n",
            "|    n_updates            | 1340       |\n",
            "|    policy_gradient_loss | -0.00343   |\n",
            "|    value_loss           | 2.11e+04   |\n",
            "----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 1.05e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 69           |\n",
            "|    time_elapsed         | 866          |\n",
            "|    total_timesteps      | 282624       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013850695 |\n",
            "|    clip_fraction        | 0.0194       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.04e+04     |\n",
            "|    n_updates            | 1360         |\n",
            "|    policy_gradient_loss | -0.00377     |\n",
            "|    value_loss           | 2.09e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 1.06e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 70           |\n",
            "|    time_elapsed         | 879          |\n",
            "|    total_timesteps      | 286720       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009934094 |\n",
            "|    clip_fraction        | 0.00679      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 2.38e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.36e+04     |\n",
            "|    n_updates            | 1380         |\n",
            "|    policy_gradient_loss | -0.00224     |\n",
            "|    value_loss           | 2.56e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 71           |\n",
            "|    time_elapsed         | 891          |\n",
            "|    total_timesteps      | 290816       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010934238 |\n",
            "|    clip_fraction        | 0.0158       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 9.49e+03     |\n",
            "|    n_updates            | 1400         |\n",
            "|    policy_gradient_loss | -0.00369     |\n",
            "|    value_loss           | 1.94e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 72           |\n",
            "|    time_elapsed         | 904          |\n",
            "|    total_timesteps      | 294912       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009020639 |\n",
            "|    clip_fraction        | 0.00352      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.24e+04     |\n",
            "|    n_updates            | 1420         |\n",
            "|    policy_gradient_loss | -0.00319     |\n",
            "|    value_loss           | 2.46e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 73           |\n",
            "|    time_elapsed         | 916          |\n",
            "|    total_timesteps      | 299008       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017741453 |\n",
            "|    clip_fraction        | 0.022        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.06e+04     |\n",
            "|    n_updates            | 1440         |\n",
            "|    policy_gradient_loss | -0.00373     |\n",
            "|    value_loss           | 2.14e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 74           |\n",
            "|    time_elapsed         | 929          |\n",
            "|    total_timesteps      | 303104       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013008544 |\n",
            "|    clip_fraction        | 0.0122       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.04e+04     |\n",
            "|    n_updates            | 1460         |\n",
            "|    policy_gradient_loss | -0.00332     |\n",
            "|    value_loss           | 2.33e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 1.17e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 75           |\n",
            "|    time_elapsed         | 941          |\n",
            "|    total_timesteps      | 307200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011926631 |\n",
            "|    clip_fraction        | 0.0161       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.1e+04      |\n",
            "|    n_updates            | 1480         |\n",
            "|    policy_gradient_loss | -0.0038      |\n",
            "|    value_loss           | 2.47e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 1.17e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 76           |\n",
            "|    time_elapsed         | 954          |\n",
            "|    total_timesteps      | 311296       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011896682 |\n",
            "|    clip_fraction        | 0.0128       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.26e+04     |\n",
            "|    n_updates            | 1500         |\n",
            "|    policy_gradient_loss | -0.00312     |\n",
            "|    value_loss           | 2.5e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 77           |\n",
            "|    time_elapsed         | 967          |\n",
            "|    total_timesteps      | 315392       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011989295 |\n",
            "|    clip_fraction        | 0.00505      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.14e+04     |\n",
            "|    n_updates            | 1520         |\n",
            "|    policy_gradient_loss | -0.00294     |\n",
            "|    value_loss           | 2.29e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 111           |\n",
            "|    ep_rew_mean          | 996           |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 78            |\n",
            "|    time_elapsed         | 979           |\n",
            "|    total_timesteps      | 319488        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00083192793 |\n",
            "|    clip_fraction        | 0.00242       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.02e+04      |\n",
            "|    n_updates            | 1540          |\n",
            "|    policy_gradient_loss | -0.00216      |\n",
            "|    value_loss           | 2.14e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 111          |\n",
            "|    ep_rew_mean          | 999          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 79           |\n",
            "|    time_elapsed         | 992          |\n",
            "|    total_timesteps      | 323584       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010739947 |\n",
            "|    clip_fraction        | 0.0156       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.12e+04     |\n",
            "|    n_updates            | 1560         |\n",
            "|    policy_gradient_loss | -0.00309     |\n",
            "|    value_loss           | 2.26e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 80           |\n",
            "|    time_elapsed         | 1004         |\n",
            "|    total_timesteps      | 327680       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009777892 |\n",
            "|    clip_fraction        | 0.00273      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.23e+04     |\n",
            "|    n_updates            | 1580         |\n",
            "|    policy_gradient_loss | -0.00303     |\n",
            "|    value_loss           | 2.42e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 1.13e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 81           |\n",
            "|    time_elapsed         | 1017         |\n",
            "|    total_timesteps      | 331776       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011512525 |\n",
            "|    clip_fraction        | 0.0131       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.12e+04     |\n",
            "|    n_updates            | 1600         |\n",
            "|    policy_gradient_loss | -0.00327     |\n",
            "|    value_loss           | 2.47e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 1.13e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 82           |\n",
            "|    time_elapsed         | 1029         |\n",
            "|    total_timesteps      | 335872       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012413873 |\n",
            "|    clip_fraction        | 0.01         |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.01e+04     |\n",
            "|    n_updates            | 1620         |\n",
            "|    policy_gradient_loss | -0.00392     |\n",
            "|    value_loss           | 2.38e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 83           |\n",
            "|    time_elapsed         | 1042         |\n",
            "|    total_timesteps      | 339968       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011698718 |\n",
            "|    clip_fraction        | 0.00918      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.2e+04      |\n",
            "|    n_updates            | 1640         |\n",
            "|    policy_gradient_loss | -0.00314     |\n",
            "|    value_loss           | 2.43e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 84           |\n",
            "|    time_elapsed         | 1054         |\n",
            "|    total_timesteps      | 344064       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013436443 |\n",
            "|    clip_fraction        | 0.02         |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.25e+04     |\n",
            "|    n_updates            | 1660         |\n",
            "|    policy_gradient_loss | -0.00389     |\n",
            "|    value_loss           | 2.36e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 85           |\n",
            "|    time_elapsed         | 1067         |\n",
            "|    total_timesteps      | 348160       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012932768 |\n",
            "|    clip_fraction        | 0.00975      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.28e+04     |\n",
            "|    n_updates            | 1680         |\n",
            "|    policy_gradient_loss | -0.00334     |\n",
            "|    value_loss           | 2.28e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 117           |\n",
            "|    ep_rew_mean          | 1.08e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 86            |\n",
            "|    time_elapsed         | 1080          |\n",
            "|    total_timesteps      | 352256        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00078109466 |\n",
            "|    clip_fraction        | 0.000854      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.22e+04      |\n",
            "|    n_updates            | 1700          |\n",
            "|    policy_gradient_loss | -0.00248      |\n",
            "|    value_loss           | 2.44e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 1.13e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 87           |\n",
            "|    time_elapsed         | 1092         |\n",
            "|    total_timesteps      | 356352       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012659433 |\n",
            "|    clip_fraction        | 0.0099       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.31e+04     |\n",
            "|    n_updates            | 1720         |\n",
            "|    policy_gradient_loss | -0.00351     |\n",
            "|    value_loss           | 2.4e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 1.17e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 88           |\n",
            "|    time_elapsed         | 1105         |\n",
            "|    total_timesteps      | 360448       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0009799027 |\n",
            "|    clip_fraction        | 0.00541      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 9.79e+03     |\n",
            "|    n_updates            | 1740         |\n",
            "|    policy_gradient_loss | -0.00296     |\n",
            "|    value_loss           | 2.53e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 1.21e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 89           |\n",
            "|    time_elapsed         | 1118         |\n",
            "|    total_timesteps      | 364544       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014345652 |\n",
            "|    clip_fraction        | 0.0274       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.25e+04     |\n",
            "|    n_updates            | 1760         |\n",
            "|    policy_gradient_loss | -0.00402     |\n",
            "|    value_loss           | 2.67e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 1.2e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 90           |\n",
            "|    time_elapsed         | 1130         |\n",
            "|    total_timesteps      | 368640       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013832798 |\n",
            "|    clip_fraction        | 0.0126       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.36e+04     |\n",
            "|    n_updates            | 1780         |\n",
            "|    policy_gradient_loss | -0.00373     |\n",
            "|    value_loss           | 2.47e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 91           |\n",
            "|    time_elapsed         | 1143         |\n",
            "|    total_timesteps      | 372736       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013090682 |\n",
            "|    clip_fraction        | 0.0108       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.01e+04     |\n",
            "|    n_updates            | 1800         |\n",
            "|    policy_gradient_loss | -0.00334     |\n",
            "|    value_loss           | 2.41e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 119           |\n",
            "|    ep_rew_mean          | 1.1e+03       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 326           |\n",
            "|    iterations           | 92            |\n",
            "|    time_elapsed         | 1155          |\n",
            "|    total_timesteps      | 376832        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00081394566 |\n",
            "|    clip_fraction        | 0.00684       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 1.19e-07      |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.09e+04      |\n",
            "|    n_updates            | 1820          |\n",
            "|    policy_gradient_loss | -0.00328      |\n",
            "|    value_loss           | 2.42e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 93           |\n",
            "|    time_elapsed         | 1168         |\n",
            "|    total_timesteps      | 380928       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015146923 |\n",
            "|    clip_fraction        | 0.0157       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.13e+04     |\n",
            "|    n_updates            | 1840         |\n",
            "|    policy_gradient_loss | -0.0039      |\n",
            "|    value_loss           | 2.29e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 94           |\n",
            "|    time_elapsed         | 1180         |\n",
            "|    total_timesteps      | 385024       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013221165 |\n",
            "|    clip_fraction        | 0.0152       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.37e+04     |\n",
            "|    n_updates            | 1860         |\n",
            "|    policy_gradient_loss | -0.00364     |\n",
            "|    value_loss           | 2.6e+04      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 120         |\n",
            "|    ep_rew_mean          | 1.12e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 326         |\n",
            "|    iterations           | 95          |\n",
            "|    time_elapsed         | 1193        |\n",
            "|    total_timesteps      | 389120      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001061413 |\n",
            "|    clip_fraction        | 0.00887     |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.35e+04    |\n",
            "|    n_updates            | 1880        |\n",
            "|    policy_gradient_loss | -0.00369    |\n",
            "|    value_loss           | 2.46e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 96           |\n",
            "|    time_elapsed         | 1206         |\n",
            "|    total_timesteps      | 393216       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010652901 |\n",
            "|    clip_fraction        | 0.00978      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.19e+04     |\n",
            "|    n_updates            | 1900         |\n",
            "|    policy_gradient_loss | -0.00382     |\n",
            "|    value_loss           | 2.4e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 97           |\n",
            "|    time_elapsed         | 1218         |\n",
            "|    total_timesteps      | 397312       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011560158 |\n",
            "|    clip_fraction        | 0.00968      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.32e+04     |\n",
            "|    n_updates            | 1920         |\n",
            "|    policy_gradient_loss | -0.00319     |\n",
            "|    value_loss           | 2.52e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 98           |\n",
            "|    time_elapsed         | 1231         |\n",
            "|    total_timesteps      | 401408       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012566826 |\n",
            "|    clip_fraction        | 0.0166       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.26e+04     |\n",
            "|    n_updates            | 1940         |\n",
            "|    policy_gradient_loss | -0.00379     |\n",
            "|    value_loss           | 2.43e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 1.16e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 99           |\n",
            "|    time_elapsed         | 1243         |\n",
            "|    total_timesteps      | 405504       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010619308 |\n",
            "|    clip_fraction        | 0.0164       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.53e+04     |\n",
            "|    n_updates            | 1960         |\n",
            "|    policy_gradient_loss | -0.00416     |\n",
            "|    value_loss           | 2.72e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 120         |\n",
            "|    ep_rew_mean          | 1.12e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 326         |\n",
            "|    iterations           | 100         |\n",
            "|    time_elapsed         | 1256        |\n",
            "|    total_timesteps      | 409600      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001286096 |\n",
            "|    clip_fraction        | 0.00715     |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.12e+04    |\n",
            "|    n_updates            | 1980        |\n",
            "|    policy_gradient_loss | -0.00451    |\n",
            "|    value_loss           | 2.57e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.14e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 101          |\n",
            "|    time_elapsed         | 1268         |\n",
            "|    total_timesteps      | 413696       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013185213 |\n",
            "|    clip_fraction        | 0.0116       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.21e+04     |\n",
            "|    n_updates            | 2000         |\n",
            "|    policy_gradient_loss | -0.00367     |\n",
            "|    value_loss           | 2.38e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 102          |\n",
            "|    time_elapsed         | 1281         |\n",
            "|    total_timesteps      | 417792       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012687566 |\n",
            "|    clip_fraction        | 0.00918      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.59e+04     |\n",
            "|    n_updates            | 2020         |\n",
            "|    policy_gradient_loss | -0.00424     |\n",
            "|    value_loss           | 2.85e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 103          |\n",
            "|    time_elapsed         | 1293         |\n",
            "|    total_timesteps      | 421888       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011663671 |\n",
            "|    clip_fraction        | 0.0163       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.12e+04     |\n",
            "|    n_updates            | 2040         |\n",
            "|    policy_gradient_loss | -0.00468     |\n",
            "|    value_loss           | 2.59e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 116         |\n",
            "|    ep_rew_mean          | 1.06e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 326         |\n",
            "|    iterations           | 104         |\n",
            "|    time_elapsed         | 1306        |\n",
            "|    total_timesteps      | 425984      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001609911 |\n",
            "|    clip_fraction        | 0.0193      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.44e+04    |\n",
            "|    n_updates            | 2060        |\n",
            "|    policy_gradient_loss | -0.00423    |\n",
            "|    value_loss           | 2.69e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 105          |\n",
            "|    time_elapsed         | 1319         |\n",
            "|    total_timesteps      | 430080       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013245642 |\n",
            "|    clip_fraction        | 0.024        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.03e+04     |\n",
            "|    n_updates            | 2080         |\n",
            "|    policy_gradient_loss | -0.0058      |\n",
            "|    value_loss           | 2.39e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 106          |\n",
            "|    time_elapsed         | 1331         |\n",
            "|    total_timesteps      | 434176       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012486768 |\n",
            "|    clip_fraction        | 0.018        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.33e+04     |\n",
            "|    n_updates            | 2100         |\n",
            "|    policy_gradient_loss | -0.00454     |\n",
            "|    value_loss           | 2.61e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.14e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 326          |\n",
            "|    iterations           | 107          |\n",
            "|    time_elapsed         | 1344         |\n",
            "|    total_timesteps      | 438272       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013375607 |\n",
            "|    clip_fraction        | 0.0158       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.31e+04     |\n",
            "|    n_updates            | 2120         |\n",
            "|    policy_gradient_loss | -0.00393     |\n",
            "|    value_loss           | 2.67e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 1.14e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 108          |\n",
            "|    time_elapsed         | 1357         |\n",
            "|    total_timesteps      | 442368       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012490414 |\n",
            "|    clip_fraction        | 0.0129       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.4e+04      |\n",
            "|    n_updates            | 2140         |\n",
            "|    policy_gradient_loss | -0.00407     |\n",
            "|    value_loss           | 2.76e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 109          |\n",
            "|    time_elapsed         | 1369         |\n",
            "|    total_timesteps      | 446464       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014386503 |\n",
            "|    clip_fraction        | 0.0206       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.56e+04     |\n",
            "|    n_updates            | 2160         |\n",
            "|    policy_gradient_loss | -0.0045      |\n",
            "|    value_loss           | 2.7e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.15e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 110          |\n",
            "|    time_elapsed         | 1382         |\n",
            "|    total_timesteps      | 450560       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0013760251 |\n",
            "|    clip_fraction        | 0.0218       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.49e+04     |\n",
            "|    n_updates            | 2180         |\n",
            "|    policy_gradient_loss | -0.0052      |\n",
            "|    value_loss           | 2.65e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.14e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 111          |\n",
            "|    time_elapsed         | 1394         |\n",
            "|    total_timesteps      | 454656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015461599 |\n",
            "|    clip_fraction        | 0.0196       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.24e+04     |\n",
            "|    n_updates            | 2200         |\n",
            "|    policy_gradient_loss | -0.00461     |\n",
            "|    value_loss           | 2.85e+04     |\n",
            "------------------------------------------\n",
            "---------------------------------------\n",
            "| rollout/                |           |\n",
            "|    ep_len_mean          | 124       |\n",
            "|    ep_rew_mean          | 1.16e+03  |\n",
            "| time/                   |           |\n",
            "|    fps                  | 325       |\n",
            "|    iterations           | 112       |\n",
            "|    time_elapsed         | 1407      |\n",
            "|    total_timesteps      | 458752    |\n",
            "| train/                  |           |\n",
            "|    approx_kl            | 0.0013581 |\n",
            "|    clip_fraction        | 0.0111    |\n",
            "|    clip_range           | 0.1       |\n",
            "|    entropy_loss         | -1.23     |\n",
            "|    explained_variance   | 5.96e-08  |\n",
            "|    learning_rate        | 5e-05     |\n",
            "|    loss                 | 1.28e+04  |\n",
            "|    n_updates            | 2220      |\n",
            "|    policy_gradient_loss | -0.00403  |\n",
            "|    value_loss           | 2.66e+04  |\n",
            "---------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.15e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 113          |\n",
            "|    time_elapsed         | 1420         |\n",
            "|    total_timesteps      | 462848       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011855352 |\n",
            "|    clip_fraction        | 0.00737      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.46e+04     |\n",
            "|    n_updates            | 2240         |\n",
            "|    policy_gradient_loss | -0.00406     |\n",
            "|    value_loss           | 2.81e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 114          |\n",
            "|    time_elapsed         | 1432         |\n",
            "|    total_timesteps      | 466944       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019630615 |\n",
            "|    clip_fraction        | 0.0586       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.49e+04     |\n",
            "|    n_updates            | 2260         |\n",
            "|    policy_gradient_loss | -0.00676     |\n",
            "|    value_loss           | 2.78e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 118         |\n",
            "|    ep_rew_mean          | 1.07e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 115         |\n",
            "|    time_elapsed         | 1445        |\n",
            "|    total_timesteps      | 471040      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001311404 |\n",
            "|    clip_fraction        | 0.0117      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.29e+04    |\n",
            "|    n_updates            | 2280        |\n",
            "|    policy_gradient_loss | -0.00465    |\n",
            "|    value_loss           | 2.54e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 116          |\n",
            "|    time_elapsed         | 1458         |\n",
            "|    total_timesteps      | 475136       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014836394 |\n",
            "|    clip_fraction        | 0.0269       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.29e+04     |\n",
            "|    n_updates            | 2300         |\n",
            "|    policy_gradient_loss | -0.00528     |\n",
            "|    value_loss           | 2.8e+04      |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 124         |\n",
            "|    ep_rew_mean          | 1.16e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 117         |\n",
            "|    time_elapsed         | 1470        |\n",
            "|    total_timesteps      | 479232      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001265323 |\n",
            "|    clip_fraction        | 0.0174      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.41e+04    |\n",
            "|    n_updates            | 2320        |\n",
            "|    policy_gradient_loss | -0.0044     |\n",
            "|    value_loss           | 2.85e+04    |\n",
            "-----------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 124         |\n",
            "|    ep_rew_mean          | 1.16e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 118         |\n",
            "|    time_elapsed         | 1483        |\n",
            "|    total_timesteps      | 483328      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001419111 |\n",
            "|    clip_fraction        | 0.0223      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.49e+04    |\n",
            "|    n_updates            | 2340        |\n",
            "|    policy_gradient_loss | -0.00507    |\n",
            "|    value_loss           | 2.63e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 1.13e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 119          |\n",
            "|    time_elapsed         | 1496         |\n",
            "|    total_timesteps      | 487424       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015871008 |\n",
            "|    clip_fraction        | 0.0287       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.46e+04     |\n",
            "|    n_updates            | 2360         |\n",
            "|    policy_gradient_loss | -0.006       |\n",
            "|    value_loss           | 2.75e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 117         |\n",
            "|    ep_rew_mean          | 1.09e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 120         |\n",
            "|    time_elapsed         | 1508        |\n",
            "|    total_timesteps      | 491520      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001436371 |\n",
            "|    clip_fraction        | 0.019       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.3e+04     |\n",
            "|    n_updates            | 2380        |\n",
            "|    policy_gradient_loss | -0.00501    |\n",
            "|    value_loss           | 2.92e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.13e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 121          |\n",
            "|    time_elapsed         | 1521         |\n",
            "|    total_timesteps      | 495616       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012329762 |\n",
            "|    clip_fraction        | 0.0116       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.24        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.6e+04      |\n",
            "|    n_updates            | 2400         |\n",
            "|    policy_gradient_loss | -0.00456     |\n",
            "|    value_loss           | 3.03e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 1.06e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 122          |\n",
            "|    time_elapsed         | 1534         |\n",
            "|    total_timesteps      | 499712       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012190051 |\n",
            "|    clip_fraction        | 0.00844      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.48e+04     |\n",
            "|    n_updates            | 2420         |\n",
            "|    policy_gradient_loss | -0.0047      |\n",
            "|    value_loss           | 2.97e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 123          |\n",
            "|    time_elapsed         | 1546         |\n",
            "|    total_timesteps      | 503808       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014628568 |\n",
            "|    clip_fraction        | 0.0394       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.48e+04     |\n",
            "|    n_updates            | 2440         |\n",
            "|    policy_gradient_loss | -0.00727     |\n",
            "|    value_loss           | 2.91e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 124          |\n",
            "|    time_elapsed         | 1559         |\n",
            "|    total_timesteps      | 507904       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025280714 |\n",
            "|    clip_fraction        | 0.043        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.54e+04     |\n",
            "|    n_updates            | 2460         |\n",
            "|    policy_gradient_loss | -0.00762     |\n",
            "|    value_loss           | 2.92e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 125          |\n",
            "|    time_elapsed         | 1572         |\n",
            "|    total_timesteps      | 512000       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0023476095 |\n",
            "|    clip_fraction        | 0.0443       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.22e+04     |\n",
            "|    n_updates            | 2480         |\n",
            "|    policy_gradient_loss | -0.00645     |\n",
            "|    value_loss           | 2.69e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 126          |\n",
            "|    time_elapsed         | 1584         |\n",
            "|    total_timesteps      | 516096       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007620808 |\n",
            "|    clip_fraction        | 0.00111      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.68e+04     |\n",
            "|    n_updates            | 2500         |\n",
            "|    policy_gradient_loss | -0.00337     |\n",
            "|    value_loss           | 2.97e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 123         |\n",
            "|    ep_rew_mean          | 1.16e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 127         |\n",
            "|    time_elapsed         | 1597        |\n",
            "|    total_timesteps      | 520192      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001448327 |\n",
            "|    clip_fraction        | 0.0307      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.23       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.26e+04    |\n",
            "|    n_updates            | 2520        |\n",
            "|    policy_gradient_loss | -0.00473    |\n",
            "|    value_loss           | 3e+04       |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 1.18e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 128          |\n",
            "|    time_elapsed         | 1609         |\n",
            "|    total_timesteps      | 524288       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0010909446 |\n",
            "|    clip_fraction        | 0.00581      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.57e+04     |\n",
            "|    n_updates            | 2540         |\n",
            "|    policy_gradient_loss | -0.00328     |\n",
            "|    value_loss           | 3.16e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 127           |\n",
            "|    ep_rew_mean          | 1.21e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 129           |\n",
            "|    time_elapsed         | 1622          |\n",
            "|    total_timesteps      | 528384        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00087104307 |\n",
            "|    clip_fraction        | 0.000867      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | -1.19e-07     |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.78e+04      |\n",
            "|    n_updates            | 2560          |\n",
            "|    policy_gradient_loss | -0.00271      |\n",
            "|    value_loss           | 3.18e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 130          |\n",
            "|    time_elapsed         | 1635         |\n",
            "|    total_timesteps      | 532480       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0034349111 |\n",
            "|    clip_fraction        | 0.189        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.59e+04     |\n",
            "|    n_updates            | 2580         |\n",
            "|    policy_gradient_loss | -0.00154     |\n",
            "|    value_loss           | 2.98e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 1.02e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 131          |\n",
            "|    time_elapsed         | 1647         |\n",
            "|    total_timesteps      | 536576       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020582713 |\n",
            "|    clip_fraction        | 0.0929       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.42e+04     |\n",
            "|    n_updates            | 2600         |\n",
            "|    policy_gradient_loss | -0.00685     |\n",
            "|    value_loss           | 2.86e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 109          |\n",
            "|    ep_rew_mean          | 967          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 132          |\n",
            "|    time_elapsed         | 1660         |\n",
            "|    total_timesteps      | 540672       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014394925 |\n",
            "|    clip_fraction        | 0.0569       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 5.96e-08     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.46e+04     |\n",
            "|    n_updates            | 2620         |\n",
            "|    policy_gradient_loss | -0.00319     |\n",
            "|    value_loss           | 3.03e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 113          |\n",
            "|    ep_rew_mean          | 1.02e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 133          |\n",
            "|    time_elapsed         | 1673         |\n",
            "|    total_timesteps      | 544768       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015307723 |\n",
            "|    clip_fraction        | 0.0528       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.53e+04     |\n",
            "|    n_updates            | 2640         |\n",
            "|    policy_gradient_loss | -0.00525     |\n",
            "|    value_loss           | 2.97e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 113           |\n",
            "|    ep_rew_mean          | 1.01e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 134           |\n",
            "|    time_elapsed         | 1686          |\n",
            "|    total_timesteps      | 548864        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00051692006 |\n",
            "|    clip_fraction        | 0.000537      |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.83e+04      |\n",
            "|    n_updates            | 2660          |\n",
            "|    policy_gradient_loss | -0.00188      |\n",
            "|    value_loss           | 3.09e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 135          |\n",
            "|    time_elapsed         | 1698         |\n",
            "|    total_timesteps      | 552960       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011945686 |\n",
            "|    clip_fraction        | 0.015        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.65e+04     |\n",
            "|    n_updates            | 2680         |\n",
            "|    policy_gradient_loss | -0.00413     |\n",
            "|    value_loss           | 3.06e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 136          |\n",
            "|    time_elapsed         | 1711         |\n",
            "|    total_timesteps      | 557056       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007012391 |\n",
            "|    clip_fraction        | 0.00309      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.58e+04     |\n",
            "|    n_updates            | 2700         |\n",
            "|    policy_gradient_loss | -0.00299     |\n",
            "|    value_loss           | 3.26e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 137          |\n",
            "|    time_elapsed         | 1724         |\n",
            "|    total_timesteps      | 561152       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0029765493 |\n",
            "|    clip_fraction        | 0.0941       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.71e+04     |\n",
            "|    n_updates            | 2720         |\n",
            "|    policy_gradient_loss | -0.00456     |\n",
            "|    value_loss           | 3.16e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 117         |\n",
            "|    ep_rew_mean          | 1.07e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 138         |\n",
            "|    time_elapsed         | 1736        |\n",
            "|    total_timesteps      | 565248      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001483366 |\n",
            "|    clip_fraction        | 0.02        |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 1.19e-07    |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.71e+04    |\n",
            "|    n_updates            | 2740        |\n",
            "|    policy_gradient_loss | -0.00292    |\n",
            "|    value_loss           | 3.27e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 139          |\n",
            "|    time_elapsed         | 1749         |\n",
            "|    total_timesteps      | 569344       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0007664008 |\n",
            "|    clip_fraction        | 0.00222      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.46e+04     |\n",
            "|    n_updates            | 2760         |\n",
            "|    policy_gradient_loss | -0.00304     |\n",
            "|    value_loss           | 3.09e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 119         |\n",
            "|    ep_rew_mean          | 1.12e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 140         |\n",
            "|    time_elapsed         | 1762        |\n",
            "|    total_timesteps      | 573440      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000529455 |\n",
            "|    clip_fraction        | 0.000781    |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.64e+04    |\n",
            "|    n_updates            | 2780        |\n",
            "|    policy_gradient_loss | -0.00232    |\n",
            "|    value_loss           | 3.5e+04     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.13e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 141          |\n",
            "|    time_elapsed         | 1774         |\n",
            "|    total_timesteps      | 577536       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008392136 |\n",
            "|    clip_fraction        | 0.0051       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 2.01e+04     |\n",
            "|    n_updates            | 2800         |\n",
            "|    policy_gradient_loss | -0.00178     |\n",
            "|    value_loss           | 3.39e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 1.15e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 142          |\n",
            "|    time_elapsed         | 1787         |\n",
            "|    total_timesteps      | 581632       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018030023 |\n",
            "|    clip_fraction        | 0.0246       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 2.12e+04     |\n",
            "|    n_updates            | 2820         |\n",
            "|    policy_gradient_loss | -0.00148     |\n",
            "|    value_loss           | 3.45e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 1.17e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 143          |\n",
            "|    time_elapsed         | 1800         |\n",
            "|    total_timesteps      | 585728       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0019708048 |\n",
            "|    clip_fraction        | 0.044        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 2.17e+04     |\n",
            "|    n_updates            | 2840         |\n",
            "|    policy_gradient_loss | -0.00342     |\n",
            "|    value_loss           | 3.47e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 1.14e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 144          |\n",
            "|    time_elapsed         | 1812         |\n",
            "|    total_timesteps      | 589824       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0005678871 |\n",
            "|    clip_fraction        | 0.00143      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.26e+04     |\n",
            "|    n_updates            | 2860         |\n",
            "|    policy_gradient_loss | -0.00144     |\n",
            "|    value_loss           | 3.38e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 145          |\n",
            "|    time_elapsed         | 1825         |\n",
            "|    total_timesteps      | 593920       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008779607 |\n",
            "|    clip_fraction        | 0.00184      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.76e+04     |\n",
            "|    n_updates            | 2880         |\n",
            "|    policy_gradient_loss | -0.00277     |\n",
            "|    value_loss           | 3.33e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 146          |\n",
            "|    time_elapsed         | 1838         |\n",
            "|    total_timesteps      | 598016       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016013745 |\n",
            "|    clip_fraction        | 0.0483       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.93e+04     |\n",
            "|    n_updates            | 2900         |\n",
            "|    policy_gradient_loss | -0.00715     |\n",
            "|    value_loss           | 3.38e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 112          |\n",
            "|    ep_rew_mean          | 996          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 147          |\n",
            "|    time_elapsed         | 1850         |\n",
            "|    total_timesteps      | 602112       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018080734 |\n",
            "|    clip_fraction        | 0.118        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.62e+04     |\n",
            "|    n_updates            | 2920         |\n",
            "|    policy_gradient_loss | -0.00465     |\n",
            "|    value_loss           | 3.39e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 114          |\n",
            "|    ep_rew_mean          | 1.02e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 148          |\n",
            "|    time_elapsed         | 1863         |\n",
            "|    total_timesteps      | 606208       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0065433374 |\n",
            "|    clip_fraction        | 0.0735       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.53e+04     |\n",
            "|    n_updates            | 2940         |\n",
            "|    policy_gradient_loss | -0.00509     |\n",
            "|    value_loss           | 3.46e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 110          |\n",
            "|    ep_rew_mean          | 971          |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 149          |\n",
            "|    time_elapsed         | 1876         |\n",
            "|    total_timesteps      | 610304       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014632827 |\n",
            "|    clip_fraction        | 0.133        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.61e+04     |\n",
            "|    n_updates            | 2960         |\n",
            "|    policy_gradient_loss | -0.00392     |\n",
            "|    value_loss           | 3.49e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 150          |\n",
            "|    time_elapsed         | 1888         |\n",
            "|    total_timesteps      | 614400       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012518915 |\n",
            "|    clip_fraction        | 0.0151       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.68e+04     |\n",
            "|    n_updates            | 2980         |\n",
            "|    policy_gradient_loss | -0.00359     |\n",
            "|    value_loss           | 3.24e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 123         |\n",
            "|    ep_rew_mean          | 1.14e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 151         |\n",
            "|    time_elapsed         | 1901        |\n",
            "|    total_timesteps      | 618496      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.001308194 |\n",
            "|    clip_fraction        | 0.00747     |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.44e+04    |\n",
            "|    n_updates            | 3000        |\n",
            "|    policy_gradient_loss | -0.00317    |\n",
            "|    value_loss           | 3.52e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 152          |\n",
            "|    time_elapsed         | 1913         |\n",
            "|    total_timesteps      | 622592       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0008891843 |\n",
            "|    clip_fraction        | 0.00398      |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.26e+04     |\n",
            "|    n_updates            | 3020         |\n",
            "|    policy_gradient_loss | -0.00227     |\n",
            "|    value_loss           | 3.36e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 1.16e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 153          |\n",
            "|    time_elapsed         | 1926         |\n",
            "|    total_timesteps      | 626688       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014889564 |\n",
            "|    clip_fraction        | 0.0528       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.55e+04     |\n",
            "|    n_updates            | 3040         |\n",
            "|    policy_gradient_loss | -0.00634     |\n",
            "|    value_loss           | 3.46e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 123           |\n",
            "|    ep_rew_mean          | 1.16e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 154           |\n",
            "|    time_elapsed         | 1939          |\n",
            "|    total_timesteps      | 630784        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00015287254 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.21         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.87e+04      |\n",
            "|    n_updates            | 3060          |\n",
            "|    policy_gradient_loss | -0.00109      |\n",
            "|    value_loss           | 3.64e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 131           |\n",
            "|    ep_rew_mean          | 1.27e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 155           |\n",
            "|    time_elapsed         | 1951          |\n",
            "|    total_timesteps      | 634880        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00081404345 |\n",
            "|    clip_fraction        | 0.00205       |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 2.01e+04      |\n",
            "|    n_updates            | 3080          |\n",
            "|    policy_gradient_loss | -0.00218      |\n",
            "|    value_loss           | 3.79e+04      |\n",
            "-------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 124         |\n",
            "|    ep_rew_mean          | 1.17e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 156         |\n",
            "|    time_elapsed         | 1964        |\n",
            "|    total_timesteps      | 638976      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 5.35493e-05 |\n",
            "|    clip_fraction        | 0           |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.64e+04    |\n",
            "|    n_updates            | 3100        |\n",
            "|    policy_gradient_loss | -0.000774   |\n",
            "|    value_loss           | 3.51e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 157          |\n",
            "|    time_elapsed         | 1976         |\n",
            "|    total_timesteps      | 643072       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018974413 |\n",
            "|    clip_fraction        | 0.108        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.84e+04     |\n",
            "|    n_updates            | 3120         |\n",
            "|    policy_gradient_loss | -0.0072      |\n",
            "|    value_loss           | 3.47e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 158          |\n",
            "|    time_elapsed         | 1989         |\n",
            "|    total_timesteps      | 647168       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015377456 |\n",
            "|    clip_fraction        | 0.0931       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 2.27e+04     |\n",
            "|    n_updates            | 3140         |\n",
            "|    policy_gradient_loss | -0.00817     |\n",
            "|    value_loss           | 3.73e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 123         |\n",
            "|    ep_rew_mean          | 1.18e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 159         |\n",
            "|    time_elapsed         | 2001        |\n",
            "|    total_timesteps      | 651264      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.000940754 |\n",
            "|    clip_fraction        | 0.00245     |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.73e+04    |\n",
            "|    n_updates            | 3160        |\n",
            "|    policy_gradient_loss | -0.00191    |\n",
            "|    value_loss           | 3.8e+04     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 1.22e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 160          |\n",
            "|    time_elapsed         | 2014         |\n",
            "|    total_timesteps      | 655360       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.605454e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 2.06e+04     |\n",
            "|    n_updates            | 3180         |\n",
            "|    policy_gradient_loss | -0.000972    |\n",
            "|    value_loss           | 4.14e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 124          |\n",
            "|    ep_rew_mean          | 1.19e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 161          |\n",
            "|    time_elapsed         | 2026         |\n",
            "|    total_timesteps      | 659456       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014803208 |\n",
            "|    clip_fraction        | 0.0585       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 2.12e+04     |\n",
            "|    n_updates            | 3200         |\n",
            "|    policy_gradient_loss | -0.00572     |\n",
            "|    value_loss           | 3.62e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 162          |\n",
            "|    time_elapsed         | 2039         |\n",
            "|    total_timesteps      | 663552       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016811796 |\n",
            "|    clip_fraction        | 0.0435       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.6e+04      |\n",
            "|    n_updates            | 3220         |\n",
            "|    policy_gradient_loss | -0.00602     |\n",
            "|    value_loss           | 3.57e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 163          |\n",
            "|    time_elapsed         | 2051         |\n",
            "|    total_timesteps      | 667648       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0020014828 |\n",
            "|    clip_fraction        | 0.0555       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.91e+04     |\n",
            "|    n_updates            | 3240         |\n",
            "|    policy_gradient_loss | -0.00607     |\n",
            "|    value_loss           | 3.6e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 164          |\n",
            "|    time_elapsed         | 2064         |\n",
            "|    total_timesteps      | 671744       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017178856 |\n",
            "|    clip_fraction        | 0.0763       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.8e+04      |\n",
            "|    n_updates            | 3260         |\n",
            "|    policy_gradient_loss | -0.00664     |\n",
            "|    value_loss           | 3.59e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 119          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 165          |\n",
            "|    time_elapsed         | 2077         |\n",
            "|    total_timesteps      | 675840       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011948032 |\n",
            "|    clip_fraction        | 0.0282       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.78e+04     |\n",
            "|    n_updates            | 3280         |\n",
            "|    policy_gradient_loss | -0.00386     |\n",
            "|    value_loss           | 3.6e+04      |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 1.15e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 166          |\n",
            "|    time_elapsed         | 2089         |\n",
            "|    total_timesteps      | 679936       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015214693 |\n",
            "|    clip_fraction        | 0.0516       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 2.16e+04     |\n",
            "|    n_updates            | 3300         |\n",
            "|    policy_gradient_loss | -0.00492     |\n",
            "|    value_loss           | 3.91e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 121          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 167          |\n",
            "|    time_elapsed         | 2102         |\n",
            "|    total_timesteps      | 684032       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0027638061 |\n",
            "|    clip_fraction        | 0.012        |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.91e+04     |\n",
            "|    n_updates            | 3320         |\n",
            "|    policy_gradient_loss | -0.00227     |\n",
            "|    value_loss           | 3.59e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 124         |\n",
            "|    ep_rew_mean          | 1.15e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 168         |\n",
            "|    time_elapsed         | 2115        |\n",
            "|    total_timesteps      | 688128      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002228082 |\n",
            "|    clip_fraction        | 0.0473      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.61e+04    |\n",
            "|    n_updates            | 3340        |\n",
            "|    policy_gradient_loss | -0.00504    |\n",
            "|    value_loss           | 3.6e+04     |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.13e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 169          |\n",
            "|    time_elapsed         | 2128         |\n",
            "|    total_timesteps      | 692224       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0015886386 |\n",
            "|    clip_fraction        | 0.0691       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 2.26e+04     |\n",
            "|    n_updates            | 3360         |\n",
            "|    policy_gradient_loss | -0.00626     |\n",
            "|    value_loss           | 3.83e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.14e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 170          |\n",
            "|    time_elapsed         | 2140         |\n",
            "|    total_timesteps      | 696320       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0012249969 |\n",
            "|    clip_fraction        | 0.0444       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.2         |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.71e+04     |\n",
            "|    n_updates            | 3380         |\n",
            "|    policy_gradient_loss | -0.00383     |\n",
            "|    value_loss           | 3.74e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 119         |\n",
            "|    ep_rew_mean          | 1.1e+03     |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 171         |\n",
            "|    time_elapsed         | 2153        |\n",
            "|    total_timesteps      | 700416      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.003806279 |\n",
            "|    clip_fraction        | 0.043       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.73e+04    |\n",
            "|    n_updates            | 3400        |\n",
            "|    policy_gradient_loss | -0.0042     |\n",
            "|    value_loss           | 3.85e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.09e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 172          |\n",
            "|    time_elapsed         | 2166         |\n",
            "|    total_timesteps      | 704512       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0016109012 |\n",
            "|    clip_fraction        | 0.0601       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.83e+04     |\n",
            "|    n_updates            | 3420         |\n",
            "|    policy_gradient_loss | -0.00564     |\n",
            "|    value_loss           | 3.87e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 115         |\n",
            "|    ep_rew_mean          | 1.04e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 173         |\n",
            "|    time_elapsed         | 2178        |\n",
            "|    total_timesteps      | 708608      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002743111 |\n",
            "|    clip_fraction        | 0.0737      |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.21       |\n",
            "|    explained_variance   | 0           |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 2.05e+04    |\n",
            "|    n_updates            | 3440        |\n",
            "|    policy_gradient_loss | -0.00514    |\n",
            "|    value_loss           | 4.13e+04    |\n",
            "-----------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 1.05e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 174          |\n",
            "|    time_elapsed         | 2191         |\n",
            "|    total_timesteps      | 712704       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0011942638 |\n",
            "|    clip_fraction        | 0.0613       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | -1.19e-07    |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.68e+04     |\n",
            "|    n_updates            | 3460         |\n",
            "|    policy_gradient_loss | -0.00408     |\n",
            "|    value_loss           | 3.85e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 175          |\n",
            "|    time_elapsed         | 2203         |\n",
            "|    total_timesteps      | 716800       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0014669381 |\n",
            "|    clip_fraction        | 0.0598       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.31e+04     |\n",
            "|    n_updates            | 3480         |\n",
            "|    policy_gradient_loss | -0.00511     |\n",
            "|    value_loss           | 3.75e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 127           |\n",
            "|    ep_rew_mean          | 1.21e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 176           |\n",
            "|    time_elapsed         | 2216          |\n",
            "|    total_timesteps      | 720896        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00022410275 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.21         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 2.08e+04      |\n",
            "|    n_updates            | 3500          |\n",
            "|    policy_gradient_loss | -0.00135      |\n",
            "|    value_loss           | 3.89e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 1.2e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 177          |\n",
            "|    time_elapsed         | 2228         |\n",
            "|    total_timesteps      | 724992       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0017380222 |\n",
            "|    clip_fraction        | 0.0206       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 2.19e+04     |\n",
            "|    n_updates            | 3520         |\n",
            "|    policy_gradient_loss | -0.00207     |\n",
            "|    value_loss           | 3.99e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 178          |\n",
            "|    time_elapsed         | 2241         |\n",
            "|    total_timesteps      | 729088       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0025513323 |\n",
            "|    clip_fraction        | 0.0344       |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.21        |\n",
            "|    explained_variance   | 0            |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 2.17e+04     |\n",
            "|    n_updates            | 3540         |\n",
            "|    policy_gradient_loss | -0.00523     |\n",
            "|    value_loss           | 3.88e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 1.05e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 179          |\n",
            "|    time_elapsed         | 2254         |\n",
            "|    total_timesteps      | 733184       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 0.0018010882 |\n",
            "|    clip_fraction        | 0.13         |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 1.19e-07     |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.76e+04     |\n",
            "|    n_updates            | 3560         |\n",
            "|    policy_gradient_loss | -0.00599     |\n",
            "|    value_loss           | 3.81e+04     |\n",
            "------------------------------------------\n",
            "-----------------------------------------\n",
            "| rollout/                |             |\n",
            "|    ep_len_mean          | 117         |\n",
            "|    ep_rew_mean          | 1.08e+03    |\n",
            "| time/                   |             |\n",
            "|    fps                  | 325         |\n",
            "|    iterations           | 180         |\n",
            "|    time_elapsed         | 2266        |\n",
            "|    total_timesteps      | 737280      |\n",
            "| train/                  |             |\n",
            "|    approx_kl            | 0.002180648 |\n",
            "|    clip_fraction        | 0.084       |\n",
            "|    clip_range           | 0.1         |\n",
            "|    entropy_loss         | -1.22       |\n",
            "|    explained_variance   | -1.19e-07   |\n",
            "|    learning_rate        | 5e-05       |\n",
            "|    loss                 | 1.99e+04    |\n",
            "|    n_updates            | 3580        |\n",
            "|    policy_gradient_loss | -0.00611    |\n",
            "|    value_loss           | 4.05e+04    |\n",
            "-----------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 120           |\n",
            "|    ep_rew_mean          | 1.13e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 181           |\n",
            "|    time_elapsed         | 2279          |\n",
            "|    total_timesteps      | 741376        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 0.00074155733 |\n",
            "|    clip_fraction        | 0.0033        |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0             |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 2.15e+04      |\n",
            "|    n_updates            | 3600          |\n",
            "|    policy_gradient_loss | -0.0039       |\n",
            "|    value_loss           | 4.02e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 182          |\n",
            "|    time_elapsed         | 2291         |\n",
            "|    total_timesteps      | 745472       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.234059e-05 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.00268      |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.97e+04     |\n",
            "|    n_updates            | 3620         |\n",
            "|    policy_gradient_loss | -0.000703    |\n",
            "|    value_loss           | 3.69e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 116           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 183           |\n",
            "|    time_elapsed         | 2304          |\n",
            "|    total_timesteps      | 749568        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7419967e-05 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.0044        |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.5e+04       |\n",
            "|    n_updates            | 3640          |\n",
            "|    policy_gradient_loss | -0.000552     |\n",
            "|    value_loss           | 3.72e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 116          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 184          |\n",
            "|    time_elapsed         | 2317         |\n",
            "|    total_timesteps      | 753664       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.773028e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.039        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 1.41e+04     |\n",
            "|    n_updates            | 3660         |\n",
            "|    policy_gradient_loss | -6.12e-05    |\n",
            "|    value_loss           | 3.36e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 117           |\n",
            "|    ep_rew_mean          | 1.09e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 185           |\n",
            "|    time_elapsed         | 2329          |\n",
            "|    total_timesteps      | 757760        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.0434104e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.141         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.2e+04       |\n",
            "|    n_updates            | 3680          |\n",
            "|    policy_gradient_loss | -2.25e-05     |\n",
            "|    value_loss           | 2.89e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 117           |\n",
            "|    ep_rew_mean          | 1.09e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 186           |\n",
            "|    time_elapsed         | 2342          |\n",
            "|    total_timesteps      | 761856        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 7.1464456e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.16          |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.29e+04      |\n",
            "|    n_updates            | 3700          |\n",
            "|    policy_gradient_loss | -2.44e-05     |\n",
            "|    value_loss           | 2.65e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 116           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 187           |\n",
            "|    time_elapsed         | 2355          |\n",
            "|    total_timesteps      | 765952        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.7121936e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.116         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 9.76e+03      |\n",
            "|    n_updates            | 3720          |\n",
            "|    policy_gradient_loss | -2.01e-05     |\n",
            "|    value_loss           | 2.72e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 119           |\n",
            "|    ep_rew_mean          | 1.1e+03       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 188           |\n",
            "|    time_elapsed         | 2368          |\n",
            "|    total_timesteps      | 770048        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 9.3117706e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.222         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 9e+03         |\n",
            "|    n_updates            | 3740          |\n",
            "|    policy_gradient_loss | -2.83e-05     |\n",
            "|    value_loss           | 2.46e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 1.2e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 189          |\n",
            "|    time_elapsed         | 2380         |\n",
            "|    total_timesteps      | 774144       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.289338e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.223        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 9.16e+03     |\n",
            "|    n_updates            | 3760         |\n",
            "|    policy_gradient_loss | -1.69e-05    |\n",
            "|    value_loss           | 2.28e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 124           |\n",
            "|    ep_rew_mean          | 1.17e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 190           |\n",
            "|    time_elapsed         | 2393          |\n",
            "|    total_timesteps      | 778240        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0477379e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.21          |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 9.14e+03      |\n",
            "|    n_updates            | 3780          |\n",
            "|    policy_gradient_loss | -2.04e-05     |\n",
            "|    value_loss           | 2.33e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 122           |\n",
            "|    ep_rew_mean          | 1.15e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 191           |\n",
            "|    time_elapsed         | 2405          |\n",
            "|    total_timesteps      | 782336        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.0224328e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.144         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 1.14e+04      |\n",
            "|    n_updates            | 3800          |\n",
            "|    policy_gradient_loss | -1.41e-05     |\n",
            "|    value_loss           | 2.65e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 117           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 192           |\n",
            "|    time_elapsed         | 2418          |\n",
            "|    total_timesteps      | 786432        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.1544762e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.142         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 8.22e+03      |\n",
            "|    n_updates            | 3820          |\n",
            "|    policy_gradient_loss | -2.66e-05     |\n",
            "|    value_loss           | 2.39e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 115          |\n",
            "|    ep_rew_mean          | 1.03e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 193          |\n",
            "|    time_elapsed         | 2431         |\n",
            "|    total_timesteps      | 790528       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.292853e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.16         |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 9.63e+03     |\n",
            "|    n_updates            | 3840         |\n",
            "|    policy_gradient_loss | -2.38e-05    |\n",
            "|    value_loss           | 2.34e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 112          |\n",
            "|    ep_rew_mean          | 1e+03        |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 194          |\n",
            "|    time_elapsed         | 2443         |\n",
            "|    total_timesteps      | 794624       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.929622e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.255        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 7.71e+03     |\n",
            "|    n_updates            | 3860         |\n",
            "|    policy_gradient_loss | -1.98e-05    |\n",
            "|    value_loss           | 2.07e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 117           |\n",
            "|    ep_rew_mean          | 1.07e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 195           |\n",
            "|    time_elapsed         | 2456          |\n",
            "|    total_timesteps      | 798720        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.6245987e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.256         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 9.69e+03      |\n",
            "|    n_updates            | 3880          |\n",
            "|    policy_gradient_loss | -1.82e-05     |\n",
            "|    value_loss           | 2.06e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 119           |\n",
            "|    ep_rew_mean          | 1.1e+03       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 196           |\n",
            "|    time_elapsed         | 2468          |\n",
            "|    total_timesteps      | 802816        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5617115e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.137         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 7.36e+03      |\n",
            "|    n_updates            | 3900          |\n",
            "|    policy_gradient_loss | -3.31e-05     |\n",
            "|    value_loss           | 2.04e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 197          |\n",
            "|    time_elapsed         | 2481         |\n",
            "|    total_timesteps      | 806912       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.499774e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.204        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 6.56e+03     |\n",
            "|    n_updates            | 3920         |\n",
            "|    policy_gradient_loss | -2.45e-05    |\n",
            "|    value_loss           | 1.97e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 118           |\n",
            "|    ep_rew_mean          | 1.08e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 198           |\n",
            "|    time_elapsed         | 2494          |\n",
            "|    total_timesteps      | 811008        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5676778e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.255         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 6.84e+03      |\n",
            "|    n_updates            | 3940          |\n",
            "|    policy_gradient_loss | -3.21e-05     |\n",
            "|    value_loss           | 2e+04         |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 117           |\n",
            "|    ep_rew_mean          | 1.08e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 199           |\n",
            "|    time_elapsed         | 2506          |\n",
            "|    total_timesteps      | 815104        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.4121407e-08 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.183         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 7.63e+03      |\n",
            "|    n_updates            | 3960          |\n",
            "|    policy_gradient_loss | -2.34e-05     |\n",
            "|    value_loss           | 2.09e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.15e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 200          |\n",
            "|    time_elapsed         | 2519         |\n",
            "|    total_timesteps      | 819200       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 8.998177e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.127        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 6.84e+03     |\n",
            "|    n_updates            | 3980         |\n",
            "|    policy_gradient_loss | -2.97e-05    |\n",
            "|    value_loss           | 2.28e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 122           |\n",
            "|    ep_rew_mean          | 1.15e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 201           |\n",
            "|    time_elapsed         | 2532          |\n",
            "|    total_timesteps      | 823296        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.1851156e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.21          |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 7.64e+03      |\n",
            "|    n_updates            | 4000          |\n",
            "|    policy_gradient_loss | -3.8e-05      |\n",
            "|    value_loss           | 2.15e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 202          |\n",
            "|    time_elapsed         | 2544         |\n",
            "|    total_timesteps      | 827392       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.180168e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.279        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 5.74e+03     |\n",
            "|    n_updates            | 4020         |\n",
            "|    policy_gradient_loss | -3.75e-05    |\n",
            "|    value_loss           | 1.91e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 117           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 203           |\n",
            "|    time_elapsed         | 2557          |\n",
            "|    total_timesteps      | 831488        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.0570587e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.288         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 8.17e+03      |\n",
            "|    n_updates            | 4040          |\n",
            "|    policy_gradient_loss | -3.18e-05     |\n",
            "|    value_loss           | 2.04e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 116           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 204           |\n",
            "|    time_elapsed         | 2570          |\n",
            "|    total_timesteps      | 835584        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.5647674e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.299         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.89e+03      |\n",
            "|    n_updates            | 4060          |\n",
            "|    policy_gradient_loss | -2.92e-05     |\n",
            "|    value_loss           | 1.95e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 120           |\n",
            "|    ep_rew_mean          | 1.11e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 205           |\n",
            "|    time_elapsed         | 2582          |\n",
            "|    total_timesteps      | 839680        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0570511e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.252         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 7.69e+03      |\n",
            "|    n_updates            | 4080          |\n",
            "|    policy_gradient_loss | -2.83e-05     |\n",
            "|    value_loss           | 1.84e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.11e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 206          |\n",
            "|    time_elapsed         | 2595         |\n",
            "|    total_timesteps      | 843776       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.336509e-08 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.315        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 4.92e+03     |\n",
            "|    n_updates            | 4100         |\n",
            "|    policy_gradient_loss | -2.95e-05    |\n",
            "|    value_loss           | 1.63e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 121           |\n",
            "|    ep_rew_mean          | 1.12e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 207           |\n",
            "|    time_elapsed         | 2607          |\n",
            "|    total_timesteps      | 847872        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.2865348e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.356         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 6.68e+03      |\n",
            "|    n_updates            | 4120          |\n",
            "|    policy_gradient_loss | -2.86e-05     |\n",
            "|    value_loss           | 1.72e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 120           |\n",
            "|    ep_rew_mean          | 1.12e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 208           |\n",
            "|    time_elapsed         | 2620          |\n",
            "|    total_timesteps      | 851968        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.6426202e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.242         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 6.15e+03      |\n",
            "|    n_updates            | 4140          |\n",
            "|    policy_gradient_loss | -2.9e-05      |\n",
            "|    value_loss           | 1.93e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 1.2e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 209          |\n",
            "|    time_elapsed         | 2632         |\n",
            "|    total_timesteps      | 856064       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.975365e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.28         |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 8.83e+03     |\n",
            "|    n_updates            | 4160         |\n",
            "|    policy_gradient_loss | -5.24e-05    |\n",
            "|    value_loss           | 2.01e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 210          |\n",
            "|    time_elapsed         | 2645         |\n",
            "|    total_timesteps      | 860160       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.616442e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.236        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 5.66e+03     |\n",
            "|    n_updates            | 4180         |\n",
            "|    policy_gradient_loss | -4.31e-05    |\n",
            "|    value_loss           | 2.11e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 118           |\n",
            "|    ep_rew_mean          | 1.1e+03       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 211           |\n",
            "|    time_elapsed         | 2658          |\n",
            "|    total_timesteps      | 864256        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7257844e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.256         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.02e+03      |\n",
            "|    n_updates            | 4200          |\n",
            "|    policy_gradient_loss | -3.05e-05     |\n",
            "|    value_loss           | 1.83e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 116           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 212           |\n",
            "|    time_elapsed         | 2670          |\n",
            "|    total_timesteps      | 868352        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.5548798e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.273         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 8.72e+03      |\n",
            "|    n_updates            | 4220          |\n",
            "|    policy_gradient_loss | -3.54e-05     |\n",
            "|    value_loss           | 2.09e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 1.2e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 213          |\n",
            "|    time_elapsed         | 2683         |\n",
            "|    total_timesteps      | 872448       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 7.999697e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.319        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 4.3e+03      |\n",
            "|    n_updates            | 4240         |\n",
            "|    policy_gradient_loss | -5.57e-05    |\n",
            "|    value_loss           | 1.47e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 125          |\n",
            "|    ep_rew_mean          | 1.18e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 214          |\n",
            "|    time_elapsed         | 2695         |\n",
            "|    total_timesteps      | 876544       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.099041e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.391        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 5.73e+03     |\n",
            "|    n_updates            | 4260         |\n",
            "|    policy_gradient_loss | -3.73e-05    |\n",
            "|    value_loss           | 1.55e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 123           |\n",
            "|    ep_rew_mean          | 1.16e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 215           |\n",
            "|    time_elapsed         | 2708          |\n",
            "|    total_timesteps      | 880640        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.5433914e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.378         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.98e+03      |\n",
            "|    n_updates            | 4280          |\n",
            "|    policy_gradient_loss | -4.95e-05     |\n",
            "|    value_loss           | 1.58e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 120           |\n",
            "|    ep_rew_mean          | 1.11e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 216           |\n",
            "|    time_elapsed         | 2720          |\n",
            "|    total_timesteps      | 884736        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 4.0992745e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.392         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.73e+03      |\n",
            "|    n_updates            | 4300          |\n",
            "|    policy_gradient_loss | -4.91e-05     |\n",
            "|    value_loss           | 1.65e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 121           |\n",
            "|    ep_rew_mean          | 1.13e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 217           |\n",
            "|    time_elapsed         | 2733          |\n",
            "|    total_timesteps      | 888832        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.5752524e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.348         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 4.59e+03      |\n",
            "|    n_updates            | 4320          |\n",
            "|    policy_gradient_loss | -3.79e-05     |\n",
            "|    value_loss           | 1.5e+04       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 121           |\n",
            "|    ep_rew_mean          | 1.13e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 218           |\n",
            "|    time_elapsed         | 2746          |\n",
            "|    total_timesteps      | 892928        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.7589052e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.21         |\n",
            "|    explained_variance   | 0.373         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 6.03e+03      |\n",
            "|    n_updates            | 4340          |\n",
            "|    policy_gradient_loss | -4.91e-05     |\n",
            "|    value_loss           | 1.64e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.1e+03      |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 219          |\n",
            "|    time_elapsed         | 2758         |\n",
            "|    total_timesteps      | 897024       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 6.843911e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.312        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 7.31e+03     |\n",
            "|    n_updates            | 4360         |\n",
            "|    policy_gradient_loss | -6.69e-05    |\n",
            "|    value_loss           | 1.79e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 121           |\n",
            "|    ep_rew_mean          | 1.14e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 220           |\n",
            "|    time_elapsed         | 2771          |\n",
            "|    total_timesteps      | 901120        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.6247213e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.335         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.44e+03      |\n",
            "|    n_updates            | 4380          |\n",
            "|    policy_gradient_loss | -3.93e-05     |\n",
            "|    value_loss           | 1.73e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 124           |\n",
            "|    ep_rew_mean          | 1.17e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 221           |\n",
            "|    time_elapsed         | 2783          |\n",
            "|    total_timesteps      | 905216        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.9987783e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.355         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 7.45e+03      |\n",
            "|    n_updates            | 4400          |\n",
            "|    policy_gradient_loss | -4e-05        |\n",
            "|    value_loss           | 1.68e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 124           |\n",
            "|    ep_rew_mean          | 1.19e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 222           |\n",
            "|    time_elapsed         | 2796          |\n",
            "|    total_timesteps      | 909312        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.7117418e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.343         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 6.3e+03       |\n",
            "|    n_updates            | 4420          |\n",
            "|    policy_gradient_loss | -3.29e-05     |\n",
            "|    value_loss           | 1.8e+04       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 123           |\n",
            "|    ep_rew_mean          | 1.16e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 223           |\n",
            "|    time_elapsed         | 2808          |\n",
            "|    total_timesteps      | 913408        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.3248216e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.305         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 6.17e+03      |\n",
            "|    n_updates            | 4440          |\n",
            "|    policy_gradient_loss | -4.18e-05     |\n",
            "|    value_loss           | 1.71e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 120           |\n",
            "|    ep_rew_mean          | 1.13e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 224           |\n",
            "|    time_elapsed         | 2821          |\n",
            "|    total_timesteps      | 917504        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.7303835e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.381         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.03e+03      |\n",
            "|    n_updates            | 4460          |\n",
            "|    policy_gradient_loss | -5.08e-05     |\n",
            "|    value_loss           | 1.56e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 124           |\n",
            "|    ep_rew_mean          | 1.15e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 225           |\n",
            "|    time_elapsed         | 2833          |\n",
            "|    total_timesteps      | 921600        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.0343654e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.381         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.28e+03      |\n",
            "|    n_updates            | 4480          |\n",
            "|    policy_gradient_loss | -4.68e-05     |\n",
            "|    value_loss           | 1.66e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 118          |\n",
            "|    ep_rew_mean          | 1.08e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 226          |\n",
            "|    time_elapsed         | 2846         |\n",
            "|    total_timesteps      | 925696       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 5.385664e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.413        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 6.13e+03     |\n",
            "|    n_updates            | 4500         |\n",
            "|    policy_gradient_loss | -5.49e-05    |\n",
            "|    value_loss           | 1.51e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 116           |\n",
            "|    ep_rew_mean          | 1.05e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 227           |\n",
            "|    time_elapsed         | 2858          |\n",
            "|    total_timesteps      | 929792        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.0046795e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.388         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 7.52e+03      |\n",
            "|    n_updates            | 4520          |\n",
            "|    policy_gradient_loss | -3.72e-05     |\n",
            "|    value_loss           | 1.89e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 114           |\n",
            "|    ep_rew_mean          | 1.02e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 228           |\n",
            "|    time_elapsed         | 2871          |\n",
            "|    total_timesteps      | 933888        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.3900066e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.436         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.28e+03      |\n",
            "|    n_updates            | 4540          |\n",
            "|    policy_gradient_loss | -4.58e-05     |\n",
            "|    value_loss           | 1.44e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 116           |\n",
            "|    ep_rew_mean          | 1.06e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 229           |\n",
            "|    time_elapsed         | 2883          |\n",
            "|    total_timesteps      | 937984        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.2528548e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.607         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 3.74e+03      |\n",
            "|    n_updates            | 4560          |\n",
            "|    policy_gradient_loss | -3.54e-05     |\n",
            "|    value_loss           | 1.09e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 122          |\n",
            "|    ep_rew_mean          | 1.13e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 230          |\n",
            "|    time_elapsed         | 2896         |\n",
            "|    total_timesteps      | 942080       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.724317e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.442        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 4.66e+03     |\n",
            "|    n_updates            | 4580         |\n",
            "|    policy_gradient_loss | -6.94e-05    |\n",
            "|    value_loss           | 1.35e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 123           |\n",
            "|    ep_rew_mean          | 1.16e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 231           |\n",
            "|    time_elapsed         | 2908          |\n",
            "|    total_timesteps      | 946176        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.9900548e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.496         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 4.61e+03      |\n",
            "|    n_updates            | 4600          |\n",
            "|    policy_gradient_loss | -3.42e-05     |\n",
            "|    value_loss           | 1.3e+04       |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 120           |\n",
            "|    ep_rew_mean          | 1.11e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 232           |\n",
            "|    time_elapsed         | 2921          |\n",
            "|    total_timesteps      | 950272        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 5.3337135e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.465         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.36e+03      |\n",
            "|    n_updates            | 4620          |\n",
            "|    policy_gradient_loss | -4.48e-05     |\n",
            "|    value_loss           | 1.47e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 120           |\n",
            "|    ep_rew_mean          | 1.1e+03       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 233           |\n",
            "|    time_elapsed         | 2934          |\n",
            "|    total_timesteps      | 954368        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 6.0757156e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.519         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 4.16e+03      |\n",
            "|    n_updates            | 4640          |\n",
            "|    policy_gradient_loss | -6.17e-05     |\n",
            "|    value_loss           | 1.38e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 119           |\n",
            "|    ep_rew_mean          | 1.1e+03       |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 234           |\n",
            "|    time_elapsed         | 2946          |\n",
            "|    total_timesteps      | 958464        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.3814285e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.538         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 4.51e+03      |\n",
            "|    n_updates            | 4660          |\n",
            "|    policy_gradient_loss | -4.43e-05     |\n",
            "|    value_loss           | 1.28e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 122           |\n",
            "|    ep_rew_mean          | 1.14e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 235           |\n",
            "|    time_elapsed         | 2959          |\n",
            "|    total_timesteps      | 962560        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.0440126e-06 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.517         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 3.68e+03      |\n",
            "|    n_updates            | 4680          |\n",
            "|    policy_gradient_loss | -6.56e-05     |\n",
            "|    value_loss           | 1.24e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 123          |\n",
            "|    ep_rew_mean          | 1.16e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 236          |\n",
            "|    time_elapsed         | 2971         |\n",
            "|    total_timesteps      | 966656       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 2.526649e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.521        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 3.49e+03     |\n",
            "|    n_updates            | 4700         |\n",
            "|    policy_gradient_loss | -4.84e-05    |\n",
            "|    value_loss           | 1.17e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 126          |\n",
            "|    ep_rew_mean          | 1.21e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 237          |\n",
            "|    time_elapsed         | 2984         |\n",
            "|    total_timesteps      | 970752       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 3.925379e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.23        |\n",
            "|    explained_variance   | 0.488        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 3.06e+03     |\n",
            "|    n_updates            | 4720         |\n",
            "|    policy_gradient_loss | -4.75e-05    |\n",
            "|    value_loss           | 1.39e+04     |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 125           |\n",
            "|    ep_rew_mean          | 1.17e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 238           |\n",
            "|    time_elapsed         | 2996          |\n",
            "|    total_timesteps      | 974848        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.8201688e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.477         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.53e+03      |\n",
            "|    n_updates            | 4740          |\n",
            "|    policy_gradient_loss | -3.88e-05     |\n",
            "|    value_loss           | 1.43e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 123           |\n",
            "|    ep_rew_mean          | 1.15e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 239           |\n",
            "|    time_elapsed         | 3009          |\n",
            "|    total_timesteps      | 978944        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.1221134e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.551         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 3.81e+03      |\n",
            "|    n_updates            | 4760          |\n",
            "|    policy_gradient_loss | -5.15e-05     |\n",
            "|    value_loss           | 1.22e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 121           |\n",
            "|    ep_rew_mean          | 1.13e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 240           |\n",
            "|    time_elapsed         | 3021          |\n",
            "|    total_timesteps      | 983040        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 8.2232145e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.437         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.36e+03      |\n",
            "|    n_updates            | 4780          |\n",
            "|    policy_gradient_loss | -6.61e-05     |\n",
            "|    value_loss           | 1.56e+04      |\n",
            "-------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 120          |\n",
            "|    ep_rew_mean          | 1.12e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 241          |\n",
            "|    time_elapsed         | 3034         |\n",
            "|    total_timesteps      | 987136       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 4.414469e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.52         |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 4.32e+03     |\n",
            "|    n_updates            | 4800         |\n",
            "|    policy_gradient_loss | -5.08e-05    |\n",
            "|    value_loss           | 1.34e+04     |\n",
            "------------------------------------------\n",
            "------------------------------------------\n",
            "| rollout/                |              |\n",
            "|    ep_len_mean          | 117          |\n",
            "|    ep_rew_mean          | 1.07e+03     |\n",
            "| time/                   |              |\n",
            "|    fps                  | 325          |\n",
            "|    iterations           | 242          |\n",
            "|    time_elapsed         | 3047         |\n",
            "|    total_timesteps      | 991232       |\n",
            "| train/                  |              |\n",
            "|    approx_kl            | 9.750875e-07 |\n",
            "|    clip_fraction        | 0            |\n",
            "|    clip_range           | 0.1          |\n",
            "|    entropy_loss         | -1.22        |\n",
            "|    explained_variance   | 0.424        |\n",
            "|    learning_rate        | 5e-05        |\n",
            "|    loss                 | 4.01e+03     |\n",
            "|    n_updates            | 4820         |\n",
            "|    policy_gradient_loss | -8.26e-05    |\n",
            "|    value_loss           | 1.4e+04      |\n",
            "------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 113           |\n",
            "|    ep_rew_mean          | 1.01e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 243           |\n",
            "|    time_elapsed         | 3059          |\n",
            "|    total_timesteps      | 995328        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 2.4917244e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.22         |\n",
            "|    explained_variance   | 0.57          |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 4.15e+03      |\n",
            "|    n_updates            | 4840          |\n",
            "|    policy_gradient_loss | -4.83e-05     |\n",
            "|    value_loss           | 1.27e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 113           |\n",
            "|    ep_rew_mean          | 1.02e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 244           |\n",
            "|    time_elapsed         | 3072          |\n",
            "|    total_timesteps      | 999424        |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 3.1894888e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.521         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 5.08e+03      |\n",
            "|    n_updates            | 4860          |\n",
            "|    policy_gradient_loss | -5.47e-05     |\n",
            "|    value_loss           | 1.45e+04      |\n",
            "-------------------------------------------\n",
            "-------------------------------------------\n",
            "| rollout/                |               |\n",
            "|    ep_len_mean          | 116           |\n",
            "|    ep_rew_mean          | 1.05e+03      |\n",
            "| time/                   |               |\n",
            "|    fps                  | 325           |\n",
            "|    iterations           | 245           |\n",
            "|    time_elapsed         | 3084          |\n",
            "|    total_timesteps      | 1003520       |\n",
            "| train/                  |               |\n",
            "|    approx_kl            | 1.9885192e-07 |\n",
            "|    clip_fraction        | 0             |\n",
            "|    clip_range           | 0.1           |\n",
            "|    entropy_loss         | -1.23         |\n",
            "|    explained_variance   | 0.438         |\n",
            "|    learning_rate        | 5e-05         |\n",
            "|    loss                 | 4.3e+03       |\n",
            "|    n_updates            | 4880          |\n",
            "|    policy_gradient_loss | -4.18e-05     |\n",
            "|    value_loss           | 1.48e+04      |\n",
            "-------------------------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sb3_contrib import MaskablePPO\n",
        "import time\n",
        "\n",
        "model = MaskablePPO.load(\"maskable_ppo_2048_twentyfortyeightenv\")\n",
        "\n",
        "eval_env = TwentyFortyEightEnvMasked()\n",
        "obs, info = eval_env.reset()\n",
        "done = False\n",
        "step = 0\n",
        "\n",
        "print(\"Initial board:\")\n",
        "eval_env.render()\n",
        "\n",
        "while not done :\n",
        "    # Get mask from env\n",
        "    mask = eval_env.get_action_mask()\n",
        "\n",
        "    # IMPORTANT: pass action_masks=mask into predict()\n",
        "    action, _ = model.predict(obs, action_masks=mask, deterministic=True)\n",
        "\n",
        "    old_board = eval_env.board.copy()\n",
        "    obs, reward, terminated, truncated, info = eval_env.step(int(action))\n",
        "    done = terminated or truncated\n",
        "    moved = not np.array_equal(eval_env.board, old_board)\n",
        "\n",
        "    print(f\"\\nStep {step} | done={done}\")\n",
        "    print(f\"  Mask:   {mask}  (up, down, left, right)\")\n",
        "    print(f\"  Action: {int(action)} | Reward: {reward} | Moved? {moved}\")\n",
        "    eval_env.render()\n",
        "\n",
        "    # Sanity check: in non-terminal states, action MUST move\n",
        "    if not done and not moved:\n",
        "        print(\"\\nDEBUG: non-terminal no-op detected. This would mean mask & env disagree.\")\n",
        "        print(\"Board was:\\n\", old_board)\n",
        "        break\n",
        "\n",
        "    step += 1\n",
        "    time.sleep(0.1)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JufQc_D3iqp2",
        "outputId": "e68da966-8ded-458b-e7ce-c8714e8eeec9"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "-------------------------\n",
            "|     |     |     | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 57 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 464\n",
            "-------------------------\n",
            "|     |  2  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     | 16  | 64  |\n",
            "-------------------------\n",
            "|     |     |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 58 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 468\n",
            "-------------------------\n",
            "|     |     |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |     | 16  | 64  |\n",
            "-------------------------\n",
            "|     |     |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 59 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 468\n",
            "-------------------------\n",
            "|  2  |     |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |     |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 60 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 468\n",
            "-------------------------\n",
            "|     |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 61 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 472\n",
            "-------------------------\n",
            "|     |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |     |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 62 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 476\n",
            "-------------------------\n",
            "|     |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |     | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 63 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 484\n",
            "-------------------------\n",
            "|     |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |     | 16  | 64  |\n",
            "-------------------------\n",
            "|     |     |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 64 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 484\n",
            "-------------------------\n",
            "|     |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 64  |\n",
            "-------------------------\n",
            "|  2  |     |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 65 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 484\n",
            "-------------------------\n",
            "|     |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 66 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 488\n",
            "-------------------------\n",
            "|     |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |  2  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 67 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 492\n",
            "-------------------------\n",
            "|  2  |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 68 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 496\n",
            "-------------------------\n",
            "|  2  |  4  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 69 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 504\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 70 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 512\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 71 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 516\n",
            "-------------------------\n",
            "|     |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 72 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 520\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 73 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 524\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 74 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 532\n",
            "-------------------------\n",
            "|     |  8  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 75 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 24.0 | Moved? True\n",
            "\n",
            "Score: 556\n",
            "-------------------------\n",
            "|  2  |     | 16  | 16  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  8  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 76 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 48.0 | Moved? True\n",
            "\n",
            "Score: 604\n",
            "-------------------------\n",
            "|     |     |  2  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |     | 16  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 77 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 32.0 | Moved? True\n",
            "\n",
            "Score: 636\n",
            "-------------------------\n",
            "|     |     |  2  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|  2  |     |     | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 78 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 636\n",
            "-------------------------\n",
            "|     |  2  |  2  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |     |  2  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 79 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 640\n",
            "-------------------------\n",
            "|     |     |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|  2  |     |  2  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 80 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 644\n",
            "-------------------------\n",
            "|     |  2  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |     |  4  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 81 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 652\n",
            "-------------------------\n",
            "|  2  |  2  |  4  | 32  |\n",
            "-------------------------\n",
            "|     |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 82 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 656\n",
            "-------------------------\n",
            "|     |  4  |  4  | 32  |\n",
            "-------------------------\n",
            "|     |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 83 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 664\n",
            "-------------------------\n",
            "|  2  |     |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 84 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 664\n",
            "-------------------------\n",
            "|     |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 85 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 668\n",
            "-------------------------\n",
            "|     |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 86 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 668\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 87 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 672\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 88 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 676\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 89 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 684\n",
            "-------------------------\n",
            "|     |  8  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 90 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 700\n",
            "-------------------------\n",
            "|     |     | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 91 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 36.0 | Moved? True\n",
            "\n",
            "Score: 736\n",
            "-------------------------\n",
            "|  4  |  8  | 32  | 32  |\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 92 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 72.0 | Moved? True\n",
            "\n",
            "Score: 808\n",
            "-------------------------\n",
            "|     |  4  |  8  | 64  |\n",
            "-------------------------\n",
            "|     |  8  |  8  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 93 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 828\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 64  |\n",
            "-------------------------\n",
            "|     |     | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  4  | 32  |\n",
            "-------------------------\n",
            "|     |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 94 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 136.0 | Moved? True\n",
            "\n",
            "Score: 964\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 128 |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |     |  8  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 95 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 968\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 128 |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |     |  8  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 96 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 968\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 128 |\n",
            "-------------------------\n",
            "|  2  |  2  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |     |  8  |  8  |\n",
            "-------------------------\n",
            "|     |  4  |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 97 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 968\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 128 |\n",
            "-------------------------\n",
            "|  2  |  2  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  4  |  8  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 98 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 972\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 128 |\n",
            "-------------------------\n",
            "|  4  |  2  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  4  |  8  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 99 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 980\n",
            "-------------------------\n",
            "|  8  |  4  |  8  | 128 |\n",
            "-------------------------\n",
            "|  2  |  2  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  4  |  8  |  8  |\n",
            "-------------------------\n",
            "|     |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 100 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 1000\n",
            "-------------------------\n",
            "|  8  |  4  |  8  | 128 |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 101 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 1008\n",
            "-------------------------\n",
            "|  8  |  8  |  8  | 128 |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |     |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 102 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1008\n",
            "-------------------------\n",
            "|  8  |  8  |  8  | 128 |\n",
            "-------------------------\n",
            "|  2  |  2  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |     |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 103 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1012\n",
            "-------------------------\n",
            "|  8  |  8  |  8  | 128 |\n",
            "-------------------------\n",
            "|  2  |  4  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |     |  4  | 16  |\n",
            "-------------------------\n",
            "|  4  |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 104 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1012\n",
            "-------------------------\n",
            "|  8  |  8  |  8  | 128 |\n",
            "-------------------------\n",
            "|  2  |  4  | 16  | 32  |\n",
            "-------------------------\n",
            "|  4  |     |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 105 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 28.0 | Moved? True\n",
            "\n",
            "Score: 1040\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 128 |\n",
            "-------------------------\n",
            "|  2  |  4  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |     |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 106 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 36.0 | Moved? True\n",
            "\n",
            "Score: 1076\n",
            "-------------------------\n",
            "|  4  |  8  | 32  | 128 |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |     |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 107 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1076\n",
            "-------------------------\n",
            "|     |     |  2  | 128 |\n",
            "-------------------------\n",
            "|     |     | 32  | 32  |\n",
            "-------------------------\n",
            "|     |  8  |  8  | 16  |\n",
            "-------------------------\n",
            "|  4  |  4  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 108 | done=False\n",
            "  Mask:   [ True False  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1076\n",
            "-------------------------\n",
            "|  4  |  8  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |  4  | 32  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 109 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1076\n",
            "-------------------------\n",
            "|  4  |  8  |  2  | 128 |\n",
            "-------------------------\n",
            "|  2  |  4  | 32  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 110 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1080\n",
            "-------------------------\n",
            "|  4  |  8  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  |  4  | 32  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 111 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 1088\n",
            "-------------------------\n",
            "|  8  |  8  |  2  | 128 |\n",
            "-------------------------\n",
            "|  2  |  4  | 32  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 112 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 80.0 | Moved? True\n",
            "\n",
            "Score: 1168\n",
            "-------------------------\n",
            "|     | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |  2  |  4  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 113 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1172\n",
            "-------------------------\n",
            "|  2  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |  4  |  4  | 64  |\n",
            "-------------------------\n",
            "|     |     |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 114 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1172\n",
            "-------------------------\n",
            "|  2  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |  4  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 115 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1176\n",
            "-------------------------\n",
            "|  4  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |  4  |  4  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 116 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1180\n",
            "-------------------------\n",
            "|  4  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |  4  |  4  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 117 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 1188\n",
            "-------------------------\n",
            "|  4  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|     |     |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 118 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1192\n",
            "-------------------------\n",
            "|  4  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 119 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 1200\n",
            "-------------------------\n",
            "|  8  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 120 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1200\n",
            "-------------------------\n",
            "|  8  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 121 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1204\n",
            "-------------------------\n",
            "|  8  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 122 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1204\n",
            "-------------------------\n",
            "|  8  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 123 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1208\n",
            "-------------------------\n",
            "|  8  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|  4  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 124 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 1220\n",
            "-------------------------\n",
            "|  8  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 125 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 1236\n",
            "-------------------------\n",
            "| 16  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 126 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1240\n",
            "-------------------------\n",
            "| 16  | 16  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 127 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 32.0 | Moved? True\n",
            "\n",
            "Score: 1272\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 128 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 2 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1272\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 129 | done=False\n",
            "  Mask:   [ True  True False  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 1284\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 130 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 24.0 | Moved? True\n",
            "\n",
            "Score: 1308\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 131 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1312\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 132 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1312\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 133 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1316\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 134 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 2 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1316\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 135 | done=False\n",
            "  Mask:   [ True  True False  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 1328\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 136 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 1348\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 64  |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 137 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 32.0 | Moved? True\n",
            "\n",
            "Score: 1380\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 64  |\n",
            "-------------------------\n",
            "|     |     |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 138 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 1388\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  8  | 64  |\n",
            "-------------------------\n",
            "|  2  |     |  2  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 139 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1392\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  8  | 64  |\n",
            "-------------------------\n",
            "|  4  |     |  2  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 140 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1392\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  8  | 64  |\n",
            "-------------------------\n",
            "|  4  |  2  |  2  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 141 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1396\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  8  | 64  |\n",
            "-------------------------\n",
            "|  4  |  4  |  2  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 142 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 1404\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  8  | 64  |\n",
            "-------------------------\n",
            "|     |  8  |  2  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 143 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1408\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  8  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 144 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1408\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  8  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 145 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1412\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  8  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 146 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 1420\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  |  8  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 147 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 1436\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  | 16  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 148 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 36.0 | Moved? True\n",
            "\n",
            "Score: 1472\n",
            "-------------------------\n",
            "|  2  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  | 32  |\n",
            "-------------------------\n",
            "|     |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 149 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 1492\n",
            "-------------------------\n",
            "|  4  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |     |  2  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 150 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 1500\n",
            "-------------------------\n",
            "|  8  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|     | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  2  | 32  |\n",
            "-------------------------\n",
            "|     |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 151 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1500\n",
            "-------------------------\n",
            "|  8  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  2  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  2  | 32  |\n",
            "-------------------------\n",
            "|     |  4  |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 152 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 1512\n",
            "-------------------------\n",
            "|  8  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  2  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  4  | 32  |\n",
            "-------------------------\n",
            "|     |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 153 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 1520\n",
            "-------------------------\n",
            "|  8  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  2  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 154 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1524\n",
            "-------------------------\n",
            "|  8  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |     |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 155 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 1532\n",
            "-------------------------\n",
            "|  8  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |     |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 156 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 1548\n",
            "-------------------------\n",
            "| 16  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  2  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 157 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1552\n",
            "-------------------------\n",
            "| 16  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |     |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 158 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1556\n",
            "-------------------------\n",
            "| 16  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 159 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 2 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 1556\n",
            "-------------------------\n",
            "| 16  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  | 32  |     |\n",
            "-------------------------\n",
            "|  2  |  8  |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 160 | done=False\n",
            "  Mask:   [ True  True False  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 88.0 | Moved? True\n",
            "\n",
            "Score: 1644\n",
            "-------------------------\n",
            "| 16  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  | 64  |\n",
            "-------------------------\n",
            "|  2  | 16  |  2  |  2  |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 161 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 32.0 | Moved? True\n",
            "\n",
            "Score: 1676\n",
            "-------------------------\n",
            "| 16  | 32  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  | 32  | 64  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  2  |  2  |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 162 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 64.0 | Moved? True\n",
            "\n",
            "Score: 1740\n",
            "-------------------------\n",
            "| 16  | 64  |  2  | 128 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 64  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  2  |\n",
            "-------------------------\n",
            "|     |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 163 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 132.0 | Moved? True\n",
            "\n",
            "Score: 1872\n",
            "-------------------------\n",
            "| 16  | 64  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |  8  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "|     |  2  |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 164 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1876\n",
            "-------------------------\n",
            "| 16  | 64  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |  8  |  2  | 128 |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "|     |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 165 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 1880\n",
            "-------------------------\n",
            "| 16  | 64  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |  8  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |     |  4  |  4  |\n",
            "-------------------------\n",
            "|  2  |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 166 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 1888\n",
            "-------------------------\n",
            "| 16  | 64  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |  8  |  2  | 128 |\n",
            "-------------------------\n",
            "|     |     |  2  |  8  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 167 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 264.0 | Moved? True\n",
            "\n",
            "Score: 2152\n",
            "-------------------------\n",
            "| 16  | 64  |  4  | 256 |\n",
            "-------------------------\n",
            "|     |  8  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |     |     |  4  |\n",
            "-------------------------\n",
            "|     |  2  |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 168 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2160\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|     |  8  |     |  8  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "|     |  2  |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 169 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2164\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|     |  8  |     |  8  |\n",
            "-------------------------\n",
            "|     |  4  |     |  4  |\n",
            "-------------------------\n",
            "|     |     |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 170 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2164\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|     |  8  |  2  |  8  |\n",
            "-------------------------\n",
            "|     |  4  |  4  |  4  |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 171 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2172\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|     |  8  |  2  |  8  |\n",
            "-------------------------\n",
            "|     |     |  4  |  8  |\n",
            "-------------------------\n",
            "|     |     |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 172 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 2188\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|     |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     |  4  |     |\n",
            "-------------------------\n",
            "|  2  |     |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 173 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2188\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  2  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     |  4  |     |\n",
            "-------------------------\n",
            "|     |  2  |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 174 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2188\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  2  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |     |\n",
            "-------------------------\n",
            "|  2  |     |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 175 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2192\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |     |\n",
            "-------------------------\n",
            "|  2  |     |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 176 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2192\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  |     |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 177 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2192\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  |  2  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 178 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2196\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|  4  |  2  |  4  |  2  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 179 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 2208\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  8  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  4  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 180 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2212\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  8  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  4  |  4  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 181 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 28.0 | Moved? True\n",
            "\n",
            "Score: 2240\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|     | 16  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     |  4  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 182 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2240\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|     | 16  |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 183 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2240\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  2  | 16  |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 184 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2244\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  | 16  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 185 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2248\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  | 16  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 186 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2256\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  | 16  |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  8  |  8  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 187 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 2272\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  | 16  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 188 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 36.0 | Moved? True\n",
            "\n",
            "Score: 2308\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "|     |  4  |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 189 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2308\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  |  4  |\n",
            "-------------------------\n",
            "|     |     |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 190 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2312\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  |  4  |\n",
            "-------------------------\n",
            "|     |     |  4  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 191 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2320\n",
            "-------------------------\n",
            "| 16  | 64  |  8  | 256 |\n",
            "-------------------------\n",
            "|  4  | 16  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  |  4  |\n",
            "-------------------------\n",
            "|     |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 192 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 2336\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  4  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |     |  4  |\n",
            "-------------------------\n",
            "|  2  |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 193 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2340\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  4  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  4  |  4  |     |  4  |\n",
            "-------------------------\n",
            "|  2  |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 194 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2348\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |     |  4  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 195 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2348\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  |  4  |\n",
            "-------------------------\n",
            "|     |  2  |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 196 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2352\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  |  4  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 197 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2360\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "|  4  |  2  |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 198 | done=False\n",
            "  Mask:   [False  True False  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2360\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "|     |  4  |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 199 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2368\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  |  2  |  8  |\n",
            "-------------------------\n",
            "|     |  4  |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 200 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2376\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  |  2  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |  8  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 201 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 2 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2376\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  |  2  |  8  |\n",
            "-------------------------\n",
            "|  2  |  8  |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 202 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 24.0 | Moved? True\n",
            "\n",
            "Score: 2400\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  4  | 16  |  4  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 203 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 40.0 | Moved? True\n",
            "\n",
            "Score: 2440\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  2  |     |  8  |\n",
            "-------------------------\n",
            "|  2  |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 204 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2444\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 205 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2444\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  |  8  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 206 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2448\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 207 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2456\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  8  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 208 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 2472\n",
            "-------------------------\n",
            "| 16  | 64  | 16  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 209 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 36.0 | Moved? True\n",
            "\n",
            "Score: 2508\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |     |  8  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 210 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2512\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |     |  8  |\n",
            "-------------------------\n",
            "|     |  4  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 211 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2520\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  |     |  8  |\n",
            "-------------------------\n",
            "|  2  |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 212 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2524\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  4  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |     |  8  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 213 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2524\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  4  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 214 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2524\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  4  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 215 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2528\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  4  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  |  8  |\n",
            "-------------------------\n",
            "|  2  |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 216 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2536\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  |  8  |\n",
            "-------------------------\n",
            "|  2  |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 217 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2540\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 218 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2548\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 219 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 2568\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 220 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2572\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 221 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2572\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 222 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2576\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 223 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2584\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 224 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 2600\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  | 16  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 225 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2604\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  | 16  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 226 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2608\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  | 16  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 227 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2616\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  | 16  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  4  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 228 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 24.0 | Moved? True\n",
            "\n",
            "Score: 2640\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 229 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 32.0 | Moved? True\n",
            "\n",
            "Score: 2672\n",
            "-------------------------\n",
            "| 16  | 64  | 32  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  | 32  | 32  |\n",
            "-------------------------\n",
            "|     |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|  4  |  2  |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 230 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 64.0 | Moved? True\n",
            "\n",
            "Score: 2736\n",
            "-------------------------\n",
            "| 16  | 64  | 64  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  2  | 32  |\n",
            "-------------------------\n",
            "|  4  |  4  |     | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 231 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2740\n",
            "-------------------------\n",
            "| 16  | 64  | 64  | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  4  | 32  |\n",
            "-------------------------\n",
            "|  4  |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 232 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 136.0 | Moved? True\n",
            "\n",
            "Score: 2876\n",
            "-------------------------\n",
            "|     | 16  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 233 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2880\n",
            "-------------------------\n",
            "|  8  | 16  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  | 32  |  4  | 32  |\n",
            "-------------------------\n",
            "|     |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 234 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 2888\n",
            "-------------------------\n",
            "|  8  | 16  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  8  |     | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 235 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2888\n",
            "-------------------------\n",
            "|  8  | 16  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 236 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2892\n",
            "-------------------------\n",
            "|  8  | 16  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |  2  |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 237 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2892\n",
            "-------------------------\n",
            "|  8  | 16  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |  2  |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 238 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 2896\n",
            "-------------------------\n",
            "|  8  | 16  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 239 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 2908\n",
            "-------------------------\n",
            "|  8  | 16  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 240 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 2924\n",
            "-------------------------\n",
            "| 16  | 16  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|  4  |  2  |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 241 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 2924\n",
            "-------------------------\n",
            "| 16  | 16  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 242 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 36.0 | Moved? True\n",
            "\n",
            "Score: 2960\n",
            "-------------------------\n",
            "|     | 32  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  | 32  |  8  | 32  |\n",
            "-------------------------\n",
            "|  4  |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 243 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 72.0 | Moved? True\n",
            "\n",
            "Score: 3032\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 244 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 3052\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  4  |     | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 245 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3052\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |     |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 246 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 2 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3052\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|  8  |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 247 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3052\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  8  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 248 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 2 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3052\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |  8  |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 249 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3060\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  4  |  4  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  8  |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 250 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3068\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  8  |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 251 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 28.0 | Moved? True\n",
            "\n",
            "Score: 3096\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     | 16  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  8  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 252 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 3116\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     | 16  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 253 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 40.0 | Moved? True\n",
            "\n",
            "Score: 3156\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     | 16  | 32  | 32  |\n",
            "-------------------------\n",
            "|     |  8  |     | 16  |\n",
            "-------------------------\n",
            "|     |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 254 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3156\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     | 16  | 32  | 32  |\n",
            "-------------------------\n",
            "|     |  8  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 255 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3164\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     | 16  | 32  | 32  |\n",
            "-------------------------\n",
            "|     |  8  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 256 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 80.0 | Moved? True\n",
            "\n",
            "Score: 3244\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     |     | 16  | 64  |\n",
            "-------------------------\n",
            "|  2  |     | 16  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 257 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 36.0 | Moved? True\n",
            "\n",
            "Score: 3280\n",
            "-------------------------\n",
            "|  4  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     |     | 32  | 64  |\n",
            "-------------------------\n",
            "|     |     |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 258 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3280\n",
            "-------------------------\n",
            "|  4  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     |  2  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |     |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 259 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3280\n",
            "-------------------------\n",
            "|  4  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     |  2  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 260 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3280\n",
            "-------------------------\n",
            "|  4  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  |  2  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 261 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3284\n",
            "-------------------------\n",
            "|  4  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 262 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3292\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     |  2  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 263 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3292\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  |  2  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 264 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3296\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  2  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 265 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3300\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  4  | 16  |\n",
            "-------------------------\n",
            "|  2  |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 266 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3300\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 267 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 3312\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 268 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3316\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |     |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 269 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3316\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 270 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3320\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 271 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3328\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 272 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 3348\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 273 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3348\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 274 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3352\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 275 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3360\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 276 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3360\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 277 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3364\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 278 | done=False\n",
            "  Mask:   [False False  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3372\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  4  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 279 | done=False\n",
            "  Mask:   [ True  True False False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 3384\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  4  | 32  | 64  |\n",
            "-------------------------\n",
            "|  8  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 280 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 24.0 | Moved? True\n",
            "\n",
            "Score: 3408\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "| 16  |  8  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |     |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 281 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 32.0 | Moved? True\n",
            "\n",
            "Score: 3440\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  |  8  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 282 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3444\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 283 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3448\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 284 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3448\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 285 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3452\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  8  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 286 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 3468\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  8  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  8  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 287 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 3484\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |     |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 288 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3488\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |     |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 289 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3488\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 290 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3496\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  8  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 291 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 3512\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  | 16  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 292 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3516\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  | 16  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 293 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3520\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  4  | 16  | 16  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 294 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 40.0 | Moved? True\n",
            "\n",
            "Score: 3560\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |     |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 295 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3560\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 296 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3564\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 297 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3568\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 298 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3576\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  8  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 299 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3576\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  8  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 300 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3580\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 301 | done=False\n",
            "  Mask:   [False False  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 3600\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 302 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3608\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 303 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3612\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 304 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3612\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 305 | done=False\n",
            "  Mask:   [False False  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3616\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 306 | done=False\n",
            "  Mask:   [False False  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3624\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 307 | done=False\n",
            "  Mask:   [False False  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 3644\n",
            "-------------------------\n",
            "| 32  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 308 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3644\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "| 32  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  8  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  4  |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 309 | done=False\n",
            "  Mask:   [False False  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 3660\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "| 32  | 16  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  | 16  | 16  | 32  |\n",
            "-------------------------\n",
            "|  4  |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 310 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 32.0 | Moved? True\n",
            "\n",
            "Score: 3692\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "| 32  | 32  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  2  | 16  | 32  |\n",
            "-------------------------\n",
            "|  4  |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 311 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3696\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "| 32  | 32  | 32  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  | 16  | 32  |\n",
            "-------------------------\n",
            "|  4  |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 312 | done=False\n",
            "  Mask:   [False False  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 64.0 | Moved? True\n",
            "\n",
            "Score: 3760\n",
            "-------------------------\n",
            "|  2  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  | 32  | 64  | 64  |\n",
            "-------------------------\n",
            "|  2  |  4  | 16  | 32  |\n",
            "-------------------------\n",
            "|  4  |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 313 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3764\n",
            "-------------------------\n",
            "|  4  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  | 32  | 64  | 64  |\n",
            "-------------------------\n",
            "|  4  |  4  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 314 | done=False\n",
            "  Mask:   [False False  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 140.0 | Moved? True\n",
            "\n",
            "Score: 3904\n",
            "-------------------------\n",
            "|  4  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  4  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 315 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3904\n",
            "-------------------------\n",
            "|  4  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|     |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 316 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3908\n",
            "-------------------------\n",
            "|  4  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  4  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 317 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3916\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  4  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 318 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3920\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  4  |  4  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 319 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3928\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  8  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 320 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3932\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  8  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 321 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 3940\n",
            "-------------------------\n",
            "|  8  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 322 | done=False\n",
            "  Mask:   [ True  True  True False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 3956\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  2  |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|     |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 323 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 3960\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  8  | 16  |\n",
            "-------------------------\n",
            "\n",
            "Step 324 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 2 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 3960\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 325 | done=False\n",
            "  Mask:   [ True  True False False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 52.0 | Moved? True\n",
            "\n",
            "Score: 4012\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  4  |  2  | 32  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  | 32  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 326 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 72.0 | Moved? True\n",
            "\n",
            "Score: 4084\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  2  | 16  |     | 32  |\n",
            "-------------------------\n",
            "|  2  |     |     |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 327 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 4088\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  |     | 32  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 328 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 4088\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  |  2  | 32  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 329 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 4092\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 330 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 4096\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 331 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 4096\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 332 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 4100\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 333 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 4108\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  2  |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 334 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 4112\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 335 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 4120\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 336 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 4124\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 337 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 2 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 4124\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 338 | done=False\n",
            "  Mask:   [ True  True False False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 4140\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  | 16  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 339 | done=False\n",
            "  Mask:   [False False  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 32.0 | Moved? True\n",
            "\n",
            "Score: 4172\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  2  |  4  | 32  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 340 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 4184\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  |  8  | 32  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 341 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 68.0 | Moved? True\n",
            "\n",
            "Score: 4252\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 64  |\n",
            "-------------------------\n",
            "|     |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 342 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 4260\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 64  |\n",
            "-------------------------\n",
            "|  2  |     |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 343 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 4264\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |     |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 344 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 4272\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|  2  |  8  |  8  | 64  |\n",
            "-------------------------\n",
            "|     |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 345 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 4288\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  2  | 64  | 128 |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 346 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 4292\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  4  | 64  | 128 |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 347 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 4296\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  4  | 64  | 128 |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 348 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 4304\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  8  | 64  | 128 |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 349 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 4308\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|  8  |  8  | 64  | 128 |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 350 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 4328\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "|     | 16  | 64  | 128 |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 64  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 351 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 2 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 4328\n",
            "-------------------------\n",
            "| 16  | 64  | 128 | 256 |\n",
            "-------------------------\n",
            "| 16  | 64  | 128 |     |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 352 | done=False\n",
            "  Mask:   [ True  True False  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 416.0 | Moved? True\n",
            "\n",
            "Score: 4744\n",
            "-------------------------\n",
            "| 32  | 128 | 256 | 256 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  |     |\n",
            "-------------------------\n",
            "|     |     |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 353 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 512.0 | Moved? True\n",
            "\n",
            "Score: 5256\n",
            "-------------------------\n",
            "|     | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 354 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5256\n",
            "-------------------------\n",
            "|  4  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|     | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 355 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5256\n",
            "-------------------------\n",
            "|  4  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  2  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 356 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 5260\n",
            "-------------------------\n",
            "|  4  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 357 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 5268\n",
            "-------------------------\n",
            "|  8  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  2  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 358 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 5272\n",
            "-------------------------\n",
            "|  8  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 359 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5272\n",
            "-------------------------\n",
            "|  8  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 360 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 5276\n",
            "-------------------------\n",
            "|  8  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  4  |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 361 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 5288\n",
            "-------------------------\n",
            "|  8  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  4  |  4  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 362 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 16.0 | Moved? True\n",
            "\n",
            "Score: 5304\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  2  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 363 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 5308\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  4  |  4  |  8  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 364 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5308\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  4  |  4  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 365 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 5320\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  2  |  8  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 366 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 5324\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  4  |  8  |  8  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 367 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 5344\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |     |  4  | 16  |\n",
            "-------------------------\n",
            "|     |  4  |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 368 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 5352\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|     |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 369 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 2 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5352\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  4  |  2  |     |     |\n",
            "-------------------------\n",
            "\n",
            "Step 370 | done=False\n",
            "  Mask:   [False  True False  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5352\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  4  |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 371 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 5360\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  | 16  |\n",
            "-------------------------\n",
            "|  2  |     |  8  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 372 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 20.0 | Moved? True\n",
            "\n",
            "Score: 5380\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  4  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  4  |  4  | 16  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 373 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 5388\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  4  | 16  | 16  |\n",
            "-------------------------\n",
            "|     |     |  2  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 374 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 36.0 | Moved? True\n",
            "\n",
            "Score: 5424\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  2  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 375 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5424\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  2  |  4  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 376 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 5428\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  4  |  2  |  4  | 32  |\n",
            "-------------------------\n",
            "|     |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 377 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 8.0 | Moved? True\n",
            "\n",
            "Score: 5436\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  4  |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |     |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 378 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5436\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  4  |  2  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 379 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 5440\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  4  |  4  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 380 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 5452\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  8  |  8  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 381 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5452\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  8  |  8  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  4  |\n",
            "-------------------------\n",
            "\n",
            "Step 382 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 24.0 | Moved? True\n",
            "\n",
            "Score: 5476\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  2  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 383 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 5480\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  4  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 384 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5480\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  4  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 385 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 5484\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  4  |  4  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  2  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 386 | done=False\n",
            "  Mask:   [False  True  True  True]  (up, down, left, right)\n",
            "  Action: 3 | Reward: 12.0 | Moved? True\n",
            "\n",
            "Score: 5496\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|     |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |     |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 387 | done=False\n",
            "  Mask:   [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5496\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|     |  2  |  4  |  8  |\n",
            "-------------------------\n",
            "\n",
            "Step 388 | done=False\n",
            "  Mask:   [False  True  True False]  (up, down, left, right)\n",
            "  Action: 2 | Reward: 0.0 | Moved? True\n",
            "\n",
            "Score: 5496\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  2  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 389 | done=True\n",
            "  Mask:   [ True  True False False]  (up, down, left, right)\n",
            "  Action: 0 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 5500\n",
            "-------------------------\n",
            "| 16  | 32  | 128 | 512 |\n",
            "-------------------------\n",
            "|  8  | 16  | 64  |  2  |\n",
            "-------------------------\n",
            "|  4  |  8  | 16  | 32  |\n",
            "-------------------------\n",
            "|  2  |  4  |  8  |  2  |\n",
            "-------------------------\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# =========================================\n",
        "# üìà Evaluate model & plot average rewards\n",
        "# =========================================\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Number of evaluation episodes\n",
        "N_EPISODES = 1000  # change if you want\n",
        "\n",
        "def moving_average(values, window):\n",
        "    if len(values) < window:\n",
        "        return np.array(values)\n",
        "    weights = np.ones(window) / window\n",
        "    return np.convolve(values, weights, mode=\"valid\")\n",
        "\n",
        "episode_returns = []\n",
        "\n",
        "for ep in range(N_EPISODES):\n",
        "    # Create a fresh env each episode\n",
        "    env = TwentyFortyEightEnvMasked()\n",
        "    obs, info = env.reset()\n",
        "    done = False\n",
        "    ep_ret = 0.0\n",
        "\n",
        "    while not done:\n",
        "        # Get valid action mask from env\n",
        "        mask = env.get_action_mask()\n",
        "\n",
        "        # IMPORTANT: pass mask into predict()\n",
        "        action, _ = model.predict(obs, action_masks=mask, deterministic=True)\n",
        "\n",
        "        obs, reward, terminated, truncated, info = env.step(int(action))\n",
        "        done = terminated or truncated\n",
        "        ep_ret += reward\n",
        "\n",
        "    episode_returns.append(ep_ret)\n",
        "    print(f\"Episode {ep+1}/{N_EPISODES} - total reward: {ep_ret:.2f}\")\n",
        "\n",
        "episode_returns = np.array(episode_returns)\n",
        "ma_window = max(5, N_EPISODES // 5)  # e.g. 5 for 25 eps, 10 for 50 eps\n",
        "smoothed = moving_average(episode_returns, ma_window)\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "plt.plot(episode_returns, label=\"Episode reward\", alpha=0.4)\n",
        "plt.plot(\n",
        "    np.arange(len(smoothed)) + (ma_window - 1),\n",
        "    smoothed,\n",
        "    label=f\"Moving avg (window={ma_window})\",\n",
        "    linewidth=2,\n",
        ")\n",
        "plt.xlabel(\"Episode\")\n",
        "plt.ylabel(\"Total reward\")\n",
        "plt.title(\"Model performance: episode rewards (evaluation)\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "print(f\"\\nMean reward over {N_EPISODES} episodes: {episode_returns.mean():.2f}\")\n",
        "print(f\"Std  reward over {N_EPISODES} episodes: {episode_returns.std():.2f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "UH4NBlhXzIke",
        "outputId": "e3bbd83e-2ce8-4794-b926-73b308d82652"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1/1000 - total reward: 3456.00\n",
            "Episode 2/1000 - total reward: 928.00\n",
            "Episode 3/1000 - total reward: 588.00\n",
            "Episode 4/1000 - total reward: 2460.00\n",
            "Episode 5/1000 - total reward: 844.00\n",
            "Episode 6/1000 - total reward: 776.00\n",
            "Episode 7/1000 - total reward: 1640.00\n",
            "Episode 8/1000 - total reward: 432.00\n",
            "Episode 9/1000 - total reward: 1032.00\n",
            "Episode 10/1000 - total reward: 428.00\n",
            "Episode 11/1000 - total reward: 968.00\n",
            "Episode 12/1000 - total reward: 2604.00\n",
            "Episode 13/1000 - total reward: 1792.00\n",
            "Episode 14/1000 - total reward: 300.00\n",
            "Episode 15/1000 - total reward: 1596.00\n",
            "Episode 16/1000 - total reward: 2636.00\n",
            "Episode 17/1000 - total reward: 3684.00\n",
            "Episode 18/1000 - total reward: 1604.00\n",
            "Episode 19/1000 - total reward: 3032.00\n",
            "Episode 20/1000 - total reward: 3964.00\n",
            "Episode 21/1000 - total reward: 1716.00\n",
            "Episode 22/1000 - total reward: 1900.00\n",
            "Episode 23/1000 - total reward: 2744.00\n",
            "Episode 24/1000 - total reward: 2060.00\n",
            "Episode 25/1000 - total reward: 2024.00\n",
            "Episode 26/1000 - total reward: 1584.00\n",
            "Episode 27/1000 - total reward: 2716.00\n",
            "Episode 28/1000 - total reward: 1540.00\n",
            "Episode 29/1000 - total reward: 4056.00\n",
            "Episode 30/1000 - total reward: 3064.00\n",
            "Episode 31/1000 - total reward: 1088.00\n",
            "Episode 32/1000 - total reward: 2976.00\n",
            "Episode 33/1000 - total reward: 1048.00\n",
            "Episode 34/1000 - total reward: 1496.00\n",
            "Episode 35/1000 - total reward: 1492.00\n",
            "Episode 36/1000 - total reward: 1688.00\n",
            "Episode 37/1000 - total reward: 4600.00\n",
            "Episode 38/1000 - total reward: 1644.00\n",
            "Episode 39/1000 - total reward: 3620.00\n",
            "Episode 40/1000 - total reward: 1580.00\n",
            "Episode 41/1000 - total reward: 3068.00\n",
            "Episode 42/1000 - total reward: 396.00\n",
            "Episode 43/1000 - total reward: 2576.00\n",
            "Episode 44/1000 - total reward: 3516.00\n",
            "Episode 45/1000 - total reward: 2112.00\n",
            "Episode 46/1000 - total reward: 2272.00\n",
            "Episode 47/1000 - total reward: 1712.00\n",
            "Episode 48/1000 - total reward: 3028.00\n",
            "Episode 49/1000 - total reward: 1116.00\n",
            "Episode 50/1000 - total reward: 1560.00\n",
            "Episode 51/1000 - total reward: 440.00\n",
            "Episode 52/1000 - total reward: 1368.00\n",
            "Episode 53/1000 - total reward: 2776.00\n",
            "Episode 54/1000 - total reward: 2900.00\n",
            "Episode 55/1000 - total reward: 1464.00\n",
            "Episode 56/1000 - total reward: 736.00\n",
            "Episode 57/1000 - total reward: 3136.00\n",
            "Episode 58/1000 - total reward: 3108.00\n",
            "Episode 59/1000 - total reward: 1692.00\n",
            "Episode 60/1000 - total reward: 2168.00\n",
            "Episode 61/1000 - total reward: 1584.00\n",
            "Episode 62/1000 - total reward: 1068.00\n",
            "Episode 63/1000 - total reward: 3824.00\n",
            "Episode 64/1000 - total reward: 2736.00\n",
            "Episode 65/1000 - total reward: 1540.00\n",
            "Episode 66/1000 - total reward: 2336.00\n",
            "Episode 67/1000 - total reward: 1584.00\n",
            "Episode 68/1000 - total reward: 3324.00\n",
            "Episode 69/1000 - total reward: 4712.00\n",
            "Episode 70/1000 - total reward: 804.00\n",
            "Episode 71/1000 - total reward: 1568.00\n",
            "Episode 72/1000 - total reward: 1360.00\n",
            "Episode 73/1000 - total reward: 1092.00\n",
            "Episode 74/1000 - total reward: 1524.00\n",
            "Episode 75/1000 - total reward: 5468.00\n",
            "Episode 76/1000 - total reward: 3992.00\n",
            "Episode 77/1000 - total reward: 3948.00\n",
            "Episode 78/1000 - total reward: 1116.00\n",
            "Episode 79/1000 - total reward: 912.00\n",
            "Episode 80/1000 - total reward: 2404.00\n",
            "Episode 81/1000 - total reward: 2756.00\n",
            "Episode 82/1000 - total reward: 2448.00\n",
            "Episode 83/1000 - total reward: 500.00\n",
            "Episode 84/1000 - total reward: 812.00\n",
            "Episode 85/1000 - total reward: 1556.00\n",
            "Episode 86/1000 - total reward: 424.00\n",
            "Episode 87/1000 - total reward: 2752.00\n",
            "Episode 88/1000 - total reward: 1672.00\n",
            "Episode 89/1000 - total reward: 3364.00\n",
            "Episode 90/1000 - total reward: 1112.00\n",
            "Episode 91/1000 - total reward: 3168.00\n",
            "Episode 92/1000 - total reward: 608.00\n",
            "Episode 93/1000 - total reward: 2136.00\n",
            "Episode 94/1000 - total reward: 1720.00\n",
            "Episode 95/1000 - total reward: 1880.00\n",
            "Episode 96/1000 - total reward: 1132.00\n",
            "Episode 97/1000 - total reward: 1516.00\n",
            "Episode 98/1000 - total reward: 3512.00\n",
            "Episode 99/1000 - total reward: 3600.00\n",
            "Episode 100/1000 - total reward: 1808.00\n",
            "Episode 101/1000 - total reward: 5640.00\n",
            "Episode 102/1000 - total reward: 1500.00\n",
            "Episode 103/1000 - total reward: 2588.00\n",
            "Episode 104/1000 - total reward: 440.00\n",
            "Episode 105/1000 - total reward: 936.00\n",
            "Episode 106/1000 - total reward: 612.00\n",
            "Episode 107/1000 - total reward: 3104.00\n",
            "Episode 108/1000 - total reward: 2324.00\n",
            "Episode 109/1000 - total reward: 488.00\n",
            "Episode 110/1000 - total reward: 3448.00\n",
            "Episode 111/1000 - total reward: 1008.00\n",
            "Episode 112/1000 - total reward: 3000.00\n",
            "Episode 113/1000 - total reward: 1368.00\n",
            "Episode 114/1000 - total reward: 2636.00\n",
            "Episode 115/1000 - total reward: 1520.00\n",
            "Episode 116/1000 - total reward: 2564.00\n",
            "Episode 117/1000 - total reward: 1344.00\n",
            "Episode 118/1000 - total reward: 1272.00\n",
            "Episode 119/1000 - total reward: 4008.00\n",
            "Episode 120/1000 - total reward: 1576.00\n",
            "Episode 121/1000 - total reward: 3420.00\n",
            "Episode 122/1000 - total reward: 1624.00\n",
            "Episode 123/1000 - total reward: 2356.00\n",
            "Episode 124/1000 - total reward: 876.00\n",
            "Episode 125/1000 - total reward: 848.00\n",
            "Episode 126/1000 - total reward: 3332.00\n",
            "Episode 127/1000 - total reward: 1308.00\n",
            "Episode 128/1000 - total reward: 1188.00\n",
            "Episode 129/1000 - total reward: 1076.00\n",
            "Episode 130/1000 - total reward: 4196.00\n",
            "Episode 131/1000 - total reward: 3988.00\n",
            "Episode 132/1000 - total reward: 644.00\n",
            "Episode 133/1000 - total reward: 1220.00\n",
            "Episode 134/1000 - total reward: 356.00\n",
            "Episode 135/1000 - total reward: 1500.00\n",
            "Episode 136/1000 - total reward: 2716.00\n",
            "Episode 137/1000 - total reward: 1780.00\n",
            "Episode 138/1000 - total reward: 1540.00\n",
            "Episode 139/1000 - total reward: 968.00\n",
            "Episode 140/1000 - total reward: 2232.00\n",
            "Episode 141/1000 - total reward: 564.00\n",
            "Episode 142/1000 - total reward: 3076.00\n",
            "Episode 143/1000 - total reward: 4840.00\n",
            "Episode 144/1000 - total reward: 1740.00\n",
            "Episode 145/1000 - total reward: 3216.00\n",
            "Episode 146/1000 - total reward: 1820.00\n",
            "Episode 147/1000 - total reward: 2648.00\n",
            "Episode 148/1000 - total reward: 832.00\n",
            "Episode 149/1000 - total reward: 580.00\n",
            "Episode 150/1000 - total reward: 1088.00\n",
            "Episode 151/1000 - total reward: 1376.00\n",
            "Episode 152/1000 - total reward: 1148.00\n",
            "Episode 153/1000 - total reward: 1708.00\n",
            "Episode 154/1000 - total reward: 3488.00\n",
            "Episode 155/1000 - total reward: 1532.00\n",
            "Episode 156/1000 - total reward: 4208.00\n",
            "Episode 157/1000 - total reward: 1316.00\n",
            "Episode 158/1000 - total reward: 1644.00\n",
            "Episode 159/1000 - total reward: 1492.00\n",
            "Episode 160/1000 - total reward: 1528.00\n",
            "Episode 161/1000 - total reward: 696.00\n",
            "Episode 162/1000 - total reward: 3928.00\n",
            "Episode 163/1000 - total reward: 1492.00\n",
            "Episode 164/1000 - total reward: 1688.00\n",
            "Episode 165/1000 - total reward: 3972.00\n",
            "Episode 166/1000 - total reward: 1744.00\n",
            "Episode 167/1000 - total reward: 1560.00\n",
            "Episode 168/1000 - total reward: 644.00\n",
            "Episode 169/1000 - total reward: 760.00\n",
            "Episode 170/1000 - total reward: 2692.00\n",
            "Episode 171/1000 - total reward: 2492.00\n",
            "Episode 172/1000 - total reward: 884.00\n",
            "Episode 173/1000 - total reward: 2004.00\n",
            "Episode 174/1000 - total reward: 2104.00\n",
            "Episode 175/1000 - total reward: 2904.00\n",
            "Episode 176/1000 - total reward: 4248.00\n",
            "Episode 177/1000 - total reward: 1668.00\n",
            "Episode 178/1000 - total reward: 1368.00\n",
            "Episode 179/1000 - total reward: 2064.00\n",
            "Episode 180/1000 - total reward: 1652.00\n",
            "Episode 181/1000 - total reward: 4068.00\n",
            "Episode 182/1000 - total reward: 2864.00\n",
            "Episode 183/1000 - total reward: 752.00\n",
            "Episode 184/1000 - total reward: 2808.00\n",
            "Episode 185/1000 - total reward: 1472.00\n",
            "Episode 186/1000 - total reward: 1412.00\n",
            "Episode 187/1000 - total reward: 3420.00\n",
            "Episode 188/1000 - total reward: 3076.00\n",
            "Episode 189/1000 - total reward: 2108.00\n",
            "Episode 190/1000 - total reward: 2220.00\n",
            "Episode 191/1000 - total reward: 408.00\n",
            "Episode 192/1000 - total reward: 3152.00\n",
            "Episode 193/1000 - total reward: 1036.00\n",
            "Episode 194/1000 - total reward: 1252.00\n",
            "Episode 195/1000 - total reward: 3480.00\n",
            "Episode 196/1000 - total reward: 1684.00\n",
            "Episode 197/1000 - total reward: 612.00\n",
            "Episode 198/1000 - total reward: 3948.00\n",
            "Episode 199/1000 - total reward: 1476.00\n",
            "Episode 200/1000 - total reward: 3524.00\n",
            "Episode 201/1000 - total reward: 1408.00\n",
            "Episode 202/1000 - total reward: 1048.00\n",
            "Episode 203/1000 - total reward: 956.00\n",
            "Episode 204/1000 - total reward: 2992.00\n",
            "Episode 205/1000 - total reward: 1288.00\n",
            "Episode 206/1000 - total reward: 4168.00\n",
            "Episode 207/1000 - total reward: 1736.00\n",
            "Episode 208/1000 - total reward: 1880.00\n",
            "Episode 209/1000 - total reward: 692.00\n",
            "Episode 210/1000 - total reward: 2008.00\n",
            "Episode 211/1000 - total reward: 1744.00\n",
            "Episode 212/1000 - total reward: 1120.00\n",
            "Episode 213/1000 - total reward: 680.00\n",
            "Episode 214/1000 - total reward: 3864.00\n",
            "Episode 215/1000 - total reward: 3032.00\n",
            "Episode 216/1000 - total reward: 748.00\n",
            "Episode 217/1000 - total reward: 1464.00\n",
            "Episode 218/1000 - total reward: 1444.00\n",
            "Episode 219/1000 - total reward: 3364.00\n",
            "Episode 220/1000 - total reward: 2152.00\n",
            "Episode 221/1000 - total reward: 1488.00\n",
            "Episode 222/1000 - total reward: 1232.00\n",
            "Episode 223/1000 - total reward: 720.00\n",
            "Episode 224/1000 - total reward: 1428.00\n",
            "Episode 225/1000 - total reward: 3568.00\n",
            "Episode 226/1000 - total reward: 440.00\n",
            "Episode 227/1000 - total reward: 3592.00\n",
            "Episode 228/1000 - total reward: 1528.00\n",
            "Episode 229/1000 - total reward: 3132.00\n",
            "Episode 230/1000 - total reward: 1792.00\n",
            "Episode 231/1000 - total reward: 3084.00\n",
            "Episode 232/1000 - total reward: 1816.00\n",
            "Episode 233/1000 - total reward: 680.00\n",
            "Episode 234/1000 - total reward: 3348.00\n",
            "Episode 235/1000 - total reward: 904.00\n",
            "Episode 236/1000 - total reward: 1672.00\n",
            "Episode 237/1000 - total reward: 1396.00\n",
            "Episode 238/1000 - total reward: 1288.00\n",
            "Episode 239/1000 - total reward: 708.00\n",
            "Episode 240/1000 - total reward: 1092.00\n",
            "Episode 241/1000 - total reward: 1824.00\n",
            "Episode 242/1000 - total reward: 1144.00\n",
            "Episode 243/1000 - total reward: 3172.00\n",
            "Episode 244/1000 - total reward: 1496.00\n",
            "Episode 245/1000 - total reward: 1696.00\n",
            "Episode 246/1000 - total reward: 2816.00\n",
            "Episode 247/1000 - total reward: 744.00\n",
            "Episode 248/1000 - total reward: 2984.00\n",
            "Episode 249/1000 - total reward: 676.00\n",
            "Episode 250/1000 - total reward: 2480.00\n",
            "Episode 251/1000 - total reward: 3072.00\n",
            "Episode 252/1000 - total reward: 2516.00\n",
            "Episode 253/1000 - total reward: 980.00\n",
            "Episode 254/1000 - total reward: 1724.00\n",
            "Episode 255/1000 - total reward: 1184.00\n",
            "Episode 256/1000 - total reward: 2496.00\n",
            "Episode 257/1000 - total reward: 3448.00\n",
            "Episode 258/1000 - total reward: 1392.00\n",
            "Episode 259/1000 - total reward: 2364.00\n",
            "Episode 260/1000 - total reward: 1776.00\n",
            "Episode 261/1000 - total reward: 1260.00\n",
            "Episode 262/1000 - total reward: 1768.00\n",
            "Episode 263/1000 - total reward: 1904.00\n",
            "Episode 264/1000 - total reward: 588.00\n",
            "Episode 265/1000 - total reward: 320.00\n",
            "Episode 266/1000 - total reward: 844.00\n",
            "Episode 267/1000 - total reward: 2964.00\n",
            "Episode 268/1000 - total reward: 700.00\n",
            "Episode 269/1000 - total reward: 1712.00\n",
            "Episode 270/1000 - total reward: 804.00\n",
            "Episode 271/1000 - total reward: 1764.00\n",
            "Episode 272/1000 - total reward: 3488.00\n",
            "Episode 273/1000 - total reward: 2200.00\n",
            "Episode 274/1000 - total reward: 1420.00\n",
            "Episode 275/1000 - total reward: 1412.00\n",
            "Episode 276/1000 - total reward: 3288.00\n",
            "Episode 277/1000 - total reward: 3324.00\n",
            "Episode 278/1000 - total reward: 1140.00\n",
            "Episode 279/1000 - total reward: 3392.00\n",
            "Episode 280/1000 - total reward: 2536.00\n",
            "Episode 281/1000 - total reward: 1912.00\n",
            "Episode 282/1000 - total reward: 704.00\n",
            "Episode 283/1000 - total reward: 2808.00\n",
            "Episode 284/1000 - total reward: 1376.00\n",
            "Episode 285/1000 - total reward: 1784.00\n",
            "Episode 286/1000 - total reward: 1464.00\n",
            "Episode 287/1000 - total reward: 1652.00\n",
            "Episode 288/1000 - total reward: 1304.00\n",
            "Episode 289/1000 - total reward: 1540.00\n",
            "Episode 290/1000 - total reward: 3580.00\n",
            "Episode 291/1000 - total reward: 1536.00\n",
            "Episode 292/1000 - total reward: 1032.00\n",
            "Episode 293/1000 - total reward: 372.00\n",
            "Episode 294/1000 - total reward: 1484.00\n",
            "Episode 295/1000 - total reward: 1464.00\n",
            "Episode 296/1000 - total reward: 1428.00\n",
            "Episode 297/1000 - total reward: 3056.00\n",
            "Episode 298/1000 - total reward: 1204.00\n",
            "Episode 299/1000 - total reward: 1084.00\n",
            "Episode 300/1000 - total reward: 2592.00\n",
            "Episode 301/1000 - total reward: 484.00\n",
            "Episode 302/1000 - total reward: 1628.00\n",
            "Episode 303/1000 - total reward: 3612.00\n",
            "Episode 304/1000 - total reward: 4660.00\n",
            "Episode 305/1000 - total reward: 1496.00\n",
            "Episode 306/1000 - total reward: 1192.00\n",
            "Episode 307/1000 - total reward: 1508.00\n",
            "Episode 308/1000 - total reward: 1772.00\n",
            "Episode 309/1000 - total reward: 1424.00\n",
            "Episode 310/1000 - total reward: 716.00\n",
            "Episode 311/1000 - total reward: 1876.00\n",
            "Episode 312/1000 - total reward: 1576.00\n",
            "Episode 313/1000 - total reward: 2176.00\n",
            "Episode 314/1000 - total reward: 1356.00\n",
            "Episode 315/1000 - total reward: 428.00\n",
            "Episode 316/1000 - total reward: 780.00\n",
            "Episode 317/1000 - total reward: 668.00\n",
            "Episode 318/1000 - total reward: 820.00\n",
            "Episode 319/1000 - total reward: 1264.00\n",
            "Episode 320/1000 - total reward: 2156.00\n",
            "Episode 321/1000 - total reward: 488.00\n",
            "Episode 322/1000 - total reward: 1684.00\n",
            "Episode 323/1000 - total reward: 1328.00\n",
            "Episode 324/1000 - total reward: 2140.00\n",
            "Episode 325/1000 - total reward: 1424.00\n",
            "Episode 326/1000 - total reward: 932.00\n",
            "Episode 327/1000 - total reward: 1436.00\n",
            "Episode 328/1000 - total reward: 1336.00\n",
            "Episode 329/1000 - total reward: 788.00\n",
            "Episode 330/1000 - total reward: 836.00\n",
            "Episode 331/1000 - total reward: 3996.00\n",
            "Episode 332/1000 - total reward: 1440.00\n",
            "Episode 333/1000 - total reward: 1764.00\n",
            "Episode 334/1000 - total reward: 1768.00\n",
            "Episode 335/1000 - total reward: 1312.00\n",
            "Episode 336/1000 - total reward: 5600.00\n",
            "Episode 337/1000 - total reward: 1188.00\n",
            "Episode 338/1000 - total reward: 2488.00\n",
            "Episode 339/1000 - total reward: 1132.00\n",
            "Episode 340/1000 - total reward: 2268.00\n",
            "Episode 341/1000 - total reward: 1032.00\n",
            "Episode 342/1000 - total reward: 1616.00\n",
            "Episode 343/1000 - total reward: 1028.00\n",
            "Episode 344/1000 - total reward: 2808.00\n",
            "Episode 345/1000 - total reward: 728.00\n",
            "Episode 346/1000 - total reward: 1692.00\n",
            "Episode 347/1000 - total reward: 1688.00\n",
            "Episode 348/1000 - total reward: 1824.00\n",
            "Episode 349/1000 - total reward: 2276.00\n",
            "Episode 350/1000 - total reward: 3592.00\n",
            "Episode 351/1000 - total reward: 2152.00\n",
            "Episode 352/1000 - total reward: 2320.00\n",
            "Episode 353/1000 - total reward: 3200.00\n",
            "Episode 354/1000 - total reward: 2436.00\n",
            "Episode 355/1000 - total reward: 5524.00\n",
            "Episode 356/1000 - total reward: 1760.00\n",
            "Episode 357/1000 - total reward: 1360.00\n",
            "Episode 358/1000 - total reward: 2800.00\n",
            "Episode 359/1000 - total reward: 3356.00\n",
            "Episode 360/1000 - total reward: 1208.00\n",
            "Episode 361/1000 - total reward: 1412.00\n",
            "Episode 362/1000 - total reward: 4008.00\n",
            "Episode 363/1000 - total reward: 1644.00\n",
            "Episode 364/1000 - total reward: 1328.00\n",
            "Episode 365/1000 - total reward: 1208.00\n",
            "Episode 366/1000 - total reward: 1968.00\n",
            "Episode 367/1000 - total reward: 508.00\n",
            "Episode 368/1000 - total reward: 876.00\n",
            "Episode 369/1000 - total reward: 1532.00\n",
            "Episode 370/1000 - total reward: 1148.00\n",
            "Episode 371/1000 - total reward: 1352.00\n",
            "Episode 372/1000 - total reward: 676.00\n",
            "Episode 373/1000 - total reward: 1828.00\n",
            "Episode 374/1000 - total reward: 1684.00\n",
            "Episode 375/1000 - total reward: 1796.00\n",
            "Episode 376/1000 - total reward: 2200.00\n",
            "Episode 377/1000 - total reward: 3324.00\n",
            "Episode 378/1000 - total reward: 2396.00\n",
            "Episode 379/1000 - total reward: 1364.00\n",
            "Episode 380/1000 - total reward: 668.00\n",
            "Episode 381/1000 - total reward: 864.00\n",
            "Episode 382/1000 - total reward: 1616.00\n",
            "Episode 383/1000 - total reward: 2416.00\n",
            "Episode 384/1000 - total reward: 1432.00\n",
            "Episode 385/1000 - total reward: 3504.00\n",
            "Episode 386/1000 - total reward: 2656.00\n",
            "Episode 387/1000 - total reward: 2864.00\n",
            "Episode 388/1000 - total reward: 1208.00\n",
            "Episode 389/1000 - total reward: 1092.00\n",
            "Episode 390/1000 - total reward: 856.00\n",
            "Episode 391/1000 - total reward: 1308.00\n",
            "Episode 392/1000 - total reward: 1640.00\n",
            "Episode 393/1000 - total reward: 1796.00\n",
            "Episode 394/1000 - total reward: 2448.00\n",
            "Episode 395/1000 - total reward: 3284.00\n",
            "Episode 396/1000 - total reward: 2740.00\n",
            "Episode 397/1000 - total reward: 1524.00\n",
            "Episode 398/1000 - total reward: 1848.00\n",
            "Episode 399/1000 - total reward: 1100.00\n",
            "Episode 400/1000 - total reward: 684.00\n",
            "Episode 401/1000 - total reward: 1444.00\n",
            "Episode 402/1000 - total reward: 116.00\n",
            "Episode 403/1000 - total reward: 3484.00\n",
            "Episode 404/1000 - total reward: 1660.00\n",
            "Episode 405/1000 - total reward: 1660.00\n",
            "Episode 406/1000 - total reward: 464.00\n",
            "Episode 407/1000 - total reward: 3332.00\n",
            "Episode 408/1000 - total reward: 1552.00\n",
            "Episode 409/1000 - total reward: 2616.00\n",
            "Episode 410/1000 - total reward: 2000.00\n",
            "Episode 411/1000 - total reward: 292.00\n",
            "Episode 412/1000 - total reward: 1556.00\n",
            "Episode 413/1000 - total reward: 3844.00\n",
            "Episode 414/1000 - total reward: 3360.00\n",
            "Episode 415/1000 - total reward: 2356.00\n",
            "Episode 416/1000 - total reward: 1436.00\n",
            "Episode 417/1000 - total reward: 632.00\n",
            "Episode 418/1000 - total reward: 3088.00\n",
            "Episode 419/1000 - total reward: 1420.00\n",
            "Episode 420/1000 - total reward: 504.00\n",
            "Episode 421/1000 - total reward: 1084.00\n",
            "Episode 422/1000 - total reward: 944.00\n",
            "Episode 423/1000 - total reward: 1568.00\n",
            "Episode 424/1000 - total reward: 3640.00\n",
            "Episode 425/1000 - total reward: 1664.00\n",
            "Episode 426/1000 - total reward: 864.00\n",
            "Episode 427/1000 - total reward: 1572.00\n",
            "Episode 428/1000 - total reward: 3940.00\n",
            "Episode 429/1000 - total reward: 860.00\n",
            "Episode 430/1000 - total reward: 1360.00\n",
            "Episode 431/1000 - total reward: 2052.00\n",
            "Episode 432/1000 - total reward: 3612.00\n",
            "Episode 433/1000 - total reward: 1296.00\n",
            "Episode 434/1000 - total reward: 1336.00\n",
            "Episode 435/1000 - total reward: 1552.00\n",
            "Episode 436/1000 - total reward: 436.00\n",
            "Episode 437/1000 - total reward: 3220.00\n",
            "Episode 438/1000 - total reward: 3924.00\n",
            "Episode 439/1000 - total reward: 1056.00\n",
            "Episode 440/1000 - total reward: 640.00\n",
            "Episode 441/1000 - total reward: 628.00\n",
            "Episode 442/1000 - total reward: 1412.00\n",
            "Episode 443/1000 - total reward: 1492.00\n",
            "Episode 444/1000 - total reward: 928.00\n",
            "Episode 445/1000 - total reward: 940.00\n",
            "Episode 446/1000 - total reward: 1020.00\n",
            "Episode 447/1000 - total reward: 3344.00\n",
            "Episode 448/1000 - total reward: 2724.00\n",
            "Episode 449/1000 - total reward: 3700.00\n",
            "Episode 450/1000 - total reward: 1048.00\n",
            "Episode 451/1000 - total reward: 3368.00\n",
            "Episode 452/1000 - total reward: 420.00\n",
            "Episode 453/1000 - total reward: 1352.00\n",
            "Episode 454/1000 - total reward: 324.00\n",
            "Episode 455/1000 - total reward: 1456.00\n",
            "Episode 456/1000 - total reward: 1156.00\n",
            "Episode 457/1000 - total reward: 1776.00\n",
            "Episode 458/1000 - total reward: 2868.00\n",
            "Episode 459/1000 - total reward: 1672.00\n",
            "Episode 460/1000 - total reward: 1304.00\n",
            "Episode 461/1000 - total reward: 1704.00\n",
            "Episode 462/1000 - total reward: 1468.00\n",
            "Episode 463/1000 - total reward: 3932.00\n",
            "Episode 464/1000 - total reward: 1928.00\n",
            "Episode 465/1000 - total reward: 2204.00\n",
            "Episode 466/1000 - total reward: 1476.00\n",
            "Episode 467/1000 - total reward: 2696.00\n",
            "Episode 468/1000 - total reward: 4116.00\n",
            "Episode 469/1000 - total reward: 1772.00\n",
            "Episode 470/1000 - total reward: 1296.00\n",
            "Episode 471/1000 - total reward: 640.00\n",
            "Episode 472/1000 - total reward: 2552.00\n",
            "Episode 473/1000 - total reward: 644.00\n",
            "Episode 474/1000 - total reward: 1552.00\n",
            "Episode 475/1000 - total reward: 1872.00\n",
            "Episode 476/1000 - total reward: 4904.00\n",
            "Episode 477/1000 - total reward: 1560.00\n",
            "Episode 478/1000 - total reward: 2824.00\n",
            "Episode 479/1000 - total reward: 2660.00\n",
            "Episode 480/1000 - total reward: 1056.00\n",
            "Episode 481/1000 - total reward: 1524.00\n",
            "Episode 482/1000 - total reward: 2612.00\n",
            "Episode 483/1000 - total reward: 2760.00\n",
            "Episode 484/1000 - total reward: 1624.00\n",
            "Episode 485/1000 - total reward: 2536.00\n",
            "Episode 486/1000 - total reward: 1696.00\n",
            "Episode 487/1000 - total reward: 724.00\n",
            "Episode 488/1000 - total reward: 2628.00\n",
            "Episode 489/1000 - total reward: 1504.00\n",
            "Episode 490/1000 - total reward: 1132.00\n",
            "Episode 491/1000 - total reward: 1308.00\n",
            "Episode 492/1000 - total reward: 2536.00\n",
            "Episode 493/1000 - total reward: 2860.00\n",
            "Episode 494/1000 - total reward: 1096.00\n",
            "Episode 495/1000 - total reward: 3336.00\n",
            "Episode 496/1000 - total reward: 1772.00\n",
            "Episode 497/1000 - total reward: 3096.00\n",
            "Episode 498/1000 - total reward: 2056.00\n",
            "Episode 499/1000 - total reward: 2160.00\n",
            "Episode 500/1000 - total reward: 1280.00\n",
            "Episode 501/1000 - total reward: 1816.00\n",
            "Episode 502/1000 - total reward: 2736.00\n",
            "Episode 503/1000 - total reward: 1188.00\n",
            "Episode 504/1000 - total reward: 1320.00\n",
            "Episode 505/1000 - total reward: 4284.00\n",
            "Episode 506/1000 - total reward: 1384.00\n",
            "Episode 507/1000 - total reward: 1368.00\n",
            "Episode 508/1000 - total reward: 1444.00\n",
            "Episode 509/1000 - total reward: 1124.00\n",
            "Episode 510/1000 - total reward: 3388.00\n",
            "Episode 511/1000 - total reward: 3192.00\n",
            "Episode 512/1000 - total reward: 2336.00\n",
            "Episode 513/1000 - total reward: 3212.00\n",
            "Episode 514/1000 - total reward: 1416.00\n",
            "Episode 515/1000 - total reward: 2524.00\n",
            "Episode 516/1000 - total reward: 1528.00\n",
            "Episode 517/1000 - total reward: 3828.00\n",
            "Episode 518/1000 - total reward: 5460.00\n",
            "Episode 519/1000 - total reward: 1860.00\n",
            "Episode 520/1000 - total reward: 740.00\n",
            "Episode 521/1000 - total reward: 2312.00\n",
            "Episode 522/1000 - total reward: 1384.00\n",
            "Episode 523/1000 - total reward: 1792.00\n",
            "Episode 524/1000 - total reward: 512.00\n",
            "Episode 525/1000 - total reward: 2772.00\n",
            "Episode 526/1000 - total reward: 2180.00\n",
            "Episode 527/1000 - total reward: 836.00\n",
            "Episode 528/1000 - total reward: 1172.00\n",
            "Episode 529/1000 - total reward: 1356.00\n",
            "Episode 530/1000 - total reward: 1980.00\n",
            "Episode 531/1000 - total reward: 1812.00\n",
            "Episode 532/1000 - total reward: 3316.00\n",
            "Episode 533/1000 - total reward: 1328.00\n",
            "Episode 534/1000 - total reward: 2368.00\n",
            "Episode 535/1000 - total reward: 3224.00\n",
            "Episode 536/1000 - total reward: 1000.00\n",
            "Episode 537/1000 - total reward: 1640.00\n",
            "Episode 538/1000 - total reward: 292.00\n",
            "Episode 539/1000 - total reward: 3340.00\n",
            "Episode 540/1000 - total reward: 1700.00\n",
            "Episode 541/1000 - total reward: 2944.00\n",
            "Episode 542/1000 - total reward: 3120.00\n",
            "Episode 543/1000 - total reward: 1924.00\n",
            "Episode 544/1000 - total reward: 204.00\n",
            "Episode 545/1000 - total reward: 1368.00\n",
            "Episode 546/1000 - total reward: 608.00\n",
            "Episode 547/1000 - total reward: 852.00\n",
            "Episode 548/1000 - total reward: 2124.00\n",
            "Episode 549/1000 - total reward: 1188.00\n",
            "Episode 550/1000 - total reward: 660.00\n",
            "Episode 551/1000 - total reward: 1616.00\n",
            "Episode 552/1000 - total reward: 1588.00\n",
            "Episode 553/1000 - total reward: 1872.00\n",
            "Episode 554/1000 - total reward: 3584.00\n",
            "Episode 555/1000 - total reward: 1508.00\n",
            "Episode 556/1000 - total reward: 3804.00\n",
            "Episode 557/1000 - total reward: 744.00\n",
            "Episode 558/1000 - total reward: 1548.00\n",
            "Episode 559/1000 - total reward: 3408.00\n",
            "Episode 560/1000 - total reward: 1168.00\n",
            "Episode 561/1000 - total reward: 3252.00\n",
            "Episode 562/1000 - total reward: 2612.00\n",
            "Episode 563/1000 - total reward: 1620.00\n",
            "Episode 564/1000 - total reward: 4328.00\n",
            "Episode 565/1000 - total reward: 2920.00\n",
            "Episode 566/1000 - total reward: 2288.00\n",
            "Episode 567/1000 - total reward: 2396.00\n",
            "Episode 568/1000 - total reward: 1476.00\n",
            "Episode 569/1000 - total reward: 2808.00\n",
            "Episode 570/1000 - total reward: 1188.00\n",
            "Episode 571/1000 - total reward: 692.00\n",
            "Episode 572/1000 - total reward: 940.00\n",
            "Episode 573/1000 - total reward: 2892.00\n",
            "Episode 574/1000 - total reward: 1884.00\n",
            "Episode 575/1000 - total reward: 1864.00\n",
            "Episode 576/1000 - total reward: 1416.00\n",
            "Episode 577/1000 - total reward: 5076.00\n",
            "Episode 578/1000 - total reward: 1472.00\n",
            "Episode 579/1000 - total reward: 1496.00\n",
            "Episode 580/1000 - total reward: 788.00\n",
            "Episode 581/1000 - total reward: 1628.00\n",
            "Episode 582/1000 - total reward: 3192.00\n",
            "Episode 583/1000 - total reward: 5648.00\n",
            "Episode 584/1000 - total reward: 2340.00\n",
            "Episode 585/1000 - total reward: 2024.00\n",
            "Episode 586/1000 - total reward: 532.00\n",
            "Episode 587/1000 - total reward: 1396.00\n",
            "Episode 588/1000 - total reward: 2552.00\n",
            "Episode 589/1000 - total reward: 3132.00\n",
            "Episode 590/1000 - total reward: 1144.00\n",
            "Episode 591/1000 - total reward: 1800.00\n",
            "Episode 592/1000 - total reward: 4560.00\n",
            "Episode 593/1000 - total reward: 3496.00\n",
            "Episode 594/1000 - total reward: 820.00\n",
            "Episode 595/1000 - total reward: 2676.00\n",
            "Episode 596/1000 - total reward: 4236.00\n",
            "Episode 597/1000 - total reward: 2504.00\n",
            "Episode 598/1000 - total reward: 1664.00\n",
            "Episode 599/1000 - total reward: 1436.00\n",
            "Episode 600/1000 - total reward: 1496.00\n",
            "Episode 601/1000 - total reward: 3256.00\n",
            "Episode 602/1000 - total reward: 1316.00\n",
            "Episode 603/1000 - total reward: 2760.00\n",
            "Episode 604/1000 - total reward: 1888.00\n",
            "Episode 605/1000 - total reward: 1192.00\n",
            "Episode 606/1000 - total reward: 3420.00\n",
            "Episode 607/1000 - total reward: 600.00\n",
            "Episode 608/1000 - total reward: 2816.00\n",
            "Episode 609/1000 - total reward: 1456.00\n",
            "Episode 610/1000 - total reward: 3016.00\n",
            "Episode 611/1000 - total reward: 1028.00\n",
            "Episode 612/1000 - total reward: 1188.00\n",
            "Episode 613/1000 - total reward: 2668.00\n",
            "Episode 614/1000 - total reward: 1800.00\n",
            "Episode 615/1000 - total reward: 2128.00\n",
            "Episode 616/1000 - total reward: 1816.00\n",
            "Episode 617/1000 - total reward: 1416.00\n",
            "Episode 618/1000 - total reward: 3556.00\n",
            "Episode 619/1000 - total reward: 2496.00\n",
            "Episode 620/1000 - total reward: 3156.00\n",
            "Episode 621/1000 - total reward: 1476.00\n",
            "Episode 622/1000 - total reward: 1608.00\n",
            "Episode 623/1000 - total reward: 2604.00\n",
            "Episode 624/1000 - total reward: 1796.00\n",
            "Episode 625/1000 - total reward: 1816.00\n",
            "Episode 626/1000 - total reward: 1696.00\n",
            "Episode 627/1000 - total reward: 2872.00\n",
            "Episode 628/1000 - total reward: 620.00\n",
            "Episode 629/1000 - total reward: 2712.00\n",
            "Episode 630/1000 - total reward: 1524.00\n",
            "Episode 631/1000 - total reward: 1056.00\n",
            "Episode 632/1000 - total reward: 3068.00\n",
            "Episode 633/1000 - total reward: 3064.00\n",
            "Episode 634/1000 - total reward: 1652.00\n",
            "Episode 635/1000 - total reward: 2900.00\n",
            "Episode 636/1000 - total reward: 3344.00\n",
            "Episode 637/1000 - total reward: 2568.00\n",
            "Episode 638/1000 - total reward: 5112.00\n",
            "Episode 639/1000 - total reward: 2304.00\n",
            "Episode 640/1000 - total reward: 1488.00\n",
            "Episode 641/1000 - total reward: 4176.00\n",
            "Episode 642/1000 - total reward: 2564.00\n",
            "Episode 643/1000 - total reward: 628.00\n",
            "Episode 644/1000 - total reward: 1124.00\n",
            "Episode 645/1000 - total reward: 1504.00\n",
            "Episode 646/1000 - total reward: 3804.00\n",
            "Episode 647/1000 - total reward: 1144.00\n",
            "Episode 648/1000 - total reward: 4416.00\n",
            "Episode 649/1000 - total reward: 668.00\n",
            "Episode 650/1000 - total reward: 1040.00\n",
            "Episode 651/1000 - total reward: 2588.00\n",
            "Episode 652/1000 - total reward: 2224.00\n",
            "Episode 653/1000 - total reward: 1752.00\n",
            "Episode 654/1000 - total reward: 6032.00\n",
            "Episode 655/1000 - total reward: 1212.00\n",
            "Episode 656/1000 - total reward: 392.00\n",
            "Episode 657/1000 - total reward: 1180.00\n",
            "Episode 658/1000 - total reward: 4156.00\n",
            "Episode 659/1000 - total reward: 3220.00\n",
            "Episode 660/1000 - total reward: 3128.00\n",
            "Episode 661/1000 - total reward: 868.00\n",
            "Episode 662/1000 - total reward: 1648.00\n",
            "Episode 663/1000 - total reward: 3444.00\n",
            "Episode 664/1000 - total reward: 4508.00\n",
            "Episode 665/1000 - total reward: 2584.00\n",
            "Episode 666/1000 - total reward: 6556.00\n",
            "Episode 667/1000 - total reward: 5812.00\n",
            "Episode 668/1000 - total reward: 1524.00\n",
            "Episode 669/1000 - total reward: 1340.00\n",
            "Episode 670/1000 - total reward: 1456.00\n",
            "Episode 671/1000 - total reward: 1844.00\n",
            "Episode 672/1000 - total reward: 1876.00\n",
            "Episode 673/1000 - total reward: 688.00\n",
            "Episode 674/1000 - total reward: 1172.00\n",
            "Episode 675/1000 - total reward: 1492.00\n",
            "Episode 676/1000 - total reward: 1904.00\n",
            "Episode 677/1000 - total reward: 1200.00\n",
            "Episode 678/1000 - total reward: 1680.00\n",
            "Episode 679/1000 - total reward: 1124.00\n",
            "Episode 680/1000 - total reward: 2636.00\n",
            "Episode 681/1000 - total reward: 1904.00\n",
            "Episode 682/1000 - total reward: 756.00\n",
            "Episode 683/1000 - total reward: 496.00\n",
            "Episode 684/1000 - total reward: 564.00\n",
            "Episode 685/1000 - total reward: 2636.00\n",
            "Episode 686/1000 - total reward: 1692.00\n",
            "Episode 687/1000 - total reward: 2232.00\n",
            "Episode 688/1000 - total reward: 1760.00\n",
            "Episode 689/1000 - total reward: 768.00\n",
            "Episode 690/1000 - total reward: 1604.00\n",
            "Episode 691/1000 - total reward: 3768.00\n",
            "Episode 692/1000 - total reward: 1580.00\n",
            "Episode 693/1000 - total reward: 720.00\n",
            "Episode 694/1000 - total reward: 3276.00\n",
            "Episode 695/1000 - total reward: 2504.00\n",
            "Episode 696/1000 - total reward: 1180.00\n",
            "Episode 697/1000 - total reward: 2328.00\n",
            "Episode 698/1000 - total reward: 1820.00\n",
            "Episode 699/1000 - total reward: 1656.00\n",
            "Episode 700/1000 - total reward: 1636.00\n",
            "Episode 701/1000 - total reward: 2544.00\n",
            "Episode 702/1000 - total reward: 3356.00\n",
            "Episode 703/1000 - total reward: 3176.00\n",
            "Episode 704/1000 - total reward: 1356.00\n",
            "Episode 705/1000 - total reward: 1384.00\n",
            "Episode 706/1000 - total reward: 912.00\n",
            "Episode 707/1000 - total reward: 2932.00\n",
            "Episode 708/1000 - total reward: 1920.00\n",
            "Episode 709/1000 - total reward: 796.00\n",
            "Episode 710/1000 - total reward: 3752.00\n",
            "Episode 711/1000 - total reward: 1400.00\n",
            "Episode 712/1000 - total reward: 1328.00\n",
            "Episode 713/1000 - total reward: 3396.00\n",
            "Episode 714/1000 - total reward: 1924.00\n",
            "Episode 715/1000 - total reward: 1684.00\n",
            "Episode 716/1000 - total reward: 1716.00\n",
            "Episode 717/1000 - total reward: 1488.00\n",
            "Episode 718/1000 - total reward: 1380.00\n",
            "Episode 719/1000 - total reward: 1504.00\n",
            "Episode 720/1000 - total reward: 1172.00\n",
            "Episode 721/1000 - total reward: 676.00\n",
            "Episode 722/1000 - total reward: 2992.00\n",
            "Episode 723/1000 - total reward: 1132.00\n",
            "Episode 724/1000 - total reward: 2404.00\n",
            "Episode 725/1000 - total reward: 3452.00\n",
            "Episode 726/1000 - total reward: 3604.00\n",
            "Episode 727/1000 - total reward: 2048.00\n",
            "Episode 728/1000 - total reward: 1112.00\n",
            "Episode 729/1000 - total reward: 1484.00\n",
            "Episode 730/1000 - total reward: 1800.00\n",
            "Episode 731/1000 - total reward: 3156.00\n",
            "Episode 732/1000 - total reward: 3432.00\n",
            "Episode 733/1000 - total reward: 712.00\n",
            "Episode 734/1000 - total reward: 2992.00\n",
            "Episode 735/1000 - total reward: 1220.00\n",
            "Episode 736/1000 - total reward: 6544.00\n",
            "Episode 737/1000 - total reward: 2340.00\n",
            "Episode 738/1000 - total reward: 2408.00\n",
            "Episode 739/1000 - total reward: 2496.00\n",
            "Episode 740/1000 - total reward: 3212.00\n",
            "Episode 741/1000 - total reward: 3316.00\n",
            "Episode 742/1000 - total reward: 380.00\n",
            "Episode 743/1000 - total reward: 1852.00\n",
            "Episode 744/1000 - total reward: 2908.00\n",
            "Episode 745/1000 - total reward: 1308.00\n",
            "Episode 746/1000 - total reward: 2640.00\n",
            "Episode 747/1000 - total reward: 2932.00\n",
            "Episode 748/1000 - total reward: 4216.00\n",
            "Episode 749/1000 - total reward: 3736.00\n",
            "Episode 750/1000 - total reward: 676.00\n",
            "Episode 751/1000 - total reward: 568.00\n",
            "Episode 752/1000 - total reward: 2536.00\n",
            "Episode 753/1000 - total reward: 2352.00\n",
            "Episode 754/1000 - total reward: 3280.00\n",
            "Episode 755/1000 - total reward: 1756.00\n",
            "Episode 756/1000 - total reward: 3492.00\n",
            "Episode 757/1000 - total reward: 708.00\n",
            "Episode 758/1000 - total reward: 2856.00\n",
            "Episode 759/1000 - total reward: 768.00\n",
            "Episode 760/1000 - total reward: 596.00\n",
            "Episode 761/1000 - total reward: 2300.00\n",
            "Episode 762/1000 - total reward: 1984.00\n",
            "Episode 763/1000 - total reward: 2536.00\n",
            "Episode 764/1000 - total reward: 436.00\n",
            "Episode 765/1000 - total reward: 1708.00\n",
            "Episode 766/1000 - total reward: 1296.00\n",
            "Episode 767/1000 - total reward: 2572.00\n",
            "Episode 768/1000 - total reward: 3272.00\n",
            "Episode 769/1000 - total reward: 1776.00\n",
            "Episode 770/1000 - total reward: 3208.00\n",
            "Episode 771/1000 - total reward: 2860.00\n",
            "Episode 772/1000 - total reward: 1120.00\n",
            "Episode 773/1000 - total reward: 712.00\n",
            "Episode 774/1000 - total reward: 1964.00\n",
            "Episode 775/1000 - total reward: 1532.00\n",
            "Episode 776/1000 - total reward: 724.00\n",
            "Episode 777/1000 - total reward: 3380.00\n",
            "Episode 778/1000 - total reward: 1644.00\n",
            "Episode 779/1000 - total reward: 2072.00\n",
            "Episode 780/1000 - total reward: 888.00\n",
            "Episode 781/1000 - total reward: 2948.00\n",
            "Episode 782/1000 - total reward: 1296.00\n",
            "Episode 783/1000 - total reward: 2288.00\n",
            "Episode 784/1000 - total reward: 1640.00\n",
            "Episode 785/1000 - total reward: 5748.00\n",
            "Episode 786/1000 - total reward: 1272.00\n",
            "Episode 787/1000 - total reward: 912.00\n",
            "Episode 788/1000 - total reward: 1308.00\n",
            "Episode 789/1000 - total reward: 1664.00\n",
            "Episode 790/1000 - total reward: 1132.00\n",
            "Episode 791/1000 - total reward: 2356.00\n",
            "Episode 792/1000 - total reward: 3564.00\n",
            "Episode 793/1000 - total reward: 3912.00\n",
            "Episode 794/1000 - total reward: 2436.00\n",
            "Episode 795/1000 - total reward: 2920.00\n",
            "Episode 796/1000 - total reward: 4252.00\n",
            "Episode 797/1000 - total reward: 1576.00\n",
            "Episode 798/1000 - total reward: 768.00\n",
            "Episode 799/1000 - total reward: 2892.00\n",
            "Episode 800/1000 - total reward: 780.00\n",
            "Episode 801/1000 - total reward: 856.00\n",
            "Episode 802/1000 - total reward: 648.00\n",
            "Episode 803/1000 - total reward: 3000.00\n",
            "Episode 804/1000 - total reward: 1220.00\n",
            "Episode 805/1000 - total reward: 2304.00\n",
            "Episode 806/1000 - total reward: 3320.00\n",
            "Episode 807/1000 - total reward: 1912.00\n",
            "Episode 808/1000 - total reward: 1136.00\n",
            "Episode 809/1000 - total reward: 3036.00\n",
            "Episode 810/1000 - total reward: 2464.00\n",
            "Episode 811/1000 - total reward: 2656.00\n",
            "Episode 812/1000 - total reward: 1924.00\n",
            "Episode 813/1000 - total reward: 684.00\n",
            "Episode 814/1000 - total reward: 5140.00\n",
            "Episode 815/1000 - total reward: 748.00\n",
            "Episode 816/1000 - total reward: 992.00\n",
            "Episode 817/1000 - total reward: 1828.00\n",
            "Episode 818/1000 - total reward: 2696.00\n",
            "Episode 819/1000 - total reward: 2984.00\n",
            "Episode 820/1000 - total reward: 2728.00\n",
            "Episode 821/1000 - total reward: 876.00\n",
            "Episode 822/1000 - total reward: 2992.00\n",
            "Episode 823/1000 - total reward: 1164.00\n",
            "Episode 824/1000 - total reward: 1840.00\n",
            "Episode 825/1000 - total reward: 1472.00\n",
            "Episode 826/1000 - total reward: 2856.00\n",
            "Episode 827/1000 - total reward: 3540.00\n",
            "Episode 828/1000 - total reward: 1192.00\n",
            "Episode 829/1000 - total reward: 784.00\n",
            "Episode 830/1000 - total reward: 872.00\n",
            "Episode 831/1000 - total reward: 4452.00\n",
            "Episode 832/1000 - total reward: 1324.00\n",
            "Episode 833/1000 - total reward: 2116.00\n",
            "Episode 834/1000 - total reward: 5244.00\n",
            "Episode 835/1000 - total reward: 3044.00\n",
            "Episode 836/1000 - total reward: 1120.00\n",
            "Episode 837/1000 - total reward: 3148.00\n",
            "Episode 838/1000 - total reward: 1284.00\n",
            "Episode 839/1000 - total reward: 2856.00\n",
            "Episode 840/1000 - total reward: 2608.00\n",
            "Episode 841/1000 - total reward: 1848.00\n",
            "Episode 842/1000 - total reward: 1672.00\n",
            "Episode 843/1000 - total reward: 3356.00\n",
            "Episode 844/1000 - total reward: 2076.00\n",
            "Episode 845/1000 - total reward: 2940.00\n",
            "Episode 846/1000 - total reward: 1492.00\n",
            "Episode 847/1000 - total reward: 2296.00\n",
            "Episode 848/1000 - total reward: 2092.00\n",
            "Episode 849/1000 - total reward: 1636.00\n",
            "Episode 850/1000 - total reward: 1584.00\n",
            "Episode 851/1000 - total reward: 824.00\n",
            "Episode 852/1000 - total reward: 768.00\n",
            "Episode 853/1000 - total reward: 2804.00\n",
            "Episode 854/1000 - total reward: 1684.00\n",
            "Episode 855/1000 - total reward: 1284.00\n",
            "Episode 856/1000 - total reward: 688.00\n",
            "Episode 857/1000 - total reward: 1244.00\n",
            "Episode 858/1000 - total reward: 4124.00\n",
            "Episode 859/1000 - total reward: 1804.00\n",
            "Episode 860/1000 - total reward: 2128.00\n",
            "Episode 861/1000 - total reward: 868.00\n",
            "Episode 862/1000 - total reward: 1924.00\n",
            "Episode 863/1000 - total reward: 1832.00\n",
            "Episode 864/1000 - total reward: 796.00\n",
            "Episode 865/1000 - total reward: 2896.00\n",
            "Episode 866/1000 - total reward: 1168.00\n",
            "Episode 867/1000 - total reward: 1236.00\n",
            "Episode 868/1000 - total reward: 2148.00\n",
            "Episode 869/1000 - total reward: 3084.00\n",
            "Episode 870/1000 - total reward: 1904.00\n",
            "Episode 871/1000 - total reward: 2416.00\n",
            "Episode 872/1000 - total reward: 3928.00\n",
            "Episode 873/1000 - total reward: 1664.00\n",
            "Episode 874/1000 - total reward: 952.00\n",
            "Episode 875/1000 - total reward: 1428.00\n",
            "Episode 876/1000 - total reward: 1064.00\n",
            "Episode 877/1000 - total reward: 1020.00\n",
            "Episode 878/1000 - total reward: 704.00\n",
            "Episode 879/1000 - total reward: 6168.00\n",
            "Episode 880/1000 - total reward: 2144.00\n",
            "Episode 881/1000 - total reward: 1264.00\n",
            "Episode 882/1000 - total reward: 712.00\n",
            "Episode 883/1000 - total reward: 2152.00\n",
            "Episode 884/1000 - total reward: 2272.00\n",
            "Episode 885/1000 - total reward: 1484.00\n",
            "Episode 886/1000 - total reward: 1492.00\n",
            "Episode 887/1000 - total reward: 1060.00\n",
            "Episode 888/1000 - total reward: 3256.00\n",
            "Episode 889/1000 - total reward: 660.00\n",
            "Episode 890/1000 - total reward: 2056.00\n",
            "Episode 891/1000 - total reward: 980.00\n",
            "Episode 892/1000 - total reward: 1140.00\n",
            "Episode 893/1000 - total reward: 760.00\n",
            "Episode 894/1000 - total reward: 2200.00\n",
            "Episode 895/1000 - total reward: 3232.00\n",
            "Episode 896/1000 - total reward: 716.00\n",
            "Episode 897/1000 - total reward: 3576.00\n",
            "Episode 898/1000 - total reward: 1900.00\n",
            "Episode 899/1000 - total reward: 2412.00\n",
            "Episode 900/1000 - total reward: 3184.00\n",
            "Episode 901/1000 - total reward: 4228.00\n",
            "Episode 902/1000 - total reward: 716.00\n",
            "Episode 903/1000 - total reward: 3108.00\n",
            "Episode 904/1000 - total reward: 1172.00\n",
            "Episode 905/1000 - total reward: 4196.00\n",
            "Episode 906/1000 - total reward: 1284.00\n",
            "Episode 907/1000 - total reward: 992.00\n",
            "Episode 908/1000 - total reward: 1100.00\n",
            "Episode 909/1000 - total reward: 1184.00\n",
            "Episode 910/1000 - total reward: 1136.00\n",
            "Episode 911/1000 - total reward: 2296.00\n",
            "Episode 912/1000 - total reward: 1796.00\n",
            "Episode 913/1000 - total reward: 2764.00\n",
            "Episode 914/1000 - total reward: 1864.00\n",
            "Episode 915/1000 - total reward: 1676.00\n",
            "Episode 916/1000 - total reward: 2740.00\n",
            "Episode 917/1000 - total reward: 3348.00\n",
            "Episode 918/1000 - total reward: 2180.00\n",
            "Episode 919/1000 - total reward: 2580.00\n",
            "Episode 920/1000 - total reward: 964.00\n",
            "Episode 921/1000 - total reward: 2296.00\n",
            "Episode 922/1000 - total reward: 1564.00\n",
            "Episode 923/1000 - total reward: 1716.00\n",
            "Episode 924/1000 - total reward: 1232.00\n",
            "Episode 925/1000 - total reward: 2508.00\n",
            "Episode 926/1000 - total reward: 1632.00\n",
            "Episode 927/1000 - total reward: 2452.00\n",
            "Episode 928/1000 - total reward: 1492.00\n",
            "Episode 929/1000 - total reward: 1240.00\n",
            "Episode 930/1000 - total reward: 1248.00\n",
            "Episode 931/1000 - total reward: 1512.00\n",
            "Episode 932/1000 - total reward: 412.00\n",
            "Episode 933/1000 - total reward: 980.00\n",
            "Episode 934/1000 - total reward: 1704.00\n",
            "Episode 935/1000 - total reward: 1548.00\n",
            "Episode 936/1000 - total reward: 1260.00\n",
            "Episode 937/1000 - total reward: 2560.00\n",
            "Episode 938/1000 - total reward: 536.00\n",
            "Episode 939/1000 - total reward: 3328.00\n",
            "Episode 940/1000 - total reward: 3744.00\n",
            "Episode 941/1000 - total reward: 1600.00\n",
            "Episode 942/1000 - total reward: 2424.00\n",
            "Episode 943/1000 - total reward: 4048.00\n",
            "Episode 944/1000 - total reward: 1900.00\n",
            "Episode 945/1000 - total reward: 1728.00\n",
            "Episode 946/1000 - total reward: 1212.00\n",
            "Episode 947/1000 - total reward: 932.00\n",
            "Episode 948/1000 - total reward: 1516.00\n",
            "Episode 949/1000 - total reward: 1472.00\n",
            "Episode 950/1000 - total reward: 1984.00\n",
            "Episode 951/1000 - total reward: 2496.00\n",
            "Episode 952/1000 - total reward: 636.00\n",
            "Episode 953/1000 - total reward: 3176.00\n",
            "Episode 954/1000 - total reward: 2872.00\n",
            "Episode 955/1000 - total reward: 1060.00\n",
            "Episode 956/1000 - total reward: 1568.00\n",
            "Episode 957/1000 - total reward: 3544.00\n",
            "Episode 958/1000 - total reward: 2020.00\n",
            "Episode 959/1000 - total reward: 4044.00\n",
            "Episode 960/1000 - total reward: 1332.00\n",
            "Episode 961/1000 - total reward: 3560.00\n",
            "Episode 962/1000 - total reward: 2320.00\n",
            "Episode 963/1000 - total reward: 1104.00\n",
            "Episode 964/1000 - total reward: 3136.00\n",
            "Episode 965/1000 - total reward: 1280.00\n",
            "Episode 966/1000 - total reward: 1212.00\n",
            "Episode 967/1000 - total reward: 3164.00\n",
            "Episode 968/1000 - total reward: 1612.00\n",
            "Episode 969/1000 - total reward: 1192.00\n",
            "Episode 970/1000 - total reward: 368.00\n",
            "Episode 971/1000 - total reward: 2944.00\n",
            "Episode 972/1000 - total reward: 1380.00\n",
            "Episode 973/1000 - total reward: 4792.00\n",
            "Episode 974/1000 - total reward: 1308.00\n",
            "Episode 975/1000 - total reward: 1552.00\n",
            "Episode 976/1000 - total reward: 3172.00\n",
            "Episode 977/1000 - total reward: 604.00\n",
            "Episode 978/1000 - total reward: 2296.00\n",
            "Episode 979/1000 - total reward: 1096.00\n",
            "Episode 980/1000 - total reward: 4260.00\n",
            "Episode 981/1000 - total reward: 1380.00\n",
            "Episode 982/1000 - total reward: 2648.00\n",
            "Episode 983/1000 - total reward: 1380.00\n",
            "Episode 984/1000 - total reward: 3648.00\n",
            "Episode 985/1000 - total reward: 760.00\n",
            "Episode 986/1000 - total reward: 2804.00\n",
            "Episode 987/1000 - total reward: 1172.00\n",
            "Episode 988/1000 - total reward: 1732.00\n",
            "Episode 989/1000 - total reward: 2520.00\n",
            "Episode 990/1000 - total reward: 1680.00\n",
            "Episode 991/1000 - total reward: 880.00\n",
            "Episode 992/1000 - total reward: 1716.00\n",
            "Episode 993/1000 - total reward: 3216.00\n",
            "Episode 994/1000 - total reward: 1372.00\n",
            "Episode 995/1000 - total reward: 1692.00\n",
            "Episode 996/1000 - total reward: 3440.00\n",
            "Episode 997/1000 - total reward: 3940.00\n",
            "Episode 998/1000 - total reward: 1600.00\n",
            "Episode 999/1000 - total reward: 1676.00\n",
            "Episode 1000/1000 - total reward: 1516.00\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1200x600 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Mean reward over 1000 episodes: 2005.64\n",
            "Std  reward over 1000 episodes: 1097.25\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "eval_env = TwentyFortyEightEnvMasked()\n",
        "eval_env = ActionMasker(eval_env, mask_fn)\n",
        "\n",
        "obs, info = eval_env.reset()\n",
        "done = False\n",
        "step = 0\n",
        "\n",
        "raw_env = eval_env.env  # underlying TwentyFortyEightEnvMasked\n",
        "\n",
        "print(\"Initial Board:\")\n",
        "raw_env.render()\n",
        "\n",
        "while not done and step < 40:\n",
        "    mask = raw_env.get_action_mask()\n",
        "    action, _ = model.predict(obs, deterministic=True)\n",
        "\n",
        "    old_board = raw_env.board.copy()\n",
        "    obs, reward, terminated, truncated, info = eval_env.step(int(action))\n",
        "    done = terminated or truncated\n",
        "    moved = not np.array_equal(raw_env.board, old_board)\n",
        "\n",
        "    print(f\"\\nStep {step} | done={done}\")\n",
        "    print(f\"  Mask:  {mask}  (up, down, left, right)\")\n",
        "    print(f\"  Action: {int(action)} | Reward: {reward} | Moved? {moved}\")\n",
        "    raw_env.render()\n",
        "\n",
        "    step += 1\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7QGwG77ylgxo",
        "outputId": "4e87572e-43bd-4434-a70d-b51bbe08b04f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Board:\n",
            "\n",
            "Score: 0\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  2  |     |\n",
            "-------------------------\n",
            "|     |     |  2  |     |\n",
            "-------------------------\n",
            "\n",
            "Step 0 | done=False\n",
            "  Mask:  [ True  True  True  True]  (up, down, left, right)\n",
            "  Action: 1 | Reward: 4.0 | Moved? True\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 1 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 2 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 3 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 4 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 5 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 6 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 7 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 8 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 9 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 10 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 11 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 12 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 13 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 14 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 15 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 16 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 17 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 18 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 19 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 20 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 21 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 22 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 23 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 24 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 25 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 26 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 27 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 28 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 29 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 30 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 31 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 32 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 33 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 34 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 35 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 36 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 37 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 38 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n",
            "\n",
            "Step 39 | done=False\n",
            "  Mask:  [ True False  True False]  (up, down, left, right)\n",
            "  Action: 1 | Reward: -2 | Moved? False\n",
            "\n",
            "Score: 4\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |     |     |\n",
            "-------------------------\n",
            "|     |     |  4  |  2  |\n",
            "-------------------------\n"
          ]
        }
      ]
    }
  ]
}